"""
æ–‡æœ¬åˆ†ç±»ï¼ˆText Classificationï¼‰
NLP æœ€å¸¸è§çš„åº”ç”¨ä¹‹ä¸€

Text Classification - One of the Most Common NLP Applications

ä»åƒåœ¾é‚®ä»¶è¿‡æ»¤åˆ°æƒ…æ„Ÿåˆ†æï¼Œæ–‡æœ¬åˆ†ç±»æ— å¤„ä¸åœ¨
From spam filtering to sentiment analysis, text classification is everywhere
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import time
import warnings
warnings.filterwarnings('ignore')

from sklearn.datasets import fetch_20newsgroups
from sklearn.model_selection import train_test_split, cross_val_score, learning_curve
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (classification_report, confusion_matrix,
                            accuracy_score, precision_recall_fscore_support,
                            roc_curve, auc, roc_auc_score)
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import label_binarize

from utils import RANDOM_STATE, setup_chinese_fonts, create_subplots, DPI, save_figure, get_output_path

# å°è¯•å¯¼å…¥æ·±åº¦å­¦ä¹ åº“
try:
    from sklearn.neural_network import MLPClassifier
    MLP_AVAILABLE = True
except ImportError:
    MLP_AVAILABLE = False

setup_chinese_fonts()

print("=" * 80)
print("æ–‡æœ¬åˆ†ç±»ï¼ˆText Classificationï¼‰æ•™ç¨‹".center(80))
print("=" * 80)

# ============================================================================
# 1. æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ä»‹ç» / Text Classification Introduction
# ============================================================================
print("\nã€1ã€‘æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ä»‹ç»")
print("-" * 80)
print("""
ä»€ä¹ˆæ˜¯æ–‡æœ¬åˆ†ç±»ï¼Ÿ
What is Text Classification?

æ–‡æœ¬åˆ†ç±»æ˜¯å°†æ–‡æœ¬è‡ªåŠ¨åˆ†é…åˆ°é¢„å®šä¹‰ç±»åˆ«çš„ä»»åŠ¡
Automatically assigning text to predefined categories

å¸¸è§åº”ç”¨åœºæ™¯ï¼š
Common Applications:

ğŸ“§ åƒåœ¾é‚®ä»¶æ£€æµ‹ (Spam Detection)
   â€¢ åˆ¤æ–­é‚®ä»¶æ˜¯å¦ä¸ºåƒåœ¾é‚®ä»¶
   â€¢ äºŒåˆ†ç±»é—®é¢˜

ğŸ˜Š æƒ…æ„Ÿåˆ†æ (Sentiment Analysis)
   â€¢ åˆ†æè¯„è®ºçš„æƒ…æ„Ÿå€¾å‘ï¼ˆæ­£é¢/è´Ÿé¢/ä¸­æ€§ï¼‰
   â€¢ å¤šåˆ†ç±»é—®é¢˜

ğŸ“° æ–°é—»åˆ†ç±» (News Classification)
   â€¢ å°†æ–°é—»åˆ†ç±»åˆ°ä¸åŒä¸»é¢˜
   â€¢ å¤šåˆ†ç±»é—®é¢˜

ğŸ·ï¸ ä¸»é¢˜æ ‡æ³¨ (Topic Labeling)
   â€¢ ä¸ºæ–‡æ¡£æ‰“ä¸Šä¸»é¢˜æ ‡ç­¾
   â€¢ å¤šæ ‡ç­¾åˆ†ç±»

ğŸ’¬ æ„å›¾è¯†åˆ« (Intent Classification)
   â€¢ ç†è§£ç”¨æˆ·æŸ¥è¯¢æ„å›¾
   â€¢ èŠå¤©æœºå™¨äººæ ¸å¿ƒåŠŸèƒ½

æ–‡æœ¬åˆ†ç±»æµç¨‹ï¼š
Classification Pipeline:

1. æ•°æ®æ”¶é›†ä¸æ ‡æ³¨ â†’ 2. æ–‡æœ¬é¢„å¤„ç† â†’ 3. ç‰¹å¾æå– â†’
4. æ¨¡å‹è®­ç»ƒ â†’ 5. æ¨¡å‹è¯„ä¼° â†’ 6. æ¨¡å‹éƒ¨ç½²
""")

# ============================================================================
# 2. æ•°æ®å‡†å¤‡ / Data Preparation
# ============================================================================
print("\nã€2ã€‘æ•°æ®å‡†å¤‡ - 20 Newsgroups æ•°æ®é›†")
print("-" * 80)
print("æ­£åœ¨åŠ è½½æ•°æ®é›†...")

# ä¸ºäº†æ¼”ç¤ºï¼Œæˆ‘ä»¬åªä½¿ç”¨ 4 ä¸ªç±»åˆ«
categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']

# åŠ è½½è®­ç»ƒæ•°æ®
try:
    newsgroups_train = fetch_20newsgroups(
        subset='train',
        categories=categories,
        shuffle=True,
        random_state=RANDOM_STATE,
        remove=('headers', 'footers', 'quotes')
    )

    # åŠ è½½æµ‹è¯•æ•°æ®
    newsgroups_test = fetch_20newsgroups(
        subset='test',
        categories=categories,
        shuffle=True,
        random_state=RANDOM_STATE,
        remove=('headers', 'footers', 'quotes')
    )

    X_train_text = newsgroups_train.data
    y_train = newsgroups_train.target
    X_test_text = newsgroups_test.data
    y_test = newsgroups_test.target

    target_names = newsgroups_train.target_names

    print(f"âœ“ æ•°æ®é›†åŠ è½½æˆåŠŸ")
    print(f"\næ•°æ®é›†ä¿¡æ¯:")
    print(f"  è®­ç»ƒé›†å¤§å°: {len(X_train_text)}")
    print(f"  æµ‹è¯•é›†å¤§å°: {len(X_test_text)}")
    print(f"  ç±»åˆ«æ•°: {len(target_names)}")
    print(f"  ç±»åˆ«åç§°: {target_names}")

    # ç±»åˆ«åˆ†å¸ƒ
    print(f"\nè®­ç»ƒé›†ç±»åˆ«åˆ†å¸ƒ:")
    unique, counts = np.unique(y_train, return_counts=True)
    for i, count in zip(unique, counts):
        print(f"  {target_names[i]}: {count} ({count/len(y_train)*100:.1f}%)")

    # æŸ¥çœ‹ç¤ºä¾‹æ–‡æ¡£
    print(f"\nç¤ºä¾‹æ–‡æ¡£ï¼ˆç±»åˆ«: {target_names[y_train[0]]}):")
    print(f"{X_train_text[0][:300]}...")

    DATA_LOADED = True

except Exception as e:
    print(f"âœ— æ•°æ®é›†åŠ è½½å¤±è´¥: {e}")
    print("ä½¿ç”¨è‡ªå®šä¹‰ç¤ºä¾‹æ•°æ®...")

    # åˆ›å»ºç®€å•çš„ç¤ºä¾‹æ•°æ®
    X_train_text = [
        "Python is a great programming language for machine learning",
        "I love using Python for data science projects",
        "Machine learning algorithms are fascinating",
        "Deep learning neural networks are powerful",
        "This movie was absolutely amazing and wonderful",
        "I really enjoyed watching this film",
        "The weather is sunny and beautiful today",
        "What a lovely day with clear blue skies",
        "This product is terrible and poorly made",
        "Very disappointed with this purchase",
        "Python programming is my favorite hobby",
        "Data science is an exciting field",
        "The acting in this movie was superb",
        "Beautiful cinematography and great story",
        "Awful experience, would not recommend",
        "Complete waste of money and time",
    ]

    y_train = np.array([0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 0, 0, 1, 1, 2, 2])

    # åˆ›å»ºæµ‹è¯•é›†
    X_test_text = [
        "Learning Python is fun and rewarding",
        "Machine learning is the future",
        "This film was excellent and entertaining",
        "Beautiful weather for outdoor activities",
        "Disappointing quality and poor service",
        "Data analysis with Python is efficient",
    ]

    y_test = np.array([0, 0, 1, 1, 2, 0])

    target_names = ['Technology', 'Positive', 'Negative']

    print(f"âœ“ ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®")
    print(f"  è®­ç»ƒé›†å¤§å°: {len(X_train_text)}")
    print(f"  æµ‹è¯•é›†å¤§å°: {len(X_test_text)}")
    print(f"  ç±»åˆ«: {target_names}")

    DATA_LOADED = False

# ============================================================================
# 3. æ•°æ®æ¢ç´¢å’Œå¯è§†åŒ– / Data Exploration
# ============================================================================
print("\nã€3ã€‘æ•°æ®æ¢ç´¢")
print("-" * 80)

# æ–‡æœ¬é•¿åº¦ç»Ÿè®¡
text_lengths = [len(text.split()) for text in X_train_text]

print(f"æ–‡æœ¬é•¿åº¦ç»Ÿè®¡:")
print(f"  å¹³å‡é•¿åº¦: {np.mean(text_lengths):.1f} è¯")
print(f"  æœ€çŸ­: {np.min(text_lengths)} è¯")
print(f"  æœ€é•¿: {np.max(text_lengths)} è¯")
print(f"  ä¸­ä½æ•°: {np.median(text_lengths):.1f} è¯")

# ============================================================================
# 4. ç‰¹å¾æå– / Feature Extraction
# ============================================================================
print("\nã€4ã€‘ç‰¹å¾æå–")
print("-" * 80)

# TF-IDF ç‰¹å¾æå–
print("ä½¿ç”¨ TF-IDF è¿›è¡Œç‰¹å¾æå–...")

tfidf_vectorizer = TfidfVectorizer(
    max_features=5000,
    min_df=2,
    max_df=0.8,
    ngram_range=(1, 2),
    stop_words='english'
)

X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_text)
X_test_tfidf = tfidf_vectorizer.transform(X_test_text)

print(f"âœ“ TF-IDF ç‰¹å¾æå–å®Œæˆ")
print(f"  è®­ç»ƒé›†å½¢çŠ¶: {X_train_tfidf.shape}")
print(f"  æµ‹è¯•é›†å½¢çŠ¶: {X_test_tfidf.shape}")
print(f"  è¯æ±‡è¡¨å¤§å°: {len(tfidf_vectorizer.vocabulary_)}")
print(f"  ç¨€ç–åº¦: {(1 - X_train_tfidf.nnz / (X_train_tfidf.shape[0] * X_train_tfidf.shape[1])) * 100:.2f}%")

# BoW ç‰¹å¾æå–ï¼ˆç”¨äº Naive Bayesï¼‰
bow_vectorizer = CountVectorizer(
    max_features=5000,
    min_df=2,
    max_df=0.8,
    stop_words='english'
)

X_train_bow = bow_vectorizer.fit_transform(X_train_text)
X_test_bow = bow_vectorizer.transform(X_test_text)

print(f"\nâœ“ BoW ç‰¹å¾æå–å®Œæˆ")
print(f"  è®­ç»ƒé›†å½¢çŠ¶: {X_train_bow.shape}")

# ============================================================================
# 5. ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹ / Traditional ML Models
# ============================================================================
print("\nã€5ã€‘è®­ç»ƒä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹")
print("=" * 80)

# å­˜å‚¨æ¨¡å‹å’Œç»“æœ
models = {}
results = {}
training_times = {}
prediction_times = {}

# 5.1 Logistic Regression + TF-IDF
print("\nã€5.1ã€‘Logistic Regression + TF-IDF")
print("-" * 80)

start_time = time.time()
lr_model = LogisticRegression(
    max_iter=1000,
    random_state=RANDOM_STATE,
    n_jobs=-1
)
lr_model.fit(X_train_tfidf, y_train)
training_times['Logistic Regression'] = time.time() - start_time

start_time = time.time()
y_pred_lr = lr_model.predict(X_test_tfidf)
prediction_times['Logistic Regression'] = time.time() - start_time

accuracy_lr = accuracy_score(y_test, y_pred_lr)
models['Logistic Regression'] = lr_model
results['Logistic Regression'] = {
    'predictions': y_pred_lr,
    'accuracy': accuracy_lr
}

print(f"âœ“ è®­ç»ƒå®Œæˆ")
print(f"  è®­ç»ƒæ—¶é—´: {training_times['Logistic Regression']:.3f} ç§’")
print(f"  é¢„æµ‹æ—¶é—´: {prediction_times['Logistic Regression']:.3f} ç§’")
print(f"  å‡†ç¡®ç‡: {accuracy_lr:.4f}")

print(f"\nåˆ†ç±»æŠ¥å‘Š:")
print(classification_report(y_test, y_pred_lr, target_names=target_names))

# 5.2 Naive Bayes + BoW
print("\nã€5.2ã€‘Naive Bayes + BoW")
print("-" * 80)

start_time = time.time()
nb_model = MultinomialNB(alpha=0.1)
nb_model.fit(X_train_bow, y_train)
training_times['Naive Bayes'] = time.time() - start_time

start_time = time.time()
y_pred_nb = nb_model.predict(X_test_bow)
prediction_times['Naive Bayes'] = time.time() - start_time

accuracy_nb = accuracy_score(y_test, y_pred_nb)
models['Naive Bayes'] = nb_model
results['Naive Bayes'] = {
    'predictions': y_pred_nb,
    'accuracy': accuracy_nb
}

print(f"âœ“ è®­ç»ƒå®Œæˆ")
print(f"  è®­ç»ƒæ—¶é—´: {training_times['Naive Bayes']:.3f} ç§’")
print(f"  é¢„æµ‹æ—¶é—´: {prediction_times['Naive Bayes']:.3f} ç§’")
print(f"  å‡†ç¡®ç‡: {accuracy_nb:.4f}")

print(f"\nåˆ†ç±»æŠ¥å‘Š:")
print(classification_report(y_test, y_pred_nb, target_names=target_names))

# 5.3 SVM (Linear) + TF-IDF
print("\nã€5.3ã€‘SVM (Linear) + TF-IDF")
print("-" * 80)

start_time = time.time()
svm_model = LinearSVC(
    random_state=RANDOM_STATE,
    max_iter=2000
)
svm_model.fit(X_train_tfidf, y_train)
training_times['SVM'] = time.time() - start_time

start_time = time.time()
y_pred_svm = svm_model.predict(X_test_tfidf)
prediction_times['SVM'] = time.time() - start_time

accuracy_svm = accuracy_score(y_test, y_pred_svm)
models['SVM'] = svm_model
results['SVM'] = {
    'predictions': y_pred_svm,
    'accuracy': accuracy_svm
}

print(f"âœ“ è®­ç»ƒå®Œæˆ")
print(f"  è®­ç»ƒæ—¶é—´: {training_times['SVM']:.3f} ç§’")
print(f"  é¢„æµ‹æ—¶é—´: {prediction_times['SVM']:.3f} ç§’")
print(f"  å‡†ç¡®ç‡: {accuracy_svm:.4f}")

print(f"\nåˆ†ç±»æŠ¥å‘Š:")
print(classification_report(y_test, y_pred_svm, target_names=target_names))

# 5.4 Random Forest + TF-IDF
print("\nã€5.4ã€‘Random Forest + TF-IDF")
print("-" * 80)

start_time = time.time()
rf_model = RandomForestClassifier(
    n_estimators=100,
    random_state=RANDOM_STATE,
    n_jobs=-1
)
rf_model.fit(X_train_tfidf, y_train)
training_times['Random Forest'] = time.time() - start_time

start_time = time.time()
y_pred_rf = rf_model.predict(X_test_tfidf)
prediction_times['Random Forest'] = time.time() - start_time

accuracy_rf = accuracy_score(y_test, y_pred_rf)
models['Random Forest'] = rf_model
results['Random Forest'] = {
    'predictions': y_pred_rf,
    'accuracy': accuracy_rf
}

print(f"âœ“ è®­ç»ƒå®Œæˆ")
print(f"  è®­ç»ƒæ—¶é—´: {training_times['Random Forest']:.3f} ç§’")
print(f"  é¢„æµ‹æ—¶é—´: {prediction_times['Random Forest']:.3f} ç§’")
print(f"  å‡†ç¡®ç‡: {accuracy_rf:.4f}")

print(f"\nåˆ†ç±»æŠ¥å‘Š:")
print(classification_report(y_test, y_pred_rf, target_names=target_names))

# ç‰¹å¾é‡è¦æ€§ï¼ˆRandom Forestï¼‰
if hasattr(rf_model, 'feature_importances_'):
    feature_importances = rf_model.feature_importances_
    top_indices = feature_importances.argsort()[-10:][::-1]
    top_features = [tfidf_vectorizer.get_feature_names_out()[i] for i in top_indices]
    top_importance = feature_importances[top_indices]

    print(f"\nTop 10 é‡è¦ç‰¹å¾:")
    for feature, importance in zip(top_features, top_importance):
        print(f"  {feature}: {importance:.4f}")

# 5.5 ç®€å•ç¥ç»ç½‘ç»œï¼ˆMLPï¼‰
if MLP_AVAILABLE:
    print("\nã€5.5ã€‘å¤šå±‚æ„ŸçŸ¥å™¨ (MLP) + TF-IDF")
    print("-" * 80)

    start_time = time.time()
    mlp_model = MLPClassifier(
        hidden_layer_sizes=(100, 50),
        max_iter=300,
        random_state=RANDOM_STATE,
        early_stopping=True,
        validation_fraction=0.1
    )
    mlp_model.fit(X_train_tfidf, y_train)
    training_times['MLP'] = time.time() - start_time

    start_time = time.time()
    y_pred_mlp = mlp_model.predict(X_test_tfidf)
    prediction_times['MLP'] = time.time() - start_time

    accuracy_mlp = accuracy_score(y_test, y_pred_mlp)
    models['MLP'] = mlp_model
    results['MLP'] = {
        'predictions': y_pred_mlp,
        'accuracy': accuracy_mlp
    }

    print(f"âœ“ è®­ç»ƒå®Œæˆ")
    print(f"  è®­ç»ƒæ—¶é—´: {training_times['MLP']:.3f} ç§’")
    print(f"  é¢„æµ‹æ—¶é—´: {prediction_times['MLP']:.3f} ç§’")
    print(f"  å‡†ç¡®ç‡: {accuracy_mlp:.4f}")
    print(f"  è¿­ä»£æ¬¡æ•°: {mlp_model.n_iter_}")

    print(f"\nåˆ†ç±»æŠ¥å‘Š:")
    print(classification_report(y_test, y_pred_mlp, target_names=target_names))

# ============================================================================
# 6. æ¨¡å‹å¯¹æ¯” / Model Comparison
# ============================================================================
print("\nã€6ã€‘æ¨¡å‹æ€§èƒ½å¯¹æ¯”")
print("=" * 80)

# æ€§èƒ½å¯¹æ¯”è¡¨æ ¼
print("\næ‰€æœ‰æ¨¡å‹æ€§èƒ½æ€»ç»“:")
print("-" * 80)
print(f"{'æ¨¡å‹':<20} {'å‡†ç¡®ç‡':<12} {'è®­ç»ƒæ—¶é—´(s)':<15} {'é¢„æµ‹æ—¶é—´(s)':<15}")
print("-" * 80)

for model_name in results.keys():
    accuracy = results[model_name]['accuracy']
    train_time = training_times[model_name]
    pred_time = prediction_times[model_name]
    print(f"{model_name:<20} {accuracy:<12.4f} {train_time:<15.3f} {pred_time:<15.3f}")

# æ‰¾å‡ºæœ€ä½³æ¨¡å‹
best_model_name = max(results.items(), key=lambda x: x[1]['accuracy'])[0]
print(f"\nğŸ† æœ€ä½³æ¨¡å‹: {best_model_name}")
print(f"   å‡†ç¡®ç‡: {results[best_model_name]['accuracy']:.4f}")

# ============================================================================
# 7. å®é™…åº”ç”¨ç¤ºä¾‹ / Practical Examples
# ============================================================================
print("\nã€7ã€‘å®é™…åº”ç”¨ç¤ºä¾‹")
print("=" * 80)

# ä½¿ç”¨æœ€ä½³æ¨¡å‹è¿›è¡Œé¢„æµ‹
best_model = models[best_model_name]

# æ–°æ–‡æœ¬ç¤ºä¾‹
new_texts = [
    "Python machine learning libraries are very powerful",
    "The documentary was inspiring and well-made",
    "Terrible customer service and low quality product"
]

print("\nä½¿ç”¨æœ€ä½³æ¨¡å‹é¢„æµ‹æ–°æ–‡æœ¬:")
print("-" * 80)

if best_model_name == 'Naive Bayes':
    new_features = bow_vectorizer.transform(new_texts)
else:
    new_features = tfidf_vectorizer.transform(new_texts)

new_predictions = best_model.predict(new_features)

# è·å–é¢„æµ‹æ¦‚ç‡
if hasattr(best_model, 'predict_proba'):
    new_probas = best_model.predict_proba(new_features)
elif hasattr(best_model, 'decision_function'):
    # SVM ä½¿ç”¨ decision_function
    decision_scores = best_model.decision_function(new_features)
    # ç®€å•å½’ä¸€åŒ–
    new_probas = np.exp(decision_scores) / np.exp(decision_scores).sum(axis=1, keepdims=True)
else:
    new_probas = None

for i, text in enumerate(new_texts):
    pred_class = target_names[new_predictions[i]]
    print(f"\næ–‡æœ¬ {i+1}: {text}")
    print(f"  é¢„æµ‹ç±»åˆ«: {pred_class}")

    if new_probas is not None:
        print(f"  ç±»åˆ«æ¦‚ç‡:")
        for j, class_name in enumerate(target_names):
            print(f"    {class_name}: {new_probas[i][j]:.4f}")

# ============================================================================
# 8. é”™è¯¯åˆ†æ / Error Analysis
# ============================================================================
print("\nã€8ã€‘é”™è¯¯åˆ†æ")
print("=" * 80)

# æ‰¾å‡ºåˆ†ç±»é”™è¯¯çš„æ ·æœ¬
y_pred_best = results[best_model_name]['predictions']
errors = np.where(y_pred_best != y_test)[0]

print(f"é”™è¯¯æ ·æœ¬æ•°: {len(errors)} / {len(y_test)} ({len(errors)/len(y_test)*100:.1f}%)")

if len(errors) > 0:
    print(f"\nå‰ 3 ä¸ªé”™è¯¯æ ·æœ¬:")
    for i, error_idx in enumerate(errors[:3], 1):
        true_label = target_names[y_test[error_idx]]
        pred_label = target_names[y_pred_best[error_idx]]
        text = X_test_text[error_idx][:150]

        print(f"\né”™è¯¯ {i}:")
        print(f"  æ–‡æœ¬: {text}...")
        print(f"  çœŸå®ç±»åˆ«: {true_label}")
        print(f"  é¢„æµ‹ç±»åˆ«: {pred_label}")

# ============================================================================
# 9. å¯è§†åŒ– / Visualization
# ============================================================================
print("\nã€9ã€‘æ¨¡å‹æ€§èƒ½å¯è§†åŒ–")
print("=" * 80)

# 9.1 æ•°æ®é›†ç±»åˆ«åˆ†å¸ƒ
fig, axes = create_subplots(2, 2, figsize=(16, 12))

# ç±»åˆ«åˆ†å¸ƒ
unique_train, counts_train = np.unique(y_train, return_counts=True)
axes[0, 0].bar([target_names[i] for i in unique_train], counts_train,
              color='steelblue', alpha=0.7, edgecolor='black')
axes[0, 0].set_xlabel('ç±»åˆ« / Category', fontsize=10)
axes[0, 0].set_ylabel('æ ·æœ¬æ•° / Count', fontsize=10)
axes[0, 0].set_title('è®­ç»ƒé›†ç±»åˆ«åˆ†å¸ƒ\nTraining Set Class Distribution',
                     fontsize=12, fontweight='bold')
axes[0, 0].tick_params(axis='x', rotation=45)

for i, v in enumerate(counts_train):
    axes[0, 0].text(i, v + 0.5, str(v), ha='center', va='bottom',
                   fontsize=10, fontweight='bold')

# æ–‡æœ¬é•¿åº¦åˆ†å¸ƒ
axes[0, 1].hist(text_lengths, bins=30, color='coral', alpha=0.7, edgecolor='black')
axes[0, 1].set_xlabel('æ–‡æœ¬é•¿åº¦ï¼ˆè¯æ•°ï¼‰/ Text Length (words)', fontsize=10)
axes[0, 1].set_ylabel('é¢‘æ•° / Frequency', fontsize=10)
axes[0, 1].set_title('æ–‡æœ¬é•¿åº¦åˆ†å¸ƒ\nText Length Distribution',
                     fontsize=12, fontweight='bold')
axes[0, 1].axvline(np.mean(text_lengths), color='red', linestyle='--',
                   label=f'å¹³å‡å€¼: {np.mean(text_lengths):.1f}')
axes[0, 1].legend()

# æ¨¡å‹å‡†ç¡®ç‡å¯¹æ¯”
model_names = list(results.keys())
accuracies = [results[name]['accuracy'] for name in model_names]

colors_bar = ['steelblue', 'coral', 'lightgreen', 'mediumpurple', 'gold'][:len(model_names)]
bars = axes[1, 0].bar(model_names, accuracies, color=colors_bar,
                      alpha=0.7, edgecolor='black')

# é«˜äº®æœ€ä½³æ¨¡å‹
best_idx = accuracies.index(max(accuracies))
bars[best_idx].set_edgecolor('red')
bars[best_idx].set_linewidth(3)

axes[1, 0].set_xlabel('æ¨¡å‹ / Model', fontsize=10)
axes[1, 0].set_ylabel('å‡†ç¡®ç‡ / Accuracy', fontsize=10)
axes[1, 0].set_title('æ¨¡å‹å‡†ç¡®ç‡å¯¹æ¯”\nModel Accuracy Comparison',
                     fontsize=12, fontweight='bold')
axes[1, 0].set_ylim([0, 1.1])
axes[1, 0].tick_params(axis='x', rotation=45)

for i, v in enumerate(accuracies):
    axes[1, 0].text(i, v + 0.02, f'{v:.3f}', ha='center', va='bottom',
                   fontsize=10, fontweight='bold')

# è®­ç»ƒ/é¢„æµ‹æ—¶é—´å¯¹æ¯”
x_pos = np.arange(len(model_names))
width = 0.35

train_times = [training_times[name] for name in model_names]
pred_times = [prediction_times[name] for name in model_names]

axes[1, 1].bar(x_pos - width/2, train_times, width, label='è®­ç»ƒæ—¶é—´',
              color='skyblue', alpha=0.8)
axes[1, 1].bar(x_pos + width/2, pred_times, width, label='é¢„æµ‹æ—¶é—´',
              color='lightcoral', alpha=0.8)

axes[1, 1].set_xlabel('æ¨¡å‹ / Model', fontsize=10)
axes[1, 1].set_ylabel('æ—¶é—´ (ç§’) / Time (seconds)', fontsize=10)
axes[1, 1].set_title('è®­ç»ƒ/é¢„æµ‹æ—¶é—´å¯¹æ¯”\nTraining/Prediction Time Comparison',
                     fontsize=12, fontweight='bold')
axes[1, 1].set_xticks(x_pos)
axes[1, 1].set_xticklabels(model_names, rotation=45, ha='right')
axes[1, 1].legend(fontsize=10)
axes[1, 1].grid(axis='y', alpha=0.3)

plt.tight_layout()
save_figure(fig, get_output_path('nlp_classification_overview.png'))
print("âœ“ å›¾è¡¨å·²ä¿å­˜: output/nlp_classification_overview.png")
plt.show()

# 9.2 æ··æ·†çŸ©é˜µå¯¹æ¯”
n_models = len(model_names)
n_rows = (n_models + 1) // 2
n_cols = 2

fig, axes = create_subplots(n_rows, n_cols, figsize=(14, 6*n_rows))

if n_models == 1:
    axes = np.array([axes])

axes_flat = axes.flatten() if n_models > 1 else axes

for idx, model_name in enumerate(model_names):
    y_pred = results[model_name]['predictions']
    cm = confusion_matrix(y_test, y_pred)

    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=target_names, yticklabels=target_names,
                cbar_kws={'label': 'æ ·æœ¬æ•° / Count'},
                ax=axes_flat[idx])

    axes_flat[idx].set_title(f'{model_name}\nå‡†ç¡®ç‡: {results[model_name]["accuracy"]:.3f}',
                            fontsize=12, fontweight='bold')
    axes_flat[idx].set_xlabel('é¢„æµ‹ç±»åˆ« / Predicted', fontsize=10)
    axes_flat[idx].set_ylabel('çœŸå®ç±»åˆ« / Actual', fontsize=10)

# éšè—å¤šä½™çš„å­å›¾
for idx in range(n_models, len(axes_flat)):
    axes_flat[idx].axis('off')

plt.tight_layout()
save_figure(fig, get_output_path('nlp_confusion_matrices.png'))
print("âœ“ å›¾è¡¨å·²ä¿å­˜: output/nlp_confusion_matrices.png")
plt.show()

# 9.3 ç‰¹å¾é‡è¦æ€§ï¼ˆRandom Forestï¼‰
if 'Random Forest' in models and hasattr(models['Random Forest'], 'feature_importances_'):
    fig, ax = create_subplots(1, 1, figsize=(12, 8))

    feature_importances = models['Random Forest'].feature_importances_
    top_indices = feature_importances.argsort()[-20:][::-1]
    top_features = [tfidf_vectorizer.get_feature_names_out()[i] for i in top_indices]
    top_importance = feature_importances[top_indices]

    ax.barh(range(len(top_features)), top_importance, color='mediumpurple', alpha=0.7)
    ax.set_yticks(range(len(top_features)))
    ax.set_yticklabels(top_features)
    ax.set_xlabel('é‡è¦æ€§ / Importance', fontsize=11)
    ax.set_title('Random Forest - Top 20 ç‰¹å¾é‡è¦æ€§\nRandom Forest - Top 20 Feature Importances',
                 fontsize=13, fontweight='bold')
    ax.invert_yaxis()
    ax.grid(axis='x', alpha=0.3)

    plt.tight_layout()
    save_figure(fig, get_output_path('nlp_feature_importance.png'))
    print("âœ“ å›¾è¡¨å·²ä¿å­˜: output/nlp_feature_importance.png")
    plt.show()

# 9.4 æ¨¡å‹æ€§èƒ½é›·è¾¾å›¾
fig, ax = create_subplots(1, 1, figsize=(10, 10))

# è®¡ç®—å„æ¨¡å‹çš„ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1
metrics = {}
for model_name in model_names:
    y_pred = results[model_name]['predictions']
    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')
    accuracy = results[model_name]['accuracy']

    # å½’ä¸€åŒ–è®­ç»ƒæ—¶é—´ï¼ˆè¶Šå¿«è¶Šå¥½ï¼Œæ‰€ä»¥ç”¨å€’æ•°ï¼‰
    max_train_time = max(training_times.values())
    normalized_speed = 1 - (training_times[model_name] / max_train_time)

    metrics[model_name] = {
        'Accuracy': accuracy,
        'Precision': precision,
        'Recall': recall,
        'F1-Score': f1,
        'Speed': normalized_speed
    }

# ç»˜åˆ¶é›·è¾¾å›¾ï¼ˆä»…ç»˜åˆ¶å‰3ä¸ªæ¨¡å‹ï¼Œé¿å…è¿‡äºæ‹¥æŒ¤ï¼‰
categories = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'Speed']
N = len(categories)

angles = [n / float(N) * 2 * np.pi for n in range(N)]
angles += angles[:1]

ax = plt.subplot(111, projection='polar')

colors_radar = ['steelblue', 'coral', 'lightgreen', 'mediumpurple', 'gold']

for idx, model_name in enumerate(model_names[:3]):  # åªæ˜¾ç¤ºå‰3ä¸ª
    values = [metrics[model_name][cat] for cat in categories]
    values += values[:1]

    ax.plot(angles, values, 'o-', linewidth=2, label=model_name,
            color=colors_radar[idx])
    ax.fill(angles, values, alpha=0.15, color=colors_radar[idx])

ax.set_xticks(angles[:-1])
ax.set_xticklabels(categories, fontsize=10)
ax.set_ylim(0, 1)
ax.set_title('æ¨¡å‹æ€§èƒ½é›·è¾¾å›¾ï¼ˆå‰3ä¸ªæ¨¡å‹ï¼‰\nModel Performance Radar Chart',
             fontsize=13, fontweight='bold', pad=20)
ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0), fontsize=10)
ax.grid(True)

plt.tight_layout()
save_figure(fig, get_output_path('nlp_performance_radar.png'))
print("âœ“ å›¾è¡¨å·²ä¿å­˜: output/nlp_performance_radar.png")
plt.show()

# 9.5 å­¦ä¹ æ›²çº¿ï¼ˆæœ€ä½³æ¨¡å‹ï¼‰
if DATA_LOADED and len(X_train_text) > 100:
    print("\nç”Ÿæˆå­¦ä¹ æ›²çº¿ï¼ˆå¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼‰...")

    fig, ax = create_subplots(1, 1, figsize=(12, 8))

    # ä½¿ç”¨ Pipeline
    if best_model_name == 'Naive Bayes':
        pipeline = Pipeline([
            ('vectorizer', bow_vectorizer),
            ('classifier', MultinomialNB(alpha=0.1))
        ])
        X_for_curve = X_train_text
    else:
        pipeline = Pipeline([
            ('vectorizer', tfidf_vectorizer),
            ('classifier', models[best_model_name])
        ])
        X_for_curve = X_train_text

    train_sizes = np.linspace(0.1, 1.0, 10)

    try:
        train_sizes_abs, train_scores, val_scores = learning_curve(
            pipeline, X_for_curve, y_train,
            train_sizes=train_sizes,
            cv=3,
            n_jobs=-1,
            random_state=RANDOM_STATE
        )

        train_mean = np.mean(train_scores, axis=1)
        train_std = np.std(train_scores, axis=1)
        val_mean = np.mean(val_scores, axis=1)
        val_std = np.std(val_scores, axis=1)

        ax.plot(train_sizes_abs, train_mean, 'o-', color='steelblue',
                label='è®­ç»ƒé›†å¾—åˆ†', linewidth=2)
        ax.fill_between(train_sizes_abs, train_mean - train_std,
                        train_mean + train_std, alpha=0.2, color='steelblue')

        ax.plot(train_sizes_abs, val_mean, 'o-', color='coral',
                label='éªŒè¯é›†å¾—åˆ†', linewidth=2)
        ax.fill_between(train_sizes_abs, val_mean - val_std,
                        val_mean + val_std, alpha=0.2, color='coral')

        ax.set_xlabel('è®­ç»ƒæ ·æœ¬æ•° / Training Set Size', fontsize=11)
        ax.set_ylabel('å¾—åˆ† / Score', fontsize=11)
        ax.set_title(f'å­¦ä¹ æ›²çº¿ - {best_model_name}\nLearning Curve',
                     fontsize=13, fontweight='bold')
        ax.legend(loc='lower right', fontsize=10)
        ax.grid(True, alpha=0.3)

        plt.tight_layout()
        save_figure(fig, get_output_path('nlp_learning_curve.png'))
        print("âœ“ å›¾è¡¨å·²ä¿å­˜: output/nlp_learning_curve.png")
        plt.show()
    except Exception as e:
        print(f"âœ— å­¦ä¹ æ›²çº¿ç”Ÿæˆå¤±è´¥: {e}")

# 9.6 ç»¼åˆæ€§èƒ½å¯¹æ¯”å›¾
fig, ax = create_subplots(1, 1, figsize=(14, 8))

# å‡†å¤‡æ•°æ®
performance_data = []
for model_name in model_names:
    y_pred = results[model_name]['predictions']
    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')

    performance_data.append({
        'Model': model_name,
        'Accuracy': results[model_name]['accuracy'],
        'Precision': precision,
        'Recall': recall,
        'F1-Score': f1
    })

df_performance = pd.DataFrame(performance_data)

x = np.arange(len(model_names))
width = 0.2

ax.bar(x - 1.5*width, df_performance['Accuracy'], width, label='Accuracy',
       color='steelblue', alpha=0.8)
ax.bar(x - 0.5*width, df_performance['Precision'], width, label='Precision',
       color='coral', alpha=0.8)
ax.bar(x + 0.5*width, df_performance['Recall'], width, label='Recall',
       color='lightgreen', alpha=0.8)
ax.bar(x + 1.5*width, df_performance['F1-Score'], width, label='F1-Score',
       color='mediumpurple', alpha=0.8)

ax.set_xlabel('æ¨¡å‹ / Model', fontsize=11)
ax.set_ylabel('å¾—åˆ† / Score', fontsize=11)
ax.set_title('æ¨¡å‹ç»¼åˆæ€§èƒ½å¯¹æ¯”\nComprehensive Model Performance Comparison',
             fontsize=13, fontweight='bold')
ax.set_xticks(x)
ax.set_xticklabels(model_names, rotation=45, ha='right')
ax.legend(fontsize=10, loc='lower right')
ax.set_ylim([0, 1.1])
ax.grid(axis='y', alpha=0.3)

plt.tight_layout()
save_figure(fig, get_output_path('nlp_comprehensive_performance.png'))
print("âœ“ å›¾è¡¨å·²ä¿å­˜: output/nlp_comprehensive_performance.png")
plt.show()

# ============================================================================
# 10. æ€»ç»“ä¸æœ€ä½³å®è·µ / Summary and Best Practices
# ============================================================================
print("\nã€10ã€‘æ€»ç»“ä¸æœ€ä½³å®è·µ")
print("=" * 80)
print(f"""
æœ¬æ•™ç¨‹æ¶µç›–çš„å†…å®¹ï¼š
Topics Covered:

âœ“ æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ä»‹ç»
  â€¢ åº”ç”¨åœºæ™¯ï¼šåƒåœ¾é‚®ä»¶æ£€æµ‹ã€æƒ…æ„Ÿåˆ†æã€æ–°é—»åˆ†ç±»ç­‰

âœ“ æ•°æ®å‡†å¤‡ä¸æ¢ç´¢
  â€¢ 20 Newsgroups æ•°æ®é›†
  â€¢ æ•°æ®åˆ†å¸ƒåˆ†æ

âœ“ ç‰¹å¾æå–
  â€¢ TF-IDF å’Œ Bag of Words
  â€¢ è¯æ±‡è¡¨æ„å»ºå’Œå‚æ•°è°ƒä¼˜

âœ“ ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹
  â€¢ Logistic Regression
  â€¢ Naive Bayes
  â€¢ SVM
  â€¢ Random Forest
  {'â€¢ MLP (Multi-Layer Perceptron)' if MLP_AVAILABLE else ''}

âœ“ æ¨¡å‹è¯„ä¼°ä¸å¯¹æ¯”
  â€¢ å‡†ç¡®ç‡ã€ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1-score
  â€¢ æ··æ·†çŸ©é˜µ
  â€¢ å­¦ä¹ æ›²çº¿

âœ“ å®é™…åº”ç”¨ä¸é”™è¯¯åˆ†æ
  â€¢ æ–°æ–‡æœ¬é¢„æµ‹
  â€¢ é”™è¯¯æ ·æœ¬åˆ†æ

æœ€ä½³å®è·µå»ºè®®ï¼š
Best Practices:

ğŸ“ æ•°æ®é¢„å¤„ç†
  1. å»é™¤ HTML æ ‡ç­¾å’Œç‰¹æ®Šå­—ç¬¦
  2. ç»Ÿä¸€æ–‡æœ¬æ ¼å¼ï¼ˆå°å†™ï¼‰
  3. å»é™¤åœç”¨è¯ï¼ˆæ ¹æ®ä»»åŠ¡å†³å®šï¼‰
  4. è¯å½¢å½’ä¸€åŒ–ï¼ˆLemmatizationï¼‰

ğŸ”§ ç‰¹å¾å·¥ç¨‹
  1. ä½¿ç”¨ TF-IDF è€Œéç®€å•çš„è¯é¢‘
  2. è€ƒè™‘ n-gramsï¼ˆbigrams, trigramsï¼‰
  3. è®¾ç½®åˆç†çš„ min_df å’Œ max_df
  4. é™åˆ¶ç‰¹å¾æ•°é‡ï¼ˆmax_featuresï¼‰

ğŸ¯ æ¨¡å‹é€‰æ‹©
  1. å¿«é€ŸåŸå‹ï¼šNaive Bayes
  2. å¹³è¡¡æ€§èƒ½ï¼šLogistic Regression æˆ– SVM
  3. éœ€è¦å¯è§£é‡Šæ€§ï¼šLogistic Regression
  4. è¿½æ±‚æ€§èƒ½ï¼šé›†æˆæ–¹æ³•æˆ–æ·±åº¦å­¦ä¹ 

ğŸ“Š æ¨¡å‹è¯„ä¼°
  1. ä½¿ç”¨å¤šä¸ªè¯„ä¼°æŒ‡æ ‡
  2. åˆ†ææ··æ·†çŸ©é˜µ
  3. è¿›è¡Œäº¤å‰éªŒè¯
  4. æ£€æŸ¥å­¦ä¹ æ›²çº¿

âš¡ æ€§èƒ½ä¼˜åŒ–
  1. Pipeline åŒ–å·¥ä½œæµç¨‹
  2. ä½¿ç”¨ n_jobs=-1 å¹¶è¡Œè®¡ç®—
  3. å¢é‡å­¦ä¹ ï¼ˆå¤§æ•°æ®é›†ï¼‰
  4. ç‰¹å¾é€‰æ‹©å‡å°‘ç»´åº¦

ğŸ” å®é™…åº”ç”¨
  1. ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹å’Œå‘é‡åŒ–å™¨
  2. è®¾ç½®åˆç†çš„é¢„æµ‹é˜ˆå€¼
  3. æŒç»­ç›‘æ§æ¨¡å‹æ€§èƒ½
  4. å®šæœŸæ›´æ–°æ¨¡å‹

æ¨¡å‹æ€§èƒ½æ€»ç»“ï¼š
Model Performance Summary:
""")

for model_name in model_names:
    print(f"\n{model_name}:")
    print(f"  âœ“ å‡†ç¡®ç‡: {results[model_name]['accuracy']:.4f}")
    print(f"  âœ“ è®­ç»ƒæ—¶é—´: {training_times[model_name]:.3f} ç§’")
    print(f"  âœ“ é¢„æµ‹æ—¶é—´: {prediction_times[model_name]:.3f} ç§’")

print(f"\nğŸ† æ¨èæ¨¡å‹: {best_model_name}")
print(f"   ç†ç”±: åœ¨å‡†ç¡®ç‡å’Œæ•ˆç‡ä¹‹é—´å–å¾—æœ€ä½³å¹³è¡¡")

print("""
ä¸‹ä¸€æ­¥å­¦ä¹ ï¼š
Next Steps:

1. å°è¯•æ·±åº¦å­¦ä¹ æ–¹æ³•ï¼ˆLSTM, BERTï¼‰
2. å¤šæ ‡ç­¾åˆ†ç±»
3. ä¸å¹³è¡¡æ•°æ®å¤„ç†
4. æ¨¡å‹éƒ¨ç½²å’ŒæœåŠ¡åŒ–
5. åœ¨çº¿å­¦ä¹ å’Œå¢é‡æ›´æ–°
""")

print("\n" + "=" * 80)
print("æ–‡æœ¬åˆ†ç±»æ•™ç¨‹å®Œæˆï¼".center(80))
print("Text Classification Tutorial Complete!".center(80))
print("=" * 80)
