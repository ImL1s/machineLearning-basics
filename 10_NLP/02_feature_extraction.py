"""
æ–‡æœ¬ç‰¹å¾æå–ï¼ˆText Feature Extractionï¼‰
å°†æ–‡æœ¬è½¬æ¢ä¸ºæ•°å€¼ç‰¹å¾å‘é‡

Text Feature Extraction - Converting Text to Numerical Features

ç‰¹å¾æå–æ˜¯ NLP çš„æ ¸å¿ƒæ­¥éª¤ï¼Œå†³å®šäº†æ¨¡å‹èƒ½å¦ç†è§£æ–‡æœ¬è¯­ä¹‰
Feature extraction is crucial for enabling machines to understand text
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
import warnings
warnings.filterwarnings('ignore')

from utils import RANDOM_STATE, setup_chinese_fonts, create_subplots

# å°è¯•å¯¼å…¥ Gensimï¼ˆWord2Vecï¼‰
try:
    from gensim.models import Word2Vec
    from gensim.models.word2vec import LineSentence
    GENSIM_AVAILABLE = True
except ImportError:
    GENSIM_AVAILABLE = False
    print("âš  Gensim æœªå®‰è£…ã€‚è¿è¡Œ: pip install gensim")

# å°è¯•å¯¼å…¥ NLTK
try:
    import nltk
    from nltk.tokenize import word_tokenize
    NLTK_AVAILABLE = True
except ImportError:
    NLTK_AVAILABLE = False
    print("âš  NLTK æœªå®‰è£…ã€‚è¿è¡Œ: pip install nltk")

setup_chinese_fonts()

print("=" * 80)
print("æ–‡æœ¬ç‰¹å¾æå–ï¼ˆText Feature Extractionï¼‰æ•™ç¨‹".center(80))
print("=" * 80)

# ============================================================================
# 1. ç‰¹å¾æå–æ–¹æ³•æ¦‚è¿° / Feature Extraction Overview
# ============================================================================
print("\nã€1ã€‘ç‰¹å¾æå–æ–¹æ³•æ¦‚è¿°")
print("-" * 80)
print("""
ä¸ºä»€ä¹ˆéœ€è¦ç‰¹å¾æå–ï¼Ÿ
Why Feature Extraction?
â€¢ æœºå™¨å­¦ä¹ ç®—æ³•åªèƒ½å¤„ç†æ•°å€¼æ•°æ®
  Machine learning algorithms work with numbers, not text
â€¢ éœ€è¦å°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡è¡¨ç¤º
  Need to convert text into vector representations

ä¸»è¦æ–¹æ³•ï¼š
Main Methods:

1. Bag of Words (BoW) - è¯è¢‹æ¨¡å‹
   â€¢ ç»Ÿè®¡è¯é¢‘
   â€¢ å¿½ç•¥è¯åºå’Œè¯­æ³•
   â€¢ ç®€å•é«˜æ•ˆ

2. TF-IDF (Term Frequency-Inverse Document Frequency)
   â€¢ è€ƒè™‘è¯çš„é‡è¦æ€§
   â€¢ é™ä½é«˜é¢‘å¸¸ç”¨è¯çš„æƒé‡
   â€¢ æå‡åŒºåˆ†æ€§è¯æ±‡çš„æƒé‡

3. Word Embeddings - è¯åµŒå…¥
   â€¢ Word2Vec, GloVe, FastText
   â€¢ æ•æ‰è¯ä¹‰å’Œä¸Šä¸‹æ–‡
   â€¢ ä½ç»´ç¨ å¯†å‘é‡

æ–¹æ³•å¯¹æ¯”ï¼š
Comparison:

ç‰¹æ€§          | BoW      | TF-IDF   | Word2Vec
-------------|----------|----------|----------
ç»´åº¦         | é«˜ï¼ˆç¨€ç–ï¼‰ | é«˜ï¼ˆç¨€ç–ï¼‰ | ä½ï¼ˆç¨ å¯†ï¼‰
è¯­ä¹‰ä¿¡æ¯     | æ—         | å°‘        | ä¸°å¯Œ
è®­ç»ƒæ—¶é—´     | å¿«        | å¿«        | æ…¢
å†…å­˜å ç”¨     | å¤§        | å¤§        | å°
é€‚ç”¨åœºæ™¯     | åˆ†ç±»      | æ£€ç´¢/åˆ†ç±» | è¯­ä¹‰ä»»åŠ¡
""")

# ============================================================================
# 2. å‡†å¤‡ç¤ºä¾‹æ•°æ® / Prepare Sample Data
# ============================================================================
print("\nã€2ã€‘å‡†å¤‡ç¤ºä¾‹æ•°æ®")
print("-" * 80)

# åˆ›å»ºæ–‡æ¡£è¯­æ–™åº“
documents = [
    "Machine learning is a fascinating field of artificial intelligence.",
    "Deep learning uses neural networks to learn from data.",
    "Natural language processing helps computers understand human language.",
    "Text mining extracts useful information from text data.",
    "Data science combines statistics and machine learning.",
    "Neural networks are the foundation of deep learning.",
    "Artificial intelligence is transforming many industries.",
    "Text classification is a common NLP task.",
    "Machine learning algorithms learn patterns from data.",
    "Deep neural networks can solve complex problems.",
]

print("æ–‡æ¡£è¯­æ–™åº“:")
for i, doc in enumerate(documents, 1):
    print(f"{i}. {doc}")

print(f"\næ€»æ–‡æ¡£æ•°: {len(documents)}")

# ============================================================================
# 3. è¯è¢‹æ¨¡å‹ï¼ˆBag of Wordsï¼‰
# ============================================================================
print("\nã€3ã€‘è¯è¢‹æ¨¡å‹ï¼ˆBag of Wordsï¼‰")
print("-" * 80)
print("""
è¯è¢‹æ¨¡å‹åŸç†ï¼š
Bag of Words Principle:

1. å»ºç«‹è¯æ±‡è¡¨ï¼ˆæ‰€æœ‰æ–‡æ¡£çš„å”¯ä¸€è¯æ±‡ï¼‰
   Build vocabulary (all unique words in all documents)

2. ç»Ÿè®¡æ¯ä¸ªæ–‡æ¡£ä¸­æ¯ä¸ªè¯çš„å‡ºç°æ¬¡æ•°
   Count word occurrences in each document

3. ç”Ÿæˆæ–‡æ¡£-è¯é¢‘çŸ©é˜µ
   Generate document-term matrix

ç‰¹ç‚¹ï¼š
â€¢ ç®€å•ç›´è§‚
â€¢ å¿½ç•¥è¯åº
â€¢ é«˜ç»´ç¨€ç–
""")

# 3.1 åŸºç¡€ CountVectorizer
print("\nã€3.1ã€‘åŸºç¡€ CountVectorizer")

vectorizer_basic = CountVectorizer()
bow_matrix = vectorizer_basic.fit_transform(documents)

print(f"è¯æ±‡è¡¨å¤§å°: {len(vectorizer_basic.vocabulary_)}")
print(f"çŸ©é˜µå½¢çŠ¶: {bow_matrix.shape}")
print(f"ç¨€ç–åº¦: {(1 - bow_matrix.nnz / (bow_matrix.shape[0] * bow_matrix.shape[1])) * 100:.2f}%")

# æŸ¥çœ‹éƒ¨åˆ†è¯æ±‡è¡¨
vocab = vectorizer_basic.get_feature_names_out()
print(f"\néƒ¨åˆ†è¯æ±‡è¡¨ï¼ˆå‰20ä¸ªï¼‰:")
print(vocab[:20])

# æŸ¥çœ‹ç¬¬ä¸€ä¸ªæ–‡æ¡£çš„å‘é‡è¡¨ç¤º
print(f"\nç¬¬ä¸€ä¸ªæ–‡æ¡£çš„å‘é‡è¡¨ç¤º:")
print(f"æ–‡æ¡£: {documents[0]}")
print(f"å‘é‡ï¼ˆéé›¶å…ƒç´ ï¼‰:")
doc_vector = bow_matrix[0].toarray()[0]
for idx, count in enumerate(doc_vector):
    if count > 0:
        print(f"  {vocab[idx]}: {count}")

# 3.2 å‚æ•°è°ƒä¼˜
print("\nã€3.2ã€‘CountVectorizer å‚æ•°è°ƒä¼˜")

# é™åˆ¶è¯æ±‡è¡¨å¤§å°
vectorizer_limited = CountVectorizer(max_features=20)
bow_limited = vectorizer_limited.fit_transform(documents)
print(f"\né™åˆ¶ max_features=20:")
print(f"è¯æ±‡è¡¨å¤§å°: {len(vectorizer_limited.vocabulary_)}")

# è®¾ç½® min_df å’Œ max_df
vectorizer_filtered = CountVectorizer(min_df=2, max_df=0.8)
bow_filtered = vectorizer_filtered.fit_transform(documents)
print(f"\nmin_df=2, max_df=0.8:")
print(f"è¯æ±‡è¡¨å¤§å°: {len(vectorizer_filtered.vocabulary_)}")
print(f"è¿‡æ»¤æ‰çš„è¯: {len(vectorizer_basic.vocabulary_) - len(vectorizer_filtered.vocabulary_)}")

# 3.3 N-grams
print("\nã€3.3ã€‘N-grams")

# Bigrams
vectorizer_bigram = CountVectorizer(ngram_range=(1, 2), max_features=30)
bow_bigram = vectorizer_bigram.fit_transform(documents)
print(f"\nBigrams (1-2 grams):")
print(f"ç‰¹å¾æ•°: {bow_bigram.shape[1]}")
bigram_features = vectorizer_bigram.get_feature_names_out()
print(f"ç¤ºä¾‹ç‰¹å¾:")
print(bigram_features[:20])

# ============================================================================
# 4. TF-IDF
# ============================================================================
print("\nã€4ã€‘TF-IDF (Term Frequency-Inverse Document Frequency)")
print("-" * 80)
print("""
TF-IDF åŸç†ï¼š
TF-IDF Formula:

TF-IDF(t, d) = TF(t, d) Ã— IDF(t)

å…¶ä¸­ï¼š
â€¢ TF(t, d) = è¯ t åœ¨æ–‡æ¡£ d ä¸­çš„é¢‘ç‡
  Term Frequency: frequency of term t in document d

â€¢ IDF(t) = log(æ€»æ–‡æ¡£æ•° / åŒ…å«è¯ t çš„æ–‡æ¡£æ•°)
  Inverse Document Frequency: log(total docs / docs containing t)

ç›´è§‰ï¼š
Intuition:
â€¢ è¯åœ¨æ–‡æ¡£ä¸­å‡ºç°è¶Šå¤šï¼ŒTF è¶Šå¤§ï¼ˆé‡è¦ï¼‰
  More frequent in document â†’ higher TF â†’ important

â€¢ è¯åœ¨è¶Šå¤šæ–‡æ¡£ä¸­å‡ºç°ï¼ŒIDF è¶Šå°ï¼ˆä¸é‡è¦ï¼‰
  Appears in more documents â†’ lower IDF â†’ less distinctive

â€¢ TF-IDF å¹³è¡¡äº†è¯é¢‘å’Œç¨€æœ‰åº¦
  TF-IDF balances frequency and rarity
""")

# 4.1 åŸºç¡€ TF-IDF
print("\nã€4.1ã€‘åŸºç¡€ TfidfVectorizer")

tfidf_vectorizer = TfidfVectorizer()
tfidf_matrix = tfidf_vectorizer.fit_transform(documents)

print(f"TF-IDF çŸ©é˜µå½¢çŠ¶: {tfidf_matrix.shape}")
print(f"è¯æ±‡è¡¨å¤§å°: {len(tfidf_vectorizer.vocabulary_)}")

# æŸ¥çœ‹ç¬¬ä¸€ä¸ªæ–‡æ¡£çš„ TF-IDF å€¼
print(f"\nç¬¬ä¸€ä¸ªæ–‡æ¡£çš„ TF-IDF å€¼ï¼ˆéé›¶ï¼‰:")
doc_tfidf = tfidf_matrix[0].toarray()[0]
tfidf_vocab = tfidf_vectorizer.get_feature_names_out()
word_tfidf = [(tfidf_vocab[i], doc_tfidf[i]) for i in range(len(doc_tfidf)) if doc_tfidf[i] > 0]
word_tfidf_sorted = sorted(word_tfidf, key=lambda x: x[1], reverse=True)

for word, score in word_tfidf_sorted:
    print(f"  {word}: {score:.4f}")

# 4.2 æå–æ¯ä¸ªæ–‡æ¡£çš„é‡è¦è¯æ±‡
print("\nã€4.2ã€‘æ¯ä¸ªæ–‡æ¡£çš„ Top 3 é‡è¦è¯æ±‡")

for i, doc in enumerate(documents):
    doc_tfidf_vector = tfidf_matrix[i].toarray()[0]
    word_scores = [(tfidf_vocab[j], doc_tfidf_vector[j])
                   for j in range(len(doc_tfidf_vector)) if doc_tfidf_vector[j] > 0]
    word_scores_sorted = sorted(word_scores, key=lambda x: x[1], reverse=True)

    print(f"\næ–‡æ¡£ {i+1}: {doc[:50]}...")
    print(f"  Top 3: {[(w, f'{s:.3f}') for w, s in word_scores_sorted[:3]]}")

# 4.3 æ–‡æ¡£ç›¸ä¼¼åº¦è®¡ç®—
print("\nã€4.3ã€‘æ–‡æ¡£ç›¸ä¼¼åº¦è®¡ç®—ï¼ˆCosine Similarityï¼‰")

# è®¡ç®—æ‰€æœ‰æ–‡æ¡£å¯¹çš„ä½™å¼¦ç›¸ä¼¼åº¦
similarity_matrix = cosine_similarity(tfidf_matrix)

print(f"ç›¸ä¼¼åº¦çŸ©é˜µå½¢çŠ¶: {similarity_matrix.shape}")
print(f"\næ–‡æ¡£ 1 ä¸å…¶ä»–æ–‡æ¡£çš„ç›¸ä¼¼åº¦:")
for i in range(len(documents)):
    if i != 0:
        print(f"  æ–‡æ¡£ {i+1}: {similarity_matrix[0, i]:.4f}")

# æ‰¾å‡ºæœ€ç›¸ä¼¼çš„æ–‡æ¡£å¯¹
print("\næœ€ç›¸ä¼¼çš„æ–‡æ¡£å¯¹ï¼ˆTop 5ï¼‰:")
similarities = []
for i in range(len(documents)):
    for j in range(i+1, len(documents)):
        similarities.append((i, j, similarity_matrix[i, j]))

similarities_sorted = sorted(similarities, key=lambda x: x[2], reverse=True)
for i, j, sim in similarities_sorted[:5]:
    print(f"\næ–‡æ¡£ {i+1} vs æ–‡æ¡£ {j+1}: {sim:.4f}")
    print(f"  æ–‡æ¡£ {i+1}: {documents[i][:60]}...")
    print(f"  æ–‡æ¡£ {j+1}: {documents[j][:60]}...")

# ============================================================================
# 5. Word2Vec
# ============================================================================
print("\nã€5ã€‘Word2Vec è¯åµŒå…¥")
print("-" * 80)
print("""
Word2Vec åŸç†ï¼š
Word2Vec Principle:

ä¸¤ç§è®­ç»ƒæ¨¡å¼ï¼š
Two Training Architectures:

1. CBOW (Continuous Bag of Words)
   â€¢ é€šè¿‡ä¸Šä¸‹æ–‡é¢„æµ‹ä¸­å¿ƒè¯
   â€¢ é€‚åˆå°æ•°æ®é›†
   â€¢ è®­ç»ƒé€Ÿåº¦å¿«

2. Skip-gram
   â€¢ é€šè¿‡ä¸­å¿ƒè¯é¢„æµ‹ä¸Šä¸‹æ–‡
   â€¢ é€‚åˆå¤§æ•°æ®é›†
   â€¢ å¯¹ä½é¢‘è¯æ•ˆæœå¥½

ä¼˜åŠ¿ï¼š
Advantages:
â€¢ æ•æ‰è¯ä¹‰ç›¸ä¼¼æ€§
  Captures semantic similarity
â€¢ æ”¯æŒè¯æ±‡ç±»æ¯”ï¼ˆking - man + woman = queenï¼‰
  Supports word analogies
â€¢ ä½ç»´ç¨ å¯†å‘é‡ï¼ˆé€šå¸¸ 100-300 ç»´ï¼‰
  Low-dimensional dense vectors
""")

if GENSIM_AVAILABLE:
    print("\nã€5.1ã€‘è®­ç»ƒ Word2Vec æ¨¡å‹")

    # å‡†å¤‡è®­ç»ƒæ•°æ®ï¼ˆéœ€è¦åˆ†è¯ï¼‰
    if NLTK_AVAILABLE:
        tokenized_docs = [word_tokenize(doc.lower()) for doc in documents]
    else:
        tokenized_docs = [doc.lower().split() for doc in documents]

    print(f"è®­ç»ƒæ•°æ®ç¤ºä¾‹:")
    for i, tokens in enumerate(tokenized_docs[:3], 1):
        print(f"  æ–‡æ¡£ {i}: {tokens}")

    # è®­ç»ƒ Word2Vec æ¨¡å‹
    # å‚æ•°è¯´æ˜:
    # - vector_size: è¯å‘é‡ç»´åº¦
    # - window: ä¸Šä¸‹æ–‡çª—å£å¤§å°
    # - min_count: æœ€å°è¯é¢‘
    # - sg: 0=CBOW, 1=Skip-gram
    # - workers: çº¿ç¨‹æ•°
    w2v_model = Word2Vec(
        sentences=tokenized_docs,
        vector_size=50,  # è¯å‘é‡ç»´åº¦
        window=5,        # ä¸Šä¸‹æ–‡çª—å£
        min_count=1,     # æœ€å°è¯é¢‘
        sg=0,            # CBOW
        workers=4,
        seed=RANDOM_STATE
    )

    print(f"\nâœ“ Word2Vec æ¨¡å‹è®­ç»ƒå®Œæˆ")
    print(f"è¯æ±‡è¡¨å¤§å°: {len(w2v_model.wv)}")
    print(f"è¯å‘é‡ç»´åº¦: {w2v_model.wv.vector_size}")

    # 5.2 è¯å‘é‡æŸ¥çœ‹
    print("\nã€5.2ã€‘è¯å‘é‡ç¤ºä¾‹")

    # æŸ¥çœ‹ 'learning' çš„è¯å‘é‡
    if 'learning' in w2v_model.wv:
        learning_vector = w2v_model.wv['learning']
        print(f"\n'learning' çš„è¯å‘é‡ï¼ˆå‰10ç»´ï¼‰:")
        print(learning_vector[:10])

    # 5.3 è¯æ±‡ç›¸ä¼¼åº¦
    print("\nã€5.3ã€‘è¯æ±‡ç›¸ä¼¼åº¦")

    test_words = ['learning', 'data', 'neural', 'text']

    for word in test_words:
        if word in w2v_model.wv:
            print(f"\n'{word}' çš„æœ€ç›¸ä¼¼è¯:")
            similar_words = w2v_model.wv.most_similar(word, topn=5)
            for similar_word, score in similar_words:
                print(f"  {similar_word}: {score:.4f}")

    # 5.4 è¯æ±‡ç±»æ¯”
    print("\nã€5.4ã€‘è¯æ±‡ç±»æ¯”ï¼ˆWord Analogiesï¼‰")
    print("å°è¯•: learning - machine + deep = ?")

    try:
        if all(word in w2v_model.wv for word in ['learning', 'machine', 'deep']):
            result = w2v_model.wv.most_similar(
                positive=['learning', 'deep'],
                negative=['machine'],
                topn=3
            )
            print("ç»“æœ:")
            for word, score in result:
                print(f"  {word}: {score:.4f}")
    except:
        print("  è¯æ±‡ä¸è¶³ä»¥è¿›è¡Œç±»æ¯”")

    # 5.5 æ–‡æ¡£å‘é‡åŒ–ï¼ˆå¹³å‡è¯å‘é‡ï¼‰
    print("\nã€5.5ã€‘æ–‡æ¡£å‘é‡åŒ–")

    def document_vector(doc_tokens, model):
        """
        é€šè¿‡å¹³å‡è¯å‘é‡è·å¾—æ–‡æ¡£å‘é‡
        Get document vector by averaging word vectors
        """
        vectors = [model.wv[word] for word in doc_tokens if word in model.wv]
        if len(vectors) == 0:
            return np.zeros(model.wv.vector_size)
        return np.mean(vectors, axis=0)

    # è®¡ç®—æ‰€æœ‰æ–‡æ¡£çš„å‘é‡
    doc_vectors = np.array([document_vector(tokens, w2v_model) for tokens in tokenized_docs])

    print(f"æ–‡æ¡£å‘é‡çŸ©é˜µå½¢çŠ¶: {doc_vectors.shape}")

    # è®¡ç®—æ–‡æ¡£ç›¸ä¼¼åº¦
    doc_similarity_w2v = cosine_similarity(doc_vectors)

    print(f"\nWord2Vec æ–‡æ¡£ç›¸ä¼¼åº¦ç¤ºä¾‹:")
    print(f"æ–‡æ¡£ 1 ä¸æ–‡æ¡£ 2: {doc_similarity_w2v[0, 1]:.4f}")
    print(f"æ–‡æ¡£ 1 ä¸æ–‡æ¡£ 3: {doc_similarity_w2v[0, 2]:.4f}")

else:
    print("âš  Gensim æœªå®‰è£…ï¼Œè·³è¿‡ Word2Vec éƒ¨åˆ†")

# ============================================================================
# 6. ç‰¹å¾æå–æ–¹æ³•å¯¹æ¯” / Feature Extraction Comparison
# ============================================================================
print("\nã€6ã€‘ç‰¹å¾æå–æ–¹æ³•å¯¹æ¯”")
print("-" * 80)

# åˆ›å»ºå¯¹æ¯”è¡¨æ ¼
comparison_data = {
    'ç‰¹å¾': ['BoW', 'TF-IDF', 'Word2Vec'],
    'ç»´åº¦': [
        bow_matrix.shape[1],
        tfidf_matrix.shape[1],
        50 if GENSIM_AVAILABLE else 'N/A'
    ],
    'ç¨€ç–åº¦': [
        f'{(1 - bow_matrix.nnz / (bow_matrix.shape[0] * bow_matrix.shape[1])) * 100:.1f}%',
        f'{(1 - tfidf_matrix.nnz / (tfidf_matrix.shape[0] * tfidf_matrix.shape[1])) * 100:.1f}%',
        '0%' if GENSIM_AVAILABLE else 'N/A'
    ],
    'è¯­ä¹‰ä¿¡æ¯': ['æ— ', 'å°‘', 'ä¸°å¯Œ'],
    'é€‚ç”¨åœºæ™¯': ['åˆ†ç±»', 'æ£€ç´¢/åˆ†ç±»', 'è¯­ä¹‰ä»»åŠ¡']
}

comparison_df = pd.DataFrame(comparison_data)
print("\nç‰¹å¾æå–æ–¹æ³•å¯¹æ¯”:")
print(comparison_df.to_string(index=False))

# ============================================================================
# 7. å¯è§†åŒ– / Visualization
# ============================================================================
print("\nã€7ã€‘ç‰¹å¾æå–å¯è§†åŒ–")
print("-" * 80)

# 7.1 BoW è¯é¢‘çŸ©é˜µçƒ­åŠ›å›¾
fig, ax = create_subplots(1, 1, figsize=(14, 8))

# é€‰æ‹©å‰15ä¸ªè¯å’Œå‰5ä¸ªæ–‡æ¡£
bow_dense = bow_matrix[:5, :15].toarray()
vocab_subset = vocab[:15]

sns.heatmap(bow_dense, annot=True, fmt='d', cmap='YlOrRd',
            xticklabels=vocab_subset, yticklabels=[f'Doc {i+1}' for i in range(5)],
            cbar_kws={'label': 'è¯é¢‘ / Word Count'}, ax=ax)
ax.set_title('è¯è¢‹æ¨¡å‹ - è¯é¢‘çŸ©é˜µçƒ­åŠ›å›¾\nBag of Words - Word Frequency Heatmap',
             fontsize=13, fontweight='bold', pad=15)
ax.set_xlabel('è¯æ±‡ / Vocabulary', fontsize=11)
ax.set_ylabel('æ–‡æ¡£ / Documents', fontsize=11)
plt.xticks(rotation=45, ha='right')

plt.tight_layout()
plt.savefig('/home/user/machineLearning-basics/output/nlp_bow_heatmap.png',
            dpi=150, bbox_inches='tight')
print("âœ“ å›¾è¡¨å·²ä¿å­˜: output/nlp_bow_heatmap.png")
plt.show()

# 7.2 TF-IDF æƒé‡åˆ†å¸ƒ
fig, axes = create_subplots(2, 2, figsize=(16, 12))

# 7.2.1 TF-IDF çƒ­åŠ›å›¾
tfidf_dense = tfidf_matrix[:5, :15].toarray()
sns.heatmap(tfidf_dense, annot=True, fmt='.2f', cmap='RdYlGn',
            xticklabels=vocab_subset, yticklabels=[f'Doc {i+1}' for i in range(5)],
            cbar_kws={'label': 'TF-IDF æƒé‡'}, ax=axes[0, 0])
axes[0, 0].set_title('TF-IDF æƒé‡çƒ­åŠ›å›¾\nTF-IDF Weight Heatmap',
                     fontsize=12, fontweight='bold')
axes[0, 0].set_xlabel('è¯æ±‡ / Vocabulary', fontsize=10)
axes[0, 0].set_ylabel('æ–‡æ¡£ / Documents', fontsize=10)
plt.sca(axes[0, 0])
plt.xticks(rotation=45, ha='right')

# 7.2.2 å…¨å±€è¯æ±‡é‡è¦æ€§ï¼ˆå¹³å‡ TF-IDFï¼‰
avg_tfidf = tfidf_matrix.mean(axis=0).A1
top_indices = avg_tfidf.argsort()[-15:][::-1]
top_words = [tfidf_vocab[i] for i in top_indices]
top_scores = [avg_tfidf[i] for i in top_indices]

axes[0, 1].barh(range(len(top_words)), top_scores, color='steelblue')
axes[0, 1].set_yticks(range(len(top_words)))
axes[0, 1].set_yticklabels(top_words)
axes[0, 1].set_xlabel('å¹³å‡ TF-IDF æƒé‡', fontsize=10)
axes[0, 1].set_title('å…¨å±€é‡è¦è¯æ±‡ (Top 15)\nGlobally Important Words',
                     fontsize=12, fontweight='bold')
axes[0, 1].invert_yaxis()

# 7.2.3 BoW vs TF-IDF å¯¹æ¯”
sample_doc_idx = 0
bow_sample = bow_matrix[sample_doc_idx].toarray()[0]
tfidf_sample = tfidf_matrix[sample_doc_idx].toarray()[0]

# é€‰æ‹©éé›¶è¯æ±‡
nonzero_indices = np.where(bow_sample > 0)[0][:10]
sample_words = [vocab[i] for i in nonzero_indices]
bow_values = [bow_sample[i] for i in nonzero_indices]
tfidf_values = [tfidf_sample[i] for i in nonzero_indices]

x = np.arange(len(sample_words))
width = 0.35

axes[1, 0].bar(x - width/2, bow_values, width, label='BoW', color='skyblue', alpha=0.8)
axes[1, 0].bar(x + width/2, tfidf_values, width, label='TF-IDF', color='coral', alpha=0.8)
axes[1, 0].set_xlabel('è¯æ±‡ / Words', fontsize=10)
axes[1, 0].set_ylabel('æƒé‡ / Weight', fontsize=10)
axes[1, 0].set_title(f'æ–‡æ¡£ 1: BoW vs TF-IDF å¯¹æ¯”\nDocument 1: BoW vs TF-IDF Comparison',
                     fontsize=12, fontweight='bold')
axes[1, 0].set_xticks(x)
axes[1, 0].set_xticklabels(sample_words, rotation=45, ha='right')
axes[1, 0].legend(fontsize=10)
axes[1, 0].grid(axis='y', alpha=0.3)

# 7.2.4 æ–‡æ¡£ç›¸ä¼¼åº¦çŸ©é˜µ
im = axes[1, 1].imshow(similarity_matrix, cmap='YlOrRd', aspect='auto')
axes[1, 1].set_xticks(range(len(documents)))
axes[1, 1].set_yticks(range(len(documents)))
axes[1, 1].set_xticklabels([f'D{i+1}' for i in range(len(documents))])
axes[1, 1].set_yticklabels([f'D{i+1}' for i in range(len(documents))])
axes[1, 1].set_title('æ–‡æ¡£ç›¸ä¼¼åº¦çŸ©é˜µ (TF-IDF)\nDocument Similarity Matrix',
                     fontsize=12, fontweight='bold')
axes[1, 1].set_xlabel('æ–‡æ¡£ / Documents', fontsize=10)
axes[1, 1].set_ylabel('æ–‡æ¡£ / Documents', fontsize=10)

# æ·»åŠ æ•°å€¼æ ‡æ³¨
for i in range(len(documents)):
    for j in range(len(documents)):
        text = axes[1, 1].text(j, i, f'{similarity_matrix[i, j]:.2f}',
                               ha="center", va="center", color="black", fontsize=7)

plt.colorbar(im, ax=axes[1, 1], label='ä½™å¼¦ç›¸ä¼¼åº¦ / Cosine Similarity')

plt.tight_layout()
plt.savefig('/home/user/machineLearning-basics/output/nlp_tfidf_analysis.png',
            dpi=150, bbox_inches='tight')
print("âœ“ å›¾è¡¨å·²ä¿å­˜: output/nlp_tfidf_analysis.png")
plt.show()

# 7.3 Word2Vec å¯è§†åŒ–
if GENSIM_AVAILABLE:
    fig, axes = create_subplots(2, 2, figsize=(16, 12))

    # 7.3.1 è¯å‘é‡ t-SNE é™ç»´å¯è§†åŒ–
    # è·å–æ‰€æœ‰è¯å‘é‡
    word_vectors = []
    words_list = []
    for word in w2v_model.wv.index_to_key:
        word_vectors.append(w2v_model.wv[word])
        words_list.append(word)

    word_vectors_array = np.array(word_vectors)

    # t-SNE é™ç»´
    if len(word_vectors_array) > 1:
        tsne = TSNE(n_components=2, random_state=RANDOM_STATE, perplexity=min(5, len(word_vectors_array)-1))
        word_vectors_2d = tsne.fit_transform(word_vectors_array)

        axes[0, 0].scatter(word_vectors_2d[:, 0], word_vectors_2d[:, 1],
                          c=range(len(words_list)), cmap='viridis', alpha=0.6, s=100)

        # æ ‡æ³¨è¯æ±‡
        for i, word in enumerate(words_list):
            axes[0, 0].annotate(word, xy=(word_vectors_2d[i, 0], word_vectors_2d[i, 1]),
                               xytext=(5, 2), textcoords='offset points',
                               fontsize=8, alpha=0.7)

        axes[0, 0].set_title('Word2Vec è¯å‘é‡ t-SNE å¯è§†åŒ–\nWord2Vec t-SNE Visualization',
                            fontsize=12, fontweight='bold')
        axes[0, 0].set_xlabel('t-SNE ç»´åº¦ 1', fontsize=10)
        axes[0, 0].set_ylabel('t-SNE ç»´åº¦ 2', fontsize=10)
        axes[0, 0].grid(True, alpha=0.3)

    # 7.3.2 è¯æ±‡ç›¸ä¼¼åº¦ç½‘ç»œ
    # é€‰æ‹©å‡ ä¸ªå…³é”®è¯
    key_words = ['learning', 'data', 'neural', 'text']
    key_words = [w for w in key_words if w in w2v_model.wv]

    if len(key_words) > 0:
        # ä¸ºæ¯ä¸ªå…³é”®è¯æ‰¾åˆ°æœ€ç›¸ä¼¼çš„è¯
        network_data = []
        for word in key_words[:3]:  # é™åˆ¶æ•°é‡
            similar = w2v_model.wv.most_similar(word, topn=3)
            for sim_word, score in similar:
                network_data.append((word, sim_word, score))

        # ç®€åŒ–çš„ç½‘ç»œå¯è§†åŒ–ï¼ˆä½¿ç”¨æ•£ç‚¹å›¾ï¼‰
        axes[0, 1].text(0.5, 0.5, 'è¯æ±‡ç›¸ä¼¼åº¦ç½‘ç»œ\nWord Similarity Network',
                       ha='center', va='center', fontsize=12, fontweight='bold')
        axes[0, 1].axis('off')

        # æ˜¾ç¤ºç›¸ä¼¼è¯å¯¹
        y_pos = 0.8
        for word, sim_word, score in network_data[:8]:
            axes[0, 1].text(0.5, y_pos, f'{word} â†” {sim_word}: {score:.3f}',
                           ha='center', va='center', fontsize=9)
            y_pos -= 0.1

    # 7.3.3 æ–‡æ¡£å‘é‡ç›¸ä¼¼åº¦
    doc_sim_w2v = cosine_similarity(doc_vectors)

    im = axes[1, 0].imshow(doc_sim_w2v, cmap='YlOrRd', aspect='auto')
    axes[1, 0].set_xticks(range(len(documents)))
    axes[1, 0].set_yticks(range(len(documents)))
    axes[1, 0].set_xticklabels([f'D{i+1}' for i in range(len(documents))])
    axes[1, 0].set_yticklabels([f'D{i+1}' for i in range(len(documents))])
    axes[1, 0].set_title('æ–‡æ¡£ç›¸ä¼¼åº¦çŸ©é˜µ (Word2Vec)\nDocument Similarity Matrix',
                        fontsize=12, fontweight='bold')
    axes[1, 0].set_xlabel('æ–‡æ¡£ / Documents', fontsize=10)
    axes[1, 0].set_ylabel('æ–‡æ¡£ / Documents', fontsize=10)

    for i in range(len(documents)):
        for j in range(len(documents)):
            text = axes[1, 0].text(j, i, f'{doc_sim_w2v[i, j]:.2f}',
                                  ha="center", va="center", color="black", fontsize=7)

    plt.colorbar(im, ax=axes[1, 0], label='ä½™å¼¦ç›¸ä¼¼åº¦ / Cosine Similarity')

    # 7.3.4 ç‰¹å¾ç»´åº¦å¯¹æ¯”
    methods = ['BoW', 'TF-IDF', 'Word2Vec']
    dimensions = [bow_matrix.shape[1], tfidf_matrix.shape[1], 50]
    colors_dim = ['skyblue', 'coral', 'lightgreen']

    axes[1, 1].bar(methods, dimensions, color=colors_dim, alpha=0.7, edgecolor='black')
    axes[1, 1].set_ylabel('ç‰¹å¾ç»´åº¦ / Feature Dimensions', fontsize=10)
    axes[1, 1].set_title('ç‰¹å¾ç»´åº¦å¯¹æ¯”\nFeature Dimensions Comparison',
                        fontsize=12, fontweight='bold')
    axes[1, 1].set_yscale('log')  # å¯¹æ•°åˆ»åº¦

    for i, v in enumerate(dimensions):
        axes[1, 1].text(i, v * 1.1, str(v), ha='center', va='bottom',
                       fontsize=10, fontweight='bold')

    plt.tight_layout()
    plt.savefig('/home/user/machineLearning-basics/output/nlp_word2vec_visualization.png',
                dpi=150, bbox_inches='tight')
    print("âœ“ å›¾è¡¨å·²ä¿å­˜: output/nlp_word2vec_visualization.png")
    plt.show()

# 7.4 æ–¹æ³•æ€§èƒ½å¯¹æ¯”
fig, axes = create_subplots(2, 2, figsize=(16, 12))

# 7.4.1 ç¨€ç–åº¦å¯¹æ¯”
methods_sparse = ['BoW', 'TF-IDF', 'Word2Vec']
sparsity = [
    (1 - bow_matrix.nnz / (bow_matrix.shape[0] * bow_matrix.shape[1])) * 100,
    (1 - tfidf_matrix.nnz / (tfidf_matrix.shape[0] * tfidf_matrix.shape[1])) * 100,
    0 if GENSIM_AVAILABLE else 0
]

axes[0, 0].bar(methods_sparse, sparsity, color=['steelblue', 'coral', 'lightgreen'],
              alpha=0.7, edgecolor='black')
axes[0, 0].set_ylabel('ç¨€ç–åº¦ / Sparsity (%)', fontsize=10)
axes[0, 0].set_title('ç‰¹å¾ç¨€ç–åº¦å¯¹æ¯”\nFeature Sparsity Comparison',
                    fontsize=12, fontweight='bold')
for i, v in enumerate(sparsity):
    axes[0, 0].text(i, v + 1, f'{v:.1f}%', ha='center', va='bottom',
                   fontsize=10, fontweight='bold')

# 7.4.2 è¯æ±‡è¡¨å¤§å°å¯¹æ¯”
vocab_sizes = [
    len(vectorizer_basic.vocabulary_),
    len(tfidf_vectorizer.vocabulary_),
    len(w2v_model.wv) if GENSIM_AVAILABLE else 0
]

axes[0, 1].bar(methods_sparse, vocab_sizes, color=['steelblue', 'coral', 'lightgreen'],
              alpha=0.7, edgecolor='black')
axes[0, 1].set_ylabel('è¯æ±‡è¡¨å¤§å° / Vocabulary Size', fontsize=10)
axes[0, 1].set_title('è¯æ±‡è¡¨å¤§å°å¯¹æ¯”\nVocabulary Size Comparison',
                    fontsize=12, fontweight='bold')
for i, v in enumerate(vocab_sizes):
    axes[0, 1].text(i, v + 0.5, str(v), ha='center', va='bottom',
                   fontsize=10, fontweight='bold')

# 7.4.3 ç‰¹å¾å‘é‡ç¤ºä¾‹ï¼ˆç¬¬ä¸€ä¸ªæ–‡æ¡£ï¼‰
feature_comparison = pd.DataFrame({
    'BoW': bow_matrix[0].toarray()[0][:10],
    'TF-IDF': tfidf_matrix[0].toarray()[0][:10],
    'Word2Vec': doc_vectors[0][:10] if GENSIM_AVAILABLE else np.zeros(10)
})

feature_comparison.plot(kind='bar', ax=axes[1, 0], color=['steelblue', 'coral', 'lightgreen'],
                       alpha=0.7, width=0.8)
axes[1, 0].set_xlabel('ç‰¹å¾ç»´åº¦ / Feature Dimension', fontsize=10)
axes[1, 0].set_ylabel('ç‰¹å¾å€¼ / Feature Value', fontsize=10)
axes[1, 0].set_title('æ–‡æ¡£ 1 çš„ç‰¹å¾å‘é‡å¯¹æ¯” (å‰10ç»´)\nDocument 1 Feature Vectors Comparison',
                    fontsize=12, fontweight='bold')
axes[1, 0].legend(fontsize=9)
axes[1, 0].grid(axis='y', alpha=0.3)

# 7.4.4 é€‚ç”¨åœºæ™¯æ€»ç»“
axes[1, 1].axis('off')

summary_text = """
ç‰¹å¾æå–æ–¹æ³•é€‚ç”¨åœºæ™¯æ€»ç»“
Feature Extraction Methods Summary

ğŸ“Š Bag of Words (BoW)
  âœ“ æ–‡æœ¬åˆ†ç±»ï¼ˆç®€å•ä»»åŠ¡ï¼‰
  âœ“ åƒåœ¾é‚®ä»¶è¿‡æ»¤
  âœ“ æ–‡æ¡£èšç±»

ğŸ“ˆ TF-IDF
  âœ“ ä¿¡æ¯æ£€ç´¢
  âœ“ æ–‡æ¡£æ’åº
  âœ“ å…³é”®è¯æå–
  âœ“ æ–‡æœ¬åˆ†ç±»ï¼ˆä¸­ç­‰å¤æ‚åº¦ï¼‰

ğŸ§  Word2Vec
  âœ“ è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—
  âœ“ è¯æ±‡ç±»æ¯”ä»»åŠ¡
  âœ“ å‘½åå®ä½“è¯†åˆ«
  âœ“ æƒ…æ„Ÿåˆ†æ
  âœ“ é—®ç­”ç³»ç»Ÿ

ğŸ’¡ é€‰æ‹©å»ºè®®ï¼š
  â€¢ å°æ•°æ®é›†ï¼Œç®€å•ä»»åŠ¡ â†’ BoW
  â€¢ éœ€è¦å…³é”®è¯æå– â†’ TF-IDF
  â€¢ éœ€è¦è¯­ä¹‰ç†è§£ â†’ Word2Vec
"""

axes[1, 1].text(0.1, 0.95, summary_text, ha='left', va='top',
               fontsize=9, family='monospace',
               bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))

plt.tight_layout()
plt.savefig('/home/user/machineLearning-basics/output/nlp_methods_comparison.png',
            dpi=150, bbox_inches='tight')
print("âœ“ å›¾è¡¨å·²ä¿å­˜: output/nlp_methods_comparison.png")
plt.show()

# ============================================================================
# 8. æ€»ç»“ / Summary
# ============================================================================
print("\nã€8ã€‘æ€»ç»“")
print("=" * 80)
print("""
æœ¬æ•™ç¨‹æ¶µç›–çš„å†…å®¹ï¼š
Topics Covered:

âœ“ è¯è¢‹æ¨¡å‹ï¼ˆBag of Wordsï¼‰
  â€¢ CountVectorizer ä½¿ç”¨
  â€¢ å‚æ•°è°ƒä¼˜ï¼ˆmax_features, min_df, max_dfï¼‰
  â€¢ N-grams ç”Ÿæˆ

âœ“ TF-IDF
  â€¢ åŸç†å’Œè®¡ç®—å…¬å¼
  â€¢ é‡è¦è¯æ±‡æå–
  â€¢ æ–‡æ¡£ç›¸ä¼¼åº¦è®¡ç®—

âœ“ Word2Vec
  â€¢ CBOW vs Skip-gram
  â€¢ è¯å‘é‡è®­ç»ƒ
  â€¢ è¯æ±‡ç›¸ä¼¼åº¦å’Œç±»æ¯”
  â€¢ æ–‡æ¡£å‘é‡åŒ–

âœ“ æ–¹æ³•å¯¹æ¯”
  â€¢ ç»´åº¦ã€ç¨€ç–åº¦ã€è¯­ä¹‰ä¿¡æ¯
  â€¢ é€‚ç”¨åœºæ™¯åˆ†æ

æœ€ä½³å®è·µå»ºè®®ï¼š
Best Practices:

1. æ ¹æ®ä»»åŠ¡é€‰æ‹©åˆé€‚çš„ç‰¹å¾æå–æ–¹æ³•
   Choose the right method for your task

2. TF-IDF é€‚åˆå¤§å¤šæ•°ä¼ ç»Ÿ ML ä»»åŠ¡
   TF-IDF works well for most traditional ML tasks

3. Word2Vec é€‚åˆéœ€è¦è¯­ä¹‰ç†è§£çš„ä»»åŠ¡
   Word2Vec is better for semantic understanding

4. å¯ä»¥ç»„åˆå¤šç§æ–¹æ³•ï¼ˆé›†æˆå­¦ä¹ ï¼‰
   Can combine multiple methods (ensemble learning)

5. å¤§è§„æ¨¡æ•°æ®è€ƒè™‘ä½¿ç”¨é¢„è®­ç»ƒè¯å‘é‡
   Use pre-trained embeddings for large-scale data

ä¸‹ä¸€æ­¥å­¦ä¹ ï¼š
Next Steps:
â€¢ 03_text_classification.py - æ–‡æœ¬åˆ†ç±»å®æˆ˜
  å°†ç‰¹å¾æå–åº”ç”¨åˆ°å®é™…åˆ†ç±»ä»»åŠ¡
""")

print("\n" + "=" * 80)
print("æ–‡æœ¬ç‰¹å¾æå–æ•™ç¨‹å®Œæˆï¼".center(80))
print("Text Feature Extraction Tutorial Complete!".center(80))
print("=" * 80)
