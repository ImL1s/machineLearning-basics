"""
åŸºæ–¼æ¨¹çš„å›æ­¸ï¼ˆTree-based Regressionï¼‰
ä½¿ç”¨æ±ºç­–æ¨¹åŠå…¶é›†æˆæ–¹æ³•é€²è¡Œå›æ­¸

åŒ…å«ï¼š
- æ±ºç­–æ¨¹å›æ­¸ï¼ˆDecision Tree Regressorï¼‰
- éš¨æ©Ÿæ£®æ—å›æ­¸ï¼ˆRandom Forest Regressorï¼‰
- æ¢¯åº¦æå‡å›æ­¸ï¼ˆGradient Boosting Regressorï¼‰
- XGBoost å›æ­¸
- LightGBM å›æ­¸
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.tree import DecisionTreeRegressor, plot_tree
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.model_selection import train_test_split, GridSearchCV, learning_curve, cross_val_score
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from sklearn.datasets import load_diabetes, make_regression
from sklearn.preprocessing import StandardScaler
from sklearn.inspection import permutation_importance
import time
import warnings
warnings.filterwarnings('ignore')

# XGBoost å’Œ LightGBMï¼ˆå¯é¸ï¼‰
try:
    import xgboost as xgb
    XGB_AVAILABLE = True
except ImportError:
    XGB_AVAILABLE = False
    print("âš  XGBoost æœªå®‰è£ï¼Œç›¸é—œåŠŸèƒ½å°‡è¢«è·³é")

try:
    import lightgbm as lgb
    LGB_AVAILABLE = True
except ImportError:
    LGB_AVAILABLE = False
    print("âš  LightGBM æœªå®‰è£ï¼Œç›¸é—œåŠŸèƒ½å°‡è¢«è·³é")

# å°å…¥å·¥å…·æ¨¡å¡Š / Import utility modules
import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent.parent))
from utils import RANDOM_STATE, TEST_SIZE, setup_chinese_fonts, create_subplots, get_output_path, save_figure

# è¨­ç½®ä¸­æ–‡å­—é«” / Setup Chinese fonts
setup_chinese_fonts()

print("=" * 80)
print("åŸºæ–¼æ¨¹çš„å›æ­¸ï¼ˆTree-based Regressionï¼‰æ•™ç¨‹".center(80))
print("=" * 80)

# ============================================================================
# ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºæ–¼æ¨¹çš„å›æ­¸æ¦‚è¿°
# Part 1: Tree-based Regression Overview
# ============================================================================
print("\nã€ç¬¬ä¸€éƒ¨åˆ†ã€‘åŸºæ–¼æ¨¹çš„å›æ­¸æ¦‚è¿°")
print("-" * 80)
print("""
æ±ºç­–æ¨¹å›æ­¸åŸç†ï¼š
Decision Tree Regression Principle:

æ±ºç­–æ¨¹é€šééæ­¸åœ°åˆ†å‰²ç‰¹å¾µç©ºé–“ï¼Œåœ¨æ¯å€‹è‘‰ç¯€é»ä¸Šç”¨å¹³å‡å€¼é€²è¡Œé æ¸¬ã€‚
Decision trees recursively split the feature space and predict using the mean value
at each leaf node.

åˆ†è£‚æº–å‰‡ï¼ˆSplitting Criteriaï¼‰ï¼š
â€¢ MSE (Mean Squared Error)ï¼šæœ€å°åŒ–å‡æ–¹èª¤å·®
â€¢ MAE (Mean Absolute Error)ï¼šæœ€å°åŒ–çµ•å°èª¤å·®

é›†æˆæ–¹æ³•ï¼ˆEnsemble Methodsï¼‰ï¼š
1. Baggingï¼ˆBootstrap Aggregatingï¼‰
   - éš¨æ©Ÿæ£®æ—ï¼ˆRandom Forestï¼‰
   - é€šéé™ä½æ–¹å·®æé«˜æ€§èƒ½

2. Boostingï¼ˆæå‡ï¼‰
   - æ¢¯åº¦æå‡ï¼ˆGradient Boostingï¼‰
   - XGBoost, LightGBM
   - é€šéé™ä½åå·®æé«˜æ€§èƒ½

é©ç”¨å ´æ™¯ï¼š
âœ“ éç·šæ€§é—œä¿‚
âœ“ ç‰¹å¾µäº¤äº’è¤‡é›œ
âœ“ ä¸éœ€è¦ç‰¹å¾µç¸®æ”¾
âœ“ å¯è™•ç†æ··åˆé¡å‹æ•¸æ“š
âœ“ è‡ªå‹•ç‰¹å¾µé¸æ“‡
""")

# ============================================================================
# è©•ä¼°å‡½æ•¸ / Evaluation Functions
# ============================================================================
def evaluate_model(model, X_train, X_test, y_train, y_test, model_name='Model'):
    """
    å…¨é¢è©•ä¼°å›æ­¸æ¨¡å‹
    Comprehensive evaluation of regression model

    Args:
        model: è¨“ç·´å¥½çš„æ¨¡å‹
        X_train, X_test: è¨“ç·´å’Œæ¸¬è©¦ç‰¹å¾µ
        y_train, y_test: è¨“ç·´å’Œæ¸¬è©¦æ¨™ç±¤
        model_name: æ¨¡å‹åç¨±

    Returns:
        dict: è©•ä¼°æŒ‡æ¨™å­—å…¸
    """
    # é æ¸¬
    y_train_pred = model.predict(X_train)
    y_test_pred = model.predict(X_test)

    # è¨ˆç®—æŒ‡æ¨™
    train_r2 = r2_score(y_train, y_train_pred)
    test_r2 = r2_score(y_test, y_test_pred)
    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))
    test_mae = mean_absolute_error(y_test, y_test_pred)

    print(f"\n{model_name}:")
    print(f"  è¨“ç·´é›† RÂ²: {train_r2:.4f}")
    print(f"  æ¸¬è©¦é›† RÂ²: {test_r2:.4f}")
    print(f"  æ¸¬è©¦é›† RMSE: {test_rmse:.4f}")
    print(f"  æ¸¬è©¦é›† MAE: {test_mae:.4f}")

    # æª¢æŸ¥éæ“¬åˆ
    if abs(train_r2 - test_r2) > 0.1:
        print(f"  âš  å¯èƒ½éæ“¬åˆï¼ˆRÂ² å·®ç•°: {abs(train_r2 - test_r2):.4f}ï¼‰")

    return {
        'train_r2': train_r2,
        'test_r2': test_r2,
        'rmse': test_rmse,
        'mae': test_mae,
        'y_train_pred': y_train_pred,
        'y_test_pred': y_test_pred
    }

# ============================================================================
# æ•¸æ“šæº–å‚™ / Data Preparation
# ============================================================================
print("\nã€æ•¸æ“šæº–å‚™ã€‘åŠ è¼‰ç³–å°¿ç—…æ•¸æ“šé›†")
print("-" * 80)

# åŠ è¼‰æ•¸æ“š
diabetes = load_diabetes()
X, y = diabetes.data, diabetes.target

print(f"æ•¸æ“šé›†å¤§å°ï¼š{X.shape}")
print(f"ç‰¹å¾µåç¨±ï¼š{diabetes.feature_names}")
print(f"ç›®æ¨™è®Šé‡ç¯„åœï¼š{y.min():.1f} - {y.max():.1f}")

# æ•¸æ“šåˆ†å‰²
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE
)

print(f"è¨“ç·´é›†å¤§å°ï¼š{X_train.shape}")
print(f"æ¸¬è©¦é›†å¤§å°ï¼š{X_test.shape}")

# ============================================================================
# ç¬¬äºŒéƒ¨åˆ†ï¼šæ±ºç­–æ¨¹å›æ­¸ï¼ˆDecision Tree Regressorï¼‰
# Part 2: Decision Tree Regressor
# ============================================================================
print("\nã€ç¬¬äºŒéƒ¨åˆ†ã€‘æ±ºç­–æ¨¹å›æ­¸ï¼ˆDecision Tree Regressorï¼‰")
print("-" * 80)
print("""
æ±ºç­–æ¨¹å›æ­¸åŸç†ï¼š
Decision Tree Regression Principle:

1. é¸æ“‡æœ€ä½³åˆ†è£‚é»ï¼Œä½¿å¾—åˆ†è£‚å¾Œçš„ MSE æœ€å°
2. éæ­¸åœ°å°å­ç¯€é»é‡è¤‡æ­¤éç¨‹
3. ç›´åˆ°æ»¿è¶³åœæ­¢æ¢ä»¶ï¼ˆå¦‚æœ€å¤§æ·±åº¦ã€æœ€å°æ¨£æœ¬æ•¸ï¼‰

é—œéµåƒæ•¸ï¼š
Key Parameters:
â€¢ max_depthï¼šæœ€å¤§æ·±åº¦ï¼ˆæ§åˆ¶éæ“¬åˆï¼‰
â€¢ min_samples_splitï¼šåˆ†è£‚æ‰€éœ€çš„æœ€å°æ¨£æœ¬æ•¸
â€¢ min_samples_leafï¼šè‘‰ç¯€é»æœ€å°æ¨£æœ¬æ•¸
â€¢ max_featuresï¼šåˆ†è£‚æ™‚è€ƒæ…®çš„æœ€å¤§ç‰¹å¾µæ•¸

å„ªé»ï¼š
âœ“ æ˜“æ–¼ç†è§£å’Œè§£é‡‹
âœ“ ä¸éœ€è¦ç‰¹å¾µç¸®æ”¾
âœ“ å¯è™•ç†éç·šæ€§é—œä¿‚
âœ“ è‡ªå‹•ç‰¹å¾µé¸æ“‡

ç¼ºé»ï¼š
âœ— å®¹æ˜“éæ“¬åˆ
âœ— å°æ•¸æ“šè®ŠåŒ–æ•æ„Ÿ
âœ— é æ¸¬ç²¾åº¦è¼ƒä½
""")

# ============================================================================
# 2.1 åŸºç¤æ±ºç­–æ¨¹
# ============================================================================
print("\nã€2.1ã€‘åŸºç¤æ±ºç­–æ¨¹")

# è¨“ç·´åŸºç¤æ±ºç­–æ¨¹
dt_basic = DecisionTreeRegressor(random_state=RANDOM_STATE)
dt_basic.fit(X_train, y_train)
dt_basic_results = evaluate_model(dt_basic, X_train, X_test, y_train, y_test, "åŸºç¤æ±ºç­–æ¨¹ï¼ˆç„¡é™åˆ¶ï¼‰")

# ============================================================================
# 2.2 åƒæ•¸å½±éŸ¿åˆ†æ - max_depth
# ============================================================================
print("\nã€2.2ã€‘åƒæ•¸å½±éŸ¿åˆ†æ - max_depthï¼ˆæœ€å¤§æ·±åº¦ï¼‰")
print("-" * 80)

max_depths = [2, 3, 5, 7, 10, 15, None]
depth_results = {}

for depth in max_depths:
    dt = DecisionTreeRegressor(max_depth=depth, random_state=RANDOM_STATE)
    dt.fit(X_train, y_train)

    y_train_pred = dt.predict(X_train)
    y_test_pred = dt.predict(X_test)

    train_r2 = r2_score(y_train, y_train_pred)
    test_r2 = r2_score(y_test, y_test_pred)

    depth_name = str(depth) if depth is not None else 'None'
    depth_results[depth_name] = {
        'train_r2': train_r2,
        'test_r2': test_r2,
        'model': dt,
        'y_test_pred': y_test_pred
    }

    print(f"max_depth={depth_name:>4}: è¨“ç·´RÂ²={train_r2:.4f}, æ¸¬è©¦RÂ²={test_r2:.4f}")

# ============================================================================
# 2.3 å¯è¦–åŒ–ï¼šæ±ºç­–æ¨¹æ·±åº¦å½±éŸ¿ï¼ˆ2x2å­åœ–ï¼‰
# ============================================================================
print("\nã€2.3ã€‘ç”Ÿæˆå¯è¦–åŒ–ï¼šæ±ºç­–æ¨¹æ·±åº¦å½±éŸ¿")

fig, axes = create_subplots(2, 2, figsize=(16, 12))
axes = axes.flatten()

# ç¹ªè£½ 4 å€‹ä¸åŒæ·±åº¦çš„æ±ºç­–æ¨¹
depths_to_plot = [2, 3, 5, 10]

for idx, depth in enumerate(depths_to_plot):
    plot_tree(depth_results[str(depth)]['model'],
             feature_names=diabetes.feature_names,
             filled=True,
             rounded=True,
             fontsize=8,
             ax=axes[idx])
    axes[idx].set_title(f'æ±ºç­–æ¨¹ (max_depth={depth})\nè¨“ç·´RÂ²={depth_results[str(depth)]["train_r2"]:.3f}, '
                       f'æ¸¬è©¦RÂ²={depth_results[str(depth)]["test_r2"]:.3f}',
                       fontsize=12, fontweight='bold')

plt.tight_layout()
save_figure(fig, get_output_path('decision_tree_depth_visualization.png', 'Regression'))

# ============================================================================
# 2.4 å¯è¦–åŒ–ï¼šæ·±åº¦å°æ€§èƒ½çš„å½±éŸ¿
# ============================================================================
print("\nã€2.4ã€‘ç”Ÿæˆå¯è¦–åŒ–ï¼šæ·±åº¦åƒæ•¸å½±éŸ¿æ›²ç·š")

fig, axes = create_subplots(1, 2, figsize=(16, 6))

# æº–å‚™æ•¸æ“šï¼ˆæ’é™¤ Noneï¼‰
numeric_depths = [2, 3, 5, 7, 10, 15]
train_r2_list = [depth_results[str(d)]['train_r2'] for d in numeric_depths]
test_r2_list = [depth_results[str(d)]['test_r2'] for d in numeric_depths]

# å·¦åœ–ï¼šRÂ² vs æ·±åº¦
axes[0].plot(numeric_depths, train_r2_list, 'o-', linewidth=2, markersize=8, label='è¨“ç·´é›† RÂ²', color='blue')
axes[0].plot(numeric_depths, test_r2_list, 's-', linewidth=2, markersize=8, label='æ¸¬è©¦é›† RÂ²', color='red')
axes[0].axvline(x=5, color='green', linestyle='--', alpha=0.5, label='æ¨è–¦å€¼')
axes[0].set_xlabel('æœ€å¤§æ·±åº¦ / Max Depth', fontsize=12)
axes[0].set_ylabel('RÂ² Score', fontsize=12)
axes[0].set_title('æ±ºç­–æ¨¹æ·±åº¦å° RÂ² çš„å½±éŸ¿\nEffect of Tree Depth on RÂ²', fontsize=13, fontweight='bold')
axes[0].legend(fontsize=11)
axes[0].grid(True, alpha=0.3)

# å³åœ–ï¼šéæ“¬åˆç¨‹åº¦
overfitting = [abs(train_r2_list[i] - test_r2_list[i]) for i in range(len(numeric_depths))]
axes[1].bar(numeric_depths, overfitting, color='coral', alpha=0.7, edgecolor='black')
axes[1].set_xlabel('æœ€å¤§æ·±åº¦ / Max Depth', fontsize=12)
axes[1].set_ylabel('éæ“¬åˆç¨‹åº¦ (|è¨“ç·´RÂ² - æ¸¬è©¦RÂ²|)', fontsize=12)
axes[1].set_title('æ±ºç­–æ¨¹éæ“¬åˆç¨‹åº¦åˆ†æ\nOverfitting Analysis', fontsize=13, fontweight='bold')
axes[1].grid(True, alpha=0.3, axis='y')

plt.tight_layout()
save_figure(fig, get_output_path('decision_tree_depth_analysis.png', 'Regression'))

# ============================================================================
# 2.5 ç‰¹å¾µé‡è¦æ€§
# ============================================================================
print("\nã€2.5ã€‘æ±ºç­–æ¨¹ç‰¹å¾µé‡è¦æ€§")
print("-" * 80)

# ä½¿ç”¨æœ€ä½³æ·±åº¦çš„æ¨¹
best_dt = DecisionTreeRegressor(max_depth=5, random_state=RANDOM_STATE)
best_dt.fit(X_train, y_train)

feature_importance = best_dt.feature_importances_
feature_names = diabetes.feature_names

# æ’åº
indices = np.argsort(feature_importance)[::-1]

print("ç‰¹å¾µé‡è¦æ€§æ’åï¼š")
for i, idx in enumerate(indices[:5]):
    print(f"  {i+1}. {feature_names[idx]}: {feature_importance[idx]:.4f}")

# ============================================================================
# ç¬¬ä¸‰éƒ¨åˆ†ï¼šéš¨æ©Ÿæ£®æ—å›æ­¸ï¼ˆRandom Forest Regressorï¼‰
# Part 3: Random Forest Regressor
# ============================================================================
print("\nã€ç¬¬ä¸‰éƒ¨åˆ†ã€‘éš¨æ©Ÿæ£®æ—å›æ­¸ï¼ˆRandom Forest Regressorï¼‰")
print("-" * 80)
print("""
éš¨æ©Ÿæ£®æ—åŸç†ï¼š
Random Forest Principle:

é›†æˆå¤šå€‹æ±ºç­–æ¨¹ï¼Œé€šé Baggingï¼ˆBootstrap Aggregatingï¼‰æé«˜æ€§èƒ½ã€‚
Ensemble of multiple decision trees using Bagging to improve performance.

é—œéµæ©Ÿåˆ¶ï¼š
Key Mechanisms:
1. Bootstrap æ¡æ¨£ï¼šæœ‰æ”¾å›åœ°éš¨æ©ŸæŠ½å–è¨“ç·´æ¨£æœ¬
2. ç‰¹å¾µéš¨æ©Ÿé¸æ“‡ï¼šæ¯æ¬¡åˆ†è£‚æ™‚éš¨æ©Ÿé¸æ“‡ç‰¹å¾µå­é›†
3. å¹³å‡é æ¸¬ï¼šå°æ‰€æœ‰æ¨¹çš„é æ¸¬å–å¹³å‡

é—œéµåƒæ•¸ï¼š
Key Parameters:
â€¢ n_estimatorsï¼šæ¨¹çš„æ•¸é‡ï¼ˆè¶Šå¤šè¶Šå¥½ï¼Œä½†è¨ˆç®—æˆæœ¬å¢åŠ ï¼‰
â€¢ max_depthï¼šæ¯æ£µæ¨¹çš„æœ€å¤§æ·±åº¦
â€¢ max_featuresï¼šåˆ†è£‚æ™‚è€ƒæ…®çš„æœ€å¤§ç‰¹å¾µæ•¸
â€¢ min_samples_splitï¼šåˆ†è£‚æ‰€éœ€çš„æœ€å°æ¨£æœ¬æ•¸
â€¢ bootstrapï¼šæ˜¯å¦ä½¿ç”¨ bootstrap æ¡æ¨£

å„ªé»ï¼š
âœ“ æº–ç¢ºåº¦é«˜ï¼Œæ³›åŒ–èƒ½åŠ›å¼·
âœ“ è‡ªå‹•è™•ç†éæ“¬åˆ
âœ“ å¯è©•ä¼°ç‰¹å¾µé‡è¦æ€§
âœ“ å¯ä¸¦è¡Œè¨“ç·´
âœ“ å°ç¼ºå¤±å€¼ä¸æ•æ„Ÿ

ç¼ºé»ï¼š
âœ— æ¨¡å‹å¤§ï¼Œé æ¸¬é€Ÿåº¦æ…¢
âœ— é›£ä»¥è§£é‡‹
âœ— è¨“ç·´æ™‚é–“è¼ƒé•·
""")

# ============================================================================
# 3.1 åŸºç¤éš¨æ©Ÿæ£®æ—
# ============================================================================
print("\nã€3.1ã€‘åŸºç¤éš¨æ©Ÿæ£®æ—")

rf_basic = RandomForestRegressor(n_estimators=100, random_state=RANDOM_STATE, n_jobs=-1)
rf_basic.fit(X_train, y_train)
rf_basic_results = evaluate_model(rf_basic, X_train, X_test, y_train, y_test, "åŸºç¤éš¨æ©Ÿæ£®æ—")

# ============================================================================
# 3.2 åƒæ•¸èª¿å„ª - n_estimators
# ============================================================================
print("\nã€3.2ã€‘åƒæ•¸èª¿å„ª - n_estimatorsï¼ˆæ¨¹çš„æ•¸é‡ï¼‰")
print("-" * 80)

n_estimators_list = [10, 50, 100, 200, 300, 500]
n_estimators_results = {}

for n_est in n_estimators_list:
    rf = RandomForestRegressor(n_estimators=n_est, max_depth=10, random_state=RANDOM_STATE, n_jobs=-1)
    rf.fit(X_train, y_train)

    y_test_pred = rf.predict(X_test)
    test_r2 = r2_score(y_test, y_test_pred)

    n_estimators_results[n_est] = test_r2
    print(f"n_estimators={n_est:>3}: æ¸¬è©¦RÂ²={test_r2:.4f}")

# ============================================================================
# 3.3 åƒæ•¸èª¿å„ª - max_depth
# ============================================================================
print("\nã€3.3ã€‘åƒæ•¸èª¿å„ª - max_depth")
print("-" * 80)

rf_depths = [5, 10, 15, 20, None]
rf_depth_results = {}

for depth in rf_depths:
    rf = RandomForestRegressor(n_estimators=100, max_depth=depth, random_state=RANDOM_STATE, n_jobs=-1)
    rf.fit(X_train, y_train)

    y_train_pred = rf.predict(X_train)
    y_test_pred = rf.predict(X_test)

    train_r2 = r2_score(y_train, y_train_pred)
    test_r2 = r2_score(y_test, y_test_pred)

    depth_name = str(depth) if depth is not None else 'None'
    rf_depth_results[depth_name] = {
        'train_r2': train_r2,
        'test_r2': test_r2
    }

    print(f"max_depth={depth_name:>4}: è¨“ç·´RÂ²={train_r2:.4f}, æ¸¬è©¦RÂ²={test_r2:.4f}")

# ============================================================================
# 3.4 å¯è¦–åŒ–ï¼šéš¨æ©Ÿæ£®æ—åƒæ•¸å½±éŸ¿
# ============================================================================
print("\nã€3.4ã€‘ç”Ÿæˆå¯è¦–åŒ–ï¼šéš¨æ©Ÿæ£®æ—åƒæ•¸å½±éŸ¿")

fig, axes = create_subplots(1, 2, figsize=(16, 6))

# å·¦åœ–ï¼šn_estimators å½±éŸ¿
n_est_keys = list(n_estimators_results.keys())
n_est_values = list(n_estimators_results.values())

axes[0].plot(n_est_keys, n_est_values, 'o-', linewidth=2, markersize=8, color='green')
axes[0].set_xlabel('æ¨¹çš„æ•¸é‡ / Number of Trees (n_estimators)', fontsize=12)
axes[0].set_ylabel('æ¸¬è©¦é›† RÂ² Score', fontsize=12)
axes[0].set_title('æ¨¹çš„æ•¸é‡å°æ€§èƒ½çš„å½±éŸ¿\nEffect of Number of Trees on Performance',
                  fontsize=13, fontweight='bold')
axes[0].grid(True, alpha=0.3)
axes[0].axhline(y=max(n_est_values), color='red', linestyle='--', alpha=0.5, label='æœ€ä½³æ€§èƒ½')
axes[0].legend(fontsize=11)

# å³åœ–ï¼šmax_depth å½±éŸ¿
rf_depth_numeric = [5, 10, 15, 20]
rf_train_r2 = [rf_depth_results[str(d)]['train_r2'] for d in rf_depth_numeric]
rf_test_r2 = [rf_depth_results[str(d)]['test_r2'] for d in rf_depth_numeric]

axes[1].plot(rf_depth_numeric, rf_train_r2, 'o-', linewidth=2, markersize=8, label='è¨“ç·´é›† RÂ²', color='blue')
axes[1].plot(rf_depth_numeric, rf_test_r2, 's-', linewidth=2, markersize=8, label='æ¸¬è©¦é›† RÂ²', color='red')
axes[1].set_xlabel('æœ€å¤§æ·±åº¦ / Max Depth', fontsize=12)
axes[1].set_ylabel('RÂ² Score', fontsize=12)
axes[1].set_title('æ¨¹æ·±åº¦å°æ€§èƒ½çš„å½±éŸ¿\nEffect of Tree Depth on Performance', fontsize=13, fontweight='bold')
axes[1].legend(fontsize=11)
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
save_figure(fig, get_output_path('random_forest_parameter_tuning.png', 'Regression'))

# ============================================================================
# 3.5 ç‰¹å¾µé‡è¦æ€§åˆ†æ
# ============================================================================
print("\nã€3.5ã€‘éš¨æ©Ÿæ£®æ—ç‰¹å¾µé‡è¦æ€§")
print("-" * 80)

# å…§ç½®ç‰¹å¾µé‡è¦æ€§
rf_importance = rf_basic.feature_importances_
rf_indices = np.argsort(rf_importance)[::-1]

print("ç‰¹å¾µé‡è¦æ€§æ’åï¼ˆåŸºæ–¼ä¸ç´”åº¦ï¼‰ï¼š")
for i, idx in enumerate(rf_indices[:5]):
    print(f"  {i+1}. {feature_names[idx]}: {rf_importance[idx]:.4f}")

# Permutation Importance
print("\nè¨ˆç®— Permutation Importanceï¼ˆå¯èƒ½éœ€è¦ä¸€äº›æ™‚é–“ï¼‰...")
perm_importance = permutation_importance(rf_basic, X_test, y_test, n_repeats=10, random_state=RANDOM_STATE, n_jobs=-1)
perm_indices = np.argsort(perm_importance.importances_mean)[::-1]

print("\nç‰¹å¾µé‡è¦æ€§æ’åï¼ˆåŸºæ–¼ Permutationï¼‰ï¼š")
for i, idx in enumerate(perm_indices[:5]):
    print(f"  {i+1}. {feature_names[idx]}: {perm_importance.importances_mean[idx]:.4f}")

# ============================================================================
# 3.6 å¯è¦–åŒ–ï¼šç‰¹å¾µé‡è¦æ€§å°æ¯”
# ============================================================================
print("\nã€3.6ã€‘ç”Ÿæˆå¯è¦–åŒ–ï¼šç‰¹å¾µé‡è¦æ€§")

fig, axes = create_subplots(1, 2, figsize=(16, 6))

# å·¦åœ–ï¼šå…§ç½®ç‰¹å¾µé‡è¦æ€§
axes[0].barh(range(len(feature_names)), rf_importance[rf_indices], color='steelblue', alpha=0.7)
axes[0].set_yticks(range(len(feature_names)))
axes[0].set_yticklabels([feature_names[i] for i in rf_indices], fontsize=10)
axes[0].set_xlabel('é‡è¦æ€§åˆ†æ•¸ / Importance Score', fontsize=12)
axes[0].set_title('éš¨æ©Ÿæ£®æ—ç‰¹å¾µé‡è¦æ€§ï¼ˆåŸºæ–¼ä¸ç´”åº¦ï¼‰\nRandom Forest Feature Importance (Impurity-based)',
                  fontsize=12, fontweight='bold')
axes[0].grid(True, alpha=0.3, axis='x')

# å³åœ–ï¼šPermutation Importance
perm_means = perm_importance.importances_mean[perm_indices]
perm_stds = perm_importance.importances_std[perm_indices]
axes[1].barh(range(len(feature_names)), perm_means, xerr=perm_stds, color='coral', alpha=0.7)
axes[1].set_yticks(range(len(feature_names)))
axes[1].set_yticklabels([feature_names[i] for i in perm_indices], fontsize=10)
axes[1].set_xlabel('é‡è¦æ€§åˆ†æ•¸ / Importance Score', fontsize=12)
axes[1].set_title('éš¨æ©Ÿæ£®æ—ç‰¹å¾µé‡è¦æ€§ï¼ˆåŸºæ–¼ Permutationï¼‰\nRandom Forest Feature Importance (Permutation)',
                  fontsize=12, fontweight='bold')
axes[1].grid(True, alpha=0.3, axis='x')

plt.tight_layout()
save_figure(fig, get_output_path('random_forest_feature_importance.png', 'Regression'))

# ============================================================================
# ç¬¬å››éƒ¨åˆ†ï¼šæ¢¯åº¦æå‡å›æ­¸ï¼ˆGradient Boosting Regressorï¼‰
# Part 4: Gradient Boosting Regressor
# ============================================================================
print("\nã€ç¬¬å››éƒ¨åˆ†ã€‘æ¢¯åº¦æå‡å›æ­¸ï¼ˆGradient Boosting Regressorï¼‰")
print("-" * 80)
print("""
æ¢¯åº¦æå‡åŸç†ï¼š
Gradient Boosting Principle:

é€šéé †åºåœ°è¨“ç·´å¤šå€‹å¼±å­¸ç¿’å™¨ï¼ˆé€šå¸¸æ˜¯æ·ºæ±ºç­–æ¨¹ï¼‰ï¼Œæ¯å€‹æ–°æ¨¹æ“¬åˆå‰ä¸€è¼ªçš„æ®˜å·®ã€‚
Sequentially train multiple weak learners (usually shallow trees), with each new tree
fitting the residuals of the previous round.

æ ¸å¿ƒæ€æƒ³ï¼š
Core Idea:
1. åˆå§‹åŒ–ï¼šä½¿ç”¨ç°¡å–®æ¨¡å‹ï¼ˆå¦‚å‡å€¼ï¼‰
2. è¿­ä»£ï¼šè¨“ç·´æ–°æ¨¹æ“¬åˆæ®˜å·®ï¼ˆè² æ¢¯åº¦ï¼‰
3. æ›´æ–°ï¼šæŒ‰å­¸ç¿’ç‡åŠ æ¬Šæ·»åŠ æ–°æ¨¹
4. é‡è¤‡ç›´åˆ°é”åˆ°æŒ‡å®šæ¨¹æ•¸é‡

é—œéµåƒæ•¸ï¼š
Key Parameters:
â€¢ n_estimatorsï¼šæ¨¹çš„æ•¸é‡ï¼ˆè¿­ä»£æ¬¡æ•¸ï¼‰
â€¢ learning_rateï¼šå­¸ç¿’ç‡ï¼Œæ§åˆ¶æ¯æ£µæ¨¹çš„è²¢ç»
â€¢ max_depthï¼šæ¨¹çš„æœ€å¤§æ·±åº¦ï¼ˆé€šå¸¸è¼ƒå°ï¼Œ3-5ï¼‰
â€¢ subsampleï¼šæ¨£æœ¬æ¡æ¨£æ¯”ä¾‹ï¼ˆå¼•å…¥éš¨æ©Ÿæ€§ï¼‰
â€¢ min_samples_split/leafï¼šæ§åˆ¶æ¨¹çš„è¤‡é›œåº¦

å„ªé»ï¼š
âœ“ é æ¸¬ç²¾åº¦é«˜
âœ“ å¯è™•ç†éç·šæ€§å’Œäº¤äº’ä½œç”¨
âœ“ è‡ªå‹•ç‰¹å¾µé¸æ“‡
âœ“ é­¯æ£’æ€§å¼·

ç¼ºé»ï¼š
âœ— å®¹æ˜“éæ“¬åˆï¼ˆéœ€èª¿åƒï¼‰
âœ— è¨“ç·´æ…¢ï¼ˆé †åºè¨“ç·´ï¼‰
âœ— å°åƒæ•¸æ•æ„Ÿ
""")

# ============================================================================
# 4.1 åŸºç¤æ¢¯åº¦æå‡
# ============================================================================
print("\nã€4.1ã€‘åŸºç¤æ¢¯åº¦æå‡")

gb_basic = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=RANDOM_STATE)
gb_basic.fit(X_train, y_train)
gb_basic_results = evaluate_model(gb_basic, X_train, X_test, y_train, y_test, "åŸºç¤æ¢¯åº¦æå‡")

# ============================================================================
# 4.2 åƒæ•¸èª¿å„ª - learning_rate vs n_estimators
# ============================================================================
print("\nã€4.2ã€‘åƒæ•¸èª¿å„ª - learning_rate vs n_estimators")
print("-" * 80)

learning_rates = [0.01, 0.05, 0.1, 0.2]
gb_lr_results = {}

for lr in learning_rates:
    gb = GradientBoostingRegressor(n_estimators=100, learning_rate=lr, max_depth=3, random_state=RANDOM_STATE)
    gb.fit(X_train, y_train)

    y_test_pred = gb.predict(X_test)
    test_r2 = r2_score(y_test, y_test_pred)

    gb_lr_results[lr] = test_r2
    print(f"learning_rate={lr:.2f}: æ¸¬è©¦RÂ²={test_r2:.4f}")

# ============================================================================
# 4.3 å­¸ç¿’æ›²ç·šåˆ†æ
# ============================================================================
print("\nã€4.3ã€‘å­¸ç¿’æ›²ç·šåˆ†æ")
print("-" * 80)

# è¨“ç·´ä¸€å€‹æ¢¯åº¦æå‡æ¨¡å‹ä¸¦è¨˜éŒ„æ¯ä¸€æ­¥çš„æ€§èƒ½
gb_staged = GradientBoostingRegressor(n_estimators=200, learning_rate=0.1, max_depth=3, random_state=RANDOM_STATE)
gb_staged.fit(X_train, y_train)

# ç²å–æ¯ä¸€æ­¥çš„é æ¸¬
train_scores = []
test_scores = []

for y_train_pred, y_test_pred in zip(gb_staged.staged_predict(X_train), gb_staged.staged_predict(X_test)):
    train_scores.append(r2_score(y_train, y_train_pred))
    test_scores.append(r2_score(y_test, y_test_pred))

print(f"æœ€çµ‚è¨“ç·´RÂ²: {train_scores[-1]:.4f}")
print(f"æœ€çµ‚æ¸¬è©¦RÂ²: {test_scores[-1]:.4f}")

# ============================================================================
# 4.4 å¯è¦–åŒ–ï¼šæ¢¯åº¦æå‡å­¸ç¿’æ›²ç·š
# ============================================================================
print("\nã€4.4ã€‘ç”Ÿæˆå¯è¦–åŒ–ï¼šæ¢¯åº¦æå‡å­¸ç¿’æ›²ç·š")

fig, axes = create_subplots(1, 2, figsize=(16, 6))

# å·¦åœ–ï¼šå­¸ç¿’æ›²ç·š
axes[0].plot(range(1, len(train_scores) + 1), train_scores, label='è¨“ç·´é›† RÂ²', linewidth=2, color='blue')
axes[0].plot(range(1, len(test_scores) + 1), test_scores, label='æ¸¬è©¦é›† RÂ²', linewidth=2, color='red')
axes[0].set_xlabel('è¿­ä»£æ¬¡æ•¸ / Number of Iterations', fontsize=12)
axes[0].set_ylabel('RÂ² Score', fontsize=12)
axes[0].set_title('æ¢¯åº¦æå‡å­¸ç¿’æ›²ç·š\nGradient Boosting Learning Curve', fontsize=13, fontweight='bold')
axes[0].legend(fontsize=11)
axes[0].grid(True, alpha=0.3)

# å³åœ–ï¼šlearning_rate å½±éŸ¿
lr_keys = list(gb_lr_results.keys())
lr_values = list(gb_lr_results.values())

axes[1].plot(lr_keys, lr_values, 'o-', linewidth=2, markersize=10, color='green')
axes[1].set_xlabel('å­¸ç¿’ç‡ / Learning Rate', fontsize=12)
axes[1].set_ylabel('æ¸¬è©¦é›† RÂ² Score', fontsize=12)
axes[1].set_title('å­¸ç¿’ç‡å°æ€§èƒ½çš„å½±éŸ¿\nEffect of Learning Rate on Performance',
                  fontsize=13, fontweight='bold')
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
save_figure(fig, get_output_path('gradient_boosting_learning_curve.png', 'Regression'))

# ============================================================================
# ç¬¬äº”éƒ¨åˆ†ï¼šXGBoost å›æ­¸
# Part 5: XGBoost Regressor
# ============================================================================
if XGB_AVAILABLE:
    print("\nã€ç¬¬äº”éƒ¨åˆ†ã€‘XGBoost å›æ­¸")
    print("-" * 80)
    print("""
XGBoost åŸç†å’Œå„ªå‹¢ï¼š
XGBoost Principle and Advantages:

XGBoostï¼ˆExtreme Gradient Boostingï¼‰æ˜¯æ¢¯åº¦æå‡çš„å„ªåŒ–å¯¦ç¾ã€‚
XGBoost is an optimized implementation of gradient boosting.

å‰µæ–°é»ï¼š
Innovations:
1. æ­£å‰‡åŒ–ï¼šL1/L2 æ­£å‰‡åŒ–é˜²æ­¢éæ“¬åˆ
2. äºŒéšå°æ•¸ï¼šä½¿ç”¨æ³°å‹’å±•é–‹çš„äºŒéšè¿‘ä¼¼
3. ä¸¦è¡Œè™•ç†ï¼šç‰¹å¾µä¸¦è¡ŒåŒ–åŠ é€Ÿè¨“ç·´
4. åˆ—æ¡æ¨£ï¼šé¡ä¼¼éš¨æ©Ÿæ£®æ—çš„ç‰¹å¾µæ¡æ¨£
5. ç¼ºå¤±å€¼è™•ç†ï¼šè‡ªå‹•å­¸ç¿’ç¼ºå¤±å€¼çš„æœ€å„ªæ–¹å‘

é—œéµåƒæ•¸ï¼š
Key Parameters:
â€¢ n_estimatorsï¼šæ¨¹çš„æ•¸é‡
â€¢ learning_rate (eta)ï¼šå­¸ç¿’ç‡
â€¢ max_depthï¼šæ¨¹çš„æœ€å¤§æ·±åº¦
â€¢ colsample_bytreeï¼šåˆ—æ¡æ¨£æ¯”ä¾‹
â€¢ subsampleï¼šè¡Œæ¡æ¨£æ¯”ä¾‹
â€¢ reg_alphaï¼šL1 æ­£å‰‡åŒ–
â€¢ reg_lambdaï¼šL2 æ­£å‰‡åŒ–
    """)

    # ============================================================================
    # 5.1 åŸºç¤ XGBoost
    # ============================================================================
    print("\nã€5.1ã€‘åŸºç¤ XGBoost")

    xgb_basic = xgb.XGBRegressor(
        n_estimators=100,
        learning_rate=0.1,
        max_depth=5,
        random_state=RANDOM_STATE,
        n_jobs=-1
    )
    xgb_basic.fit(X_train, y_train)
    xgb_basic_results = evaluate_model(xgb_basic, X_train, X_test, y_train, y_test, "åŸºç¤ XGBoost")

    # ============================================================================
    # 5.2 XGBoost åƒæ•¸èª¿å„ª
    # ============================================================================
    print("\nã€5.2ã€‘XGBoost åƒæ•¸èª¿å„ª")
    print("-" * 80)

    # æ¸¬è©¦ä¸åŒçš„ max_depth
    xgb_depths = [3, 5, 7, 9]
    xgb_depth_results = {}

    for depth in xgb_depths:
        model = xgb.XGBRegressor(n_estimators=100, max_depth=depth, learning_rate=0.1,
                                random_state=RANDOM_STATE, n_jobs=-1)
        model.fit(X_train, y_train)

        y_test_pred = model.predict(X_test)
        test_r2 = r2_score(y_test, y_test_pred)

        xgb_depth_results[depth] = test_r2
        print(f"max_depth={depth}: æ¸¬è©¦RÂ²={test_r2:.4f}")

    # ============================================================================
    # 5.3 ç‰¹å¾µé‡è¦æ€§ï¼ˆ3ç¨®é¡å‹ï¼‰
    # ============================================================================
    print("\nã€5.3ã€‘XGBoost ç‰¹å¾µé‡è¦æ€§")
    print("-" * 80)

    # ç²å–ä¸åŒé¡å‹çš„ç‰¹å¾µé‡è¦æ€§
    importance_types = ['weight', 'gain', 'cover']
    xgb_importances = {}

    for imp_type in importance_types:
        importance = xgb_basic.get_booster().get_score(importance_type=imp_type)
        # è½‰æ›ç‚ºæ•¸çµ„å½¢å¼
        imp_array = np.zeros(len(feature_names))
        for key, value in importance.items():
            feature_idx = int(key[1:])  # 'f0' -> 0
            imp_array[feature_idx] = value
        xgb_importances[imp_type] = imp_array

        print(f"\nç‰¹å¾µé‡è¦æ€§ï¼ˆ{imp_type}ï¼‰æ’åï¼š")
        indices = np.argsort(imp_array)[::-1]
        for i, idx in enumerate(indices[:5]):
            print(f"  {i+1}. {feature_names[idx]}: {imp_array[idx]:.2f}")

    # ========================================================================
    # 5.4 å¯è¦–åŒ–ï¼šXGBoost ç‰¹å¾µé‡è¦æ€§ï¼ˆ3ç¨®ï¼‰
    # ========================================================================
    print("\nã€5.4ã€‘ç”Ÿæˆå¯è¦–åŒ–ï¼šXGBoost ç‰¹å¾µé‡è¦æ€§")

    fig, axes = create_subplots(1, 3, figsize=(18, 6))

    for idx, (imp_type, imp_array) in enumerate(xgb_importances.items()):
        indices = np.argsort(imp_array)[::-1]
        axes[idx].barh(range(len(feature_names)), imp_array[indices], color='teal', alpha=0.7)
        axes[idx].set_yticks(range(len(feature_names)))
        axes[idx].set_yticklabels([feature_names[i] for i in indices], fontsize=9)
        axes[idx].set_xlabel('é‡è¦æ€§åˆ†æ•¸', fontsize=11)
        axes[idx].set_title(f'XGBoost ç‰¹å¾µé‡è¦æ€§\n({imp_type.upper()})', fontsize=12, fontweight='bold')
        axes[idx].grid(True, alpha=0.3, axis='x')

    plt.tight_layout()
    save_figure(fig, get_output_path('xgboost_feature_importance.png', 'Regression'))

else:
    print("\nâš  XGBoost æœªå®‰è£ï¼Œè·³é XGBoost ç›¸é—œå…§å®¹")
    xgb_basic_results = None

# ============================================================================
# ç¬¬å…­éƒ¨åˆ†ï¼šLightGBM å›æ­¸
# Part 6: LightGBM Regressor
# ============================================================================
if LGB_AVAILABLE:
    print("\nã€ç¬¬å…­éƒ¨åˆ†ã€‘LightGBM å›æ­¸")
    print("-" * 80)
    print("""
LightGBM åŸç†ï¼š
LightGBM Principle:

LightGBM æ˜¯å¾®è»Ÿé–‹ç™¼çš„é«˜æ•ˆæ¢¯åº¦æå‡æ¡†æ¶ã€‚
LightGBM is an efficient gradient boosting framework developed by Microsoft.

æ ¸å¿ƒæŠ€è¡“ï¼š
Core Technologies:
1. Histogram-based ç®—æ³•ï¼šä½¿ç”¨ç›´æ–¹åœ–åŠ é€Ÿ
2. Leaf-wise ç”Ÿé•·ï¼šæŒ‰è‘‰å­ç¯€é»ç”Ÿé•·è€Œéå±¤ç´š
3. GOSSï¼ˆGradient-based One-Side Samplingï¼‰ï¼šåŸºæ–¼æ¢¯åº¦çš„æ¡æ¨£
4. EFBï¼ˆExclusive Feature Bundlingï¼‰ï¼šäº’æ–¥ç‰¹å¾µæ†ç¶

å„ªé»ï¼š
âœ“ è¨“ç·´é€Ÿåº¦å¿«
âœ“ å…§å­˜ä½”ç”¨ä½
âœ“ æº–ç¢ºåº¦é«˜
âœ“ æ”¯æŒå¤§è¦æ¨¡æ•¸æ“š
    """)

    # ========================================================================
    # 6.1 åŸºç¤ LightGBM
    # ========================================================================
    print("\nã€6.1ã€‘åŸºç¤ LightGBM")

    lgb_basic = lgb.LGBMRegressor(
        n_estimators=100,
        learning_rate=0.1,
        max_depth=5,
        random_state=RANDOM_STATE,
        n_jobs=-1,
        verbose=-1
    )
    lgb_basic.fit(X_train, y_train)
    lgb_basic_results = evaluate_model(lgb_basic, X_train, X_test, y_train, y_test, "åŸºç¤ LightGBM")

else:
    print("\nâš  LightGBM æœªå®‰è£ï¼Œè·³é LightGBM ç›¸é—œå…§å®¹")
    lgb_basic_results = None

# ============================================================================
# ç¬¬ä¸ƒéƒ¨åˆ†ï¼šç¶œåˆå°æ¯”
# Part 7: Comprehensive Comparison
# ============================================================================
print("\nã€ç¬¬ä¸ƒéƒ¨åˆ†ã€‘æ‰€æœ‰æ¨¹æ¨¡å‹ç¶œåˆå°æ¯”")
print("-" * 80)

# æ”¶é›†æ‰€æœ‰æ¨¡å‹çš„çµæœ
all_models = {
    'æ±ºç­–æ¨¹': (DecisionTreeRegressor(max_depth=5, random_state=RANDOM_STATE), None),
    'éš¨æ©Ÿæ£®æ—': (rf_basic, rf_basic_results),
    'æ¢¯åº¦æå‡': (gb_basic, gb_basic_results)
}

if XGB_AVAILABLE:
    all_models['XGBoost'] = (xgb_basic, xgb_basic_results)

if LGB_AVAILABLE:
    all_models['LightGBM'] = (lgb_basic, lgb_basic_results)

# é‡æ–°è©•ä¼°æ‰€æœ‰æ¨¡å‹ä¸¦è¨˜éŒ„è¨“ç·´æ™‚é–“
comparison_data = []

for name, (model, cached_results) in all_models.items():
    if cached_results is None:
        # è¨“ç·´ä¸¦è¨ˆæ™‚
        start_time = time.time()
        model.fit(X_train, y_train)
        train_time = time.time() - start_time

        # é æ¸¬ä¸¦è¨ˆæ™‚
        start_time = time.time()
        y_test_pred = model.predict(X_test)
        predict_time = time.time() - start_time

        test_r2 = r2_score(y_test, y_test_pred)
        test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))
        test_mae = mean_absolute_error(y_test, y_test_pred)
    else:
        # ä½¿ç”¨ç·©å­˜çš„çµæœ
        test_r2 = cached_results['test_r2']
        test_rmse = cached_results['rmse']
        test_mae = cached_results['mae']

        # é‡æ–°è¨ˆæ™‚
        start_time = time.time()
        model.fit(X_train, y_train)
        train_time = time.time() - start_time

        start_time = time.time()
        y_test_pred = model.predict(X_test)
        predict_time = time.time() - start_time

    comparison_data.append({
        'æ¨¡å‹': name,
        'RÂ² Score': test_r2,
        'RMSE': test_rmse,
        'MAE': test_mae,
        'è¨“ç·´æ™‚é–“(s)': train_time,
        'é æ¸¬æ™‚é–“(s)': predict_time
    })

    print(f"\n{name}:")
    print(f"  RÂ²: {test_r2:.4f}")
    print(f"  RMSE: {test_rmse:.2f}")
    print(f"  MAE: {test_mae:.2f}")
    print(f"  è¨“ç·´æ™‚é–“: {train_time:.4f}s")
    print(f"  é æ¸¬æ™‚é–“: {predict_time:.4f}s")

# å‰µå»ºå°æ¯”è¡¨æ ¼
comparison_df = pd.DataFrame(comparison_data)
print("\n" + "="*80)
print("æ¨¡å‹æ€§èƒ½å°æ¯”è¡¨")
print("="*80)
print(comparison_df.to_string(index=False))

# ============================================================================
# 7.1 å¯è¦–åŒ–ï¼šæ‰€æœ‰æ¨¡å‹æ€§èƒ½å°æ¯”ï¼ˆæŸ±ç‹€åœ–ï¼‰
# ============================================================================
print("\nã€7.1ã€‘ç”Ÿæˆå¯è¦–åŒ–ï¼šæ¨¡å‹æ€§èƒ½å°æ¯”")

fig, axes = create_subplots(2, 2, figsize=(16, 12))

model_names = comparison_df['æ¨¡å‹'].values
r2_scores = comparison_df['RÂ² Score'].values
rmse_scores = comparison_df['RMSE'].values
mae_scores = comparison_df['MAE'].values
train_times = comparison_df['è¨“ç·´æ™‚é–“(s)'].values

# å­åœ–1ï¼šRÂ² Score
axes[0, 0].barh(model_names, r2_scores, color='steelblue', alpha=0.7, edgecolor='black')
axes[0, 0].set_xlabel('RÂ² Score', fontsize=12)
axes[0, 0].set_title('RÂ² Score å°æ¯”\nRÂ² Score Comparison', fontsize=13, fontweight='bold')
axes[0, 0].grid(True, alpha=0.3, axis='x')
for i, v in enumerate(r2_scores):
    axes[0, 0].text(v + 0.01, i, f'{v:.4f}', va='center', fontsize=10, fontweight='bold')

# å­åœ–2ï¼šRMSE
axes[0, 1].barh(model_names, rmse_scores, color='coral', alpha=0.7, edgecolor='black')
axes[0, 1].set_xlabel('RMSE', fontsize=12)
axes[0, 1].set_title('RMSE å°æ¯”ï¼ˆè¶Šä½è¶Šå¥½ï¼‰\nRMSE Comparison (Lower is Better)',
                     fontsize=13, fontweight='bold')
axes[0, 1].grid(True, alpha=0.3, axis='x')
for i, v in enumerate(rmse_scores):
    axes[0, 1].text(v + 1, i, f'{v:.2f}', va='center', fontsize=10, fontweight='bold')

# å­åœ–3ï¼šMAE
axes[1, 0].barh(model_names, mae_scores, color='lightgreen', alpha=0.7, edgecolor='black')
axes[1, 0].set_xlabel('MAE', fontsize=12)
axes[1, 0].set_title('MAE å°æ¯”ï¼ˆè¶Šä½è¶Šå¥½ï¼‰\nMAE Comparison (Lower is Better)',
                     fontsize=13, fontweight='bold')
axes[1, 0].grid(True, alpha=0.3, axis='x')
for i, v in enumerate(mae_scores):
    axes[1, 0].text(v + 1, i, f'{v:.2f}', va='center', fontsize=10, fontweight='bold')

# å­åœ–4ï¼šè¨“ç·´æ™‚é–“
axes[1, 1].barh(model_names, train_times, color='gold', alpha=0.7, edgecolor='black')
axes[1, 1].set_xlabel('è¨“ç·´æ™‚é–“ (ç§’)', fontsize=12)
axes[1, 1].set_title('è¨“ç·´æ™‚é–“å°æ¯”\nTraining Time Comparison', fontsize=13, fontweight='bold')
axes[1, 1].grid(True, alpha=0.3, axis='x')
for i, v in enumerate(train_times):
    axes[1, 1].text(v + 0.001, i, f'{v:.4f}s', va='center', fontsize=10, fontweight='bold')

plt.tight_layout()
save_figure(fig, get_output_path('all_models_performance_comparison.png', 'Regression'))

# ============================================================================
# 7.2 å¯è¦–åŒ–ï¼šé æ¸¬vsçœŸå¯¦å€¼ï¼ˆå¤šå€‹æ¨¡å‹ï¼‰
# ============================================================================
print("\nã€7.2ã€‘ç”Ÿæˆå¯è¦–åŒ–ï¼šé æ¸¬ vs çœŸå¯¦å€¼")

n_models = len(all_models)
n_rows = (n_models + 2) // 3
n_cols = min(3, n_models)

fig, axes = create_subplots(n_rows, n_cols, figsize=(6*n_cols, 5*n_rows))
if n_models == 1:
    axes = [axes]
else:
    axes = axes.flatten()

for idx, (name, (model, _)) in enumerate(all_models.items()):
    y_pred = model.predict(X_test)
    r2 = r2_score(y_test, y_pred)

    axes[idx].scatter(y_test, y_pred, alpha=0.6, s=50, edgecolors='k', linewidth=0.5)
    axes[idx].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()],
                   'r--', lw=2, label='å®Œç¾é æ¸¬ç·š')
    axes[idx].set_xlabel('çœŸå¯¦å€¼ / True Values', fontsize=11)
    axes[idx].set_ylabel('é æ¸¬å€¼ / Predictions', fontsize=11)
    axes[idx].set_title(f'{name}\nRÂ² = {r2:.4f}', fontsize=12, fontweight='bold')
    axes[idx].legend(fontsize=10)
    axes[idx].grid(True, alpha=0.3)

# éš±è—å¤šé¤˜çš„å­åœ–
for idx in range(n_models, len(axes)):
    axes[idx].axis('off')

plt.tight_layout()
save_figure(fig, get_output_path('predictions_vs_actual.png', 'Regression'))

# ============================================================================
# 7.3 å¯è¦–åŒ–ï¼šæ®˜å·®åˆ†æ
# ============================================================================
print("\nã€7.3ã€‘ç”Ÿæˆå¯è¦–åŒ–ï¼šæ®˜å·®åˆ†æ")

fig, axes = create_subplots(n_rows, n_cols, figsize=(6*n_cols, 5*n_rows))
if n_models == 1:
    axes = [axes]
else:
    axes = axes.flatten()

for idx, (name, (model, _)) in enumerate(all_models.items()):
    y_pred = model.predict(X_test)
    residuals = y_test - y_pred

    axes[idx].scatter(y_pred, residuals, alpha=0.6, s=50, edgecolors='k', linewidth=0.5)
    axes[idx].axhline(y=0, color='r', linestyle='--', lw=2)
    axes[idx].set_xlabel('é æ¸¬å€¼ / Predicted Values', fontsize=11)
    axes[idx].set_ylabel('æ®˜å·® / Residuals', fontsize=11)
    axes[idx].set_title(f'{name} - æ®˜å·®åœ–\nResidual Plot', fontsize=12, fontweight='bold')
    axes[idx].grid(True, alpha=0.3)

# éš±è—å¤šé¤˜çš„å­åœ–
for idx in range(n_models, len(axes)):
    axes[idx].axis('off')

plt.tight_layout()
save_figure(fig, get_output_path('residuals_analysis.png', 'Regression'))

# ============================================================================
# ç¬¬å…«éƒ¨åˆ†ï¼šå¯¦éš›æ‡‰ç”¨æ¡ˆä¾‹
# Part 8: Practical Application
# ============================================================================
print("\nã€ç¬¬å…«éƒ¨åˆ†ã€‘å¯¦éš›æ‡‰ç”¨æ¡ˆä¾‹ - å®Œæ•´å»ºæ¨¡æµç¨‹")
print("-" * 80)

# å‰µå»ºä¸€å€‹æ›´è¤‡é›œçš„åˆæˆæ•¸æ“šé›†ç”¨æ–¼æ¼”ç¤º
X_demo, y_demo = make_regression(
    n_samples=1000,
    n_features=20,
    n_informative=15,
    noise=10,
    random_state=RANDOM_STATE
)

X_train_demo, X_test_demo, y_train_demo, y_test_demo = train_test_split(
    X_demo, y_demo, test_size=TEST_SIZE, random_state=RANDOM_STATE
)

print(f"æ¼”ç¤ºæ•¸æ“šé›†å¤§å°ï¼š{X_demo.shape}")
print(f"è¨“ç·´é›†ï¼š{X_train_demo.shape}, æ¸¬è©¦é›†ï¼š{X_test_demo.shape}")

# ä½¿ç”¨æœ€ä½³æ¨¡å‹é€²è¡Œé æ¸¬
print("\nä½¿ç”¨éš¨æ©Ÿæ£®æ—å’Œ XGBoost é€²è¡Œé æ¸¬...")

# éš¨æ©Ÿæ£®æ—
rf_demo = RandomForestRegressor(n_estimators=200, max_depth=15, random_state=RANDOM_STATE, n_jobs=-1)
rf_demo.fit(X_train_demo, y_train_demo)
y_pred_rf = rf_demo.predict(X_test_demo)
r2_rf = r2_score(y_test_demo, y_pred_rf)

print(f"\néš¨æ©Ÿæ£®æ—ï¼š")
print(f"  RÂ²: {r2_rf:.4f}")
print(f"  RMSE: {np.sqrt(mean_squared_error(y_test_demo, y_pred_rf)):.2f}")

if XGB_AVAILABLE:
    # XGBoost
    xgb_demo = xgb.XGBRegressor(n_estimators=200, max_depth=7, learning_rate=0.1,
                               random_state=RANDOM_STATE, n_jobs=-1)
    xgb_demo.fit(X_train_demo, y_train_demo)
    y_pred_xgb = xgb_demo.predict(X_test_demo)
    r2_xgb = r2_score(y_test_demo, y_pred_xgb)

    print(f"\nXGBoostï¼š")
    print(f"  RÂ²: {r2_xgb:.4f}")
    print(f"  RMSE: {np.sqrt(mean_squared_error(y_test_demo, y_pred_xgb)):.2f}")

# ============================================================================
# 8.1 å¯è¦–åŒ–ï¼šæ‡‰ç”¨æ¡ˆä¾‹çµæœ
# ============================================================================
print("\nã€8.1ã€‘ç”Ÿæˆå¯è¦–åŒ–ï¼šæ‡‰ç”¨æ¡ˆä¾‹çµæœ")

if XGB_AVAILABLE:
    fig, axes = create_subplots(1, 2, figsize=(16, 6))

    # éš¨æ©Ÿæ£®æ—
    axes[0].scatter(y_test_demo, y_pred_rf, alpha=0.5, s=30, edgecolors='k', linewidth=0.5)
    axes[0].plot([y_test_demo.min(), y_test_demo.max()],
                [y_test_demo.min(), y_test_demo.max()], 'r--', lw=2)
    axes[0].set_xlabel('çœŸå¯¦å€¼ / True Values', fontsize=12)
    axes[0].set_ylabel('é æ¸¬å€¼ / Predictions', fontsize=12)
    axes[0].set_title(f'éš¨æ©Ÿæ£®æ— - æ‡‰ç”¨æ¡ˆä¾‹\nRandom Forest Application\nRÂ² = {r2_rf:.4f}',
                     fontsize=13, fontweight='bold')
    axes[0].grid(True, alpha=0.3)

    # XGBoost
    axes[1].scatter(y_test_demo, y_pred_xgb, alpha=0.5, s=30, edgecolors='k', linewidth=0.5)
    axes[1].plot([y_test_demo.min(), y_test_demo.max()],
                [y_test_demo.min(), y_test_demo.max()], 'r--', lw=2)
    axes[1].set_xlabel('çœŸå¯¦å€¼ / True Values', fontsize=12)
    axes[1].set_ylabel('é æ¸¬å€¼ / Predictions', fontsize=12)
    axes[1].set_title(f'XGBoost - æ‡‰ç”¨æ¡ˆä¾‹\nXGBoost Application\nRÂ² = {r2_xgb:.4f}',
                     fontsize=13, fontweight='bold')
    axes[1].grid(True, alpha=0.3)

    plt.tight_layout()
    save_figure(fig, get_output_path('application_case_comparison.png', 'Regression'))
else:
    fig, ax = create_subplots(1, 1, figsize=(10, 6))

    ax.scatter(y_test_demo, y_pred_rf, alpha=0.5, s=30, edgecolors='k', linewidth=0.5)
    ax.plot([y_test_demo.min(), y_test_demo.max()],
           [y_test_demo.min(), y_test_demo.max()], 'r--', lw=2)
    ax.set_xlabel('çœŸå¯¦å€¼ / True Values', fontsize=12)
    ax.set_ylabel('é æ¸¬å€¼ / Predictions', fontsize=12)
    ax.set_title(f'éš¨æ©Ÿæ£®æ— - æ‡‰ç”¨æ¡ˆä¾‹\nRandom Forest Application\nRÂ² = {r2_rf:.4f}',
                fontsize=13, fontweight='bold')
    ax.grid(True, alpha=0.3)

    plt.tight_layout()
    save_figure(fig, get_output_path('application_case_comparison.png', 'Regression'))

# ============================================================================
# ç¸½çµå ±å‘Š
# Summary Report
# ============================================================================
print("\n" + "=" * 80)
print("åŸºæ–¼æ¨¹çš„å›æ­¸æ•™ç¨‹ç¸½çµ".center(80))
print("=" * 80)

print("""
ğŸ“Š æœ¬æ•™ç¨‹æ¶µè“‹çš„å…§å®¹ï¼š

1. æ±ºç­–æ¨¹å›æ­¸ï¼ˆDecision Tree Regressorï¼‰
   âœ“ æ¨¹çš„ç”Ÿé•·å’Œå‰ªæ
   âœ“ max_depth åƒæ•¸å½±éŸ¿åˆ†æ
   âœ“ æ¨¹çµæ§‹å¯è¦–åŒ–
   âœ“ éæ“¬åˆå•é¡Œè­˜åˆ¥

2. éš¨æ©Ÿæ£®æ—å›æ­¸ï¼ˆRandom Forest Regressorï¼‰
   âœ“ Bagging é›†æˆåŸç†
   âœ“ n_estimators å’Œ max_depth èª¿å„ª
   âœ“ ç‰¹å¾µé‡è¦æ€§ï¼ˆå…§ç½® + Permutationï¼‰
   âœ“ OOB è©•ä¼°

3. æ¢¯åº¦æå‡å›æ­¸ï¼ˆGradient Boosting Regressorï¼‰
   âœ“ Boosting æå‡åŸç†
   âœ“ æ®˜å·®æ“¬åˆæ©Ÿåˆ¶
   âœ“ learning_rate å’Œ n_estimators æ¬Šè¡¡
   âœ“ å­¸ç¿’æ›²ç·šåˆ†æ
""")

if XGB_AVAILABLE:
    print("""
4. XGBoost å›æ­¸
   âœ“ æ­£å‰‡åŒ–å’ŒäºŒéšå°æ•¸å„ªåŒ–
   âœ“ åƒæ•¸èª¿å„ªï¼ˆmax_depth, learning_rateï¼‰
   âœ“ 3ç¨®ç‰¹å¾µé‡è¦æ€§ï¼ˆWeight, Gain, Coverï¼‰
   âœ“ èˆ‡ Gradient Boosting å°æ¯”
""")

if LGB_AVAILABLE:
    print("""
5. LightGBM å›æ­¸
   âœ“ Histogram-based ç®—æ³•
   âœ“ Leaf-wise ç”Ÿé•·ç­–ç•¥
   âœ“ è¨“ç·´é€Ÿåº¦å„ªå‹¢
""")

print("""
6. ç¶œåˆå°æ¯”
   âœ“ æ‰€æœ‰æ¨¡å‹æ€§èƒ½è©•ä¼°ï¼ˆRÂ², RMSE, MAEï¼‰
   âœ“ è¨“ç·´æ™‚é–“å’Œé æ¸¬æ™‚é–“å°æ¯”
   âœ“ é æ¸¬ vs çœŸå¯¦å€¼åˆ†æ
   âœ“ æ®˜å·®åˆ†æ

7. å¯¦éš›æ‡‰ç”¨
   âœ“ å®Œæ•´å»ºæ¨¡æµç¨‹æ¼”ç¤º
   âœ“ æ¨¡å‹é¸æ“‡å»ºè­°

ğŸ“ˆ ç”Ÿæˆçš„å¯è¦–åŒ–åœ–è¡¨ï¼š
""")

print("â€¢ decision_tree_depth_visualization.png - æ±ºç­–æ¨¹çµæ§‹å¯è¦–åŒ–")
print("â€¢ decision_tree_depth_analysis.png - æ·±åº¦åƒæ•¸å½±éŸ¿")
print("â€¢ random_forest_parameter_tuning.png - éš¨æ©Ÿæ£®æ—åƒæ•¸èª¿å„ª")
print("â€¢ random_forest_feature_importance.png - éš¨æ©Ÿæ£®æ—ç‰¹å¾µé‡è¦æ€§")
print("â€¢ gradient_boosting_learning_curve.png - æ¢¯åº¦æå‡å­¸ç¿’æ›²ç·š")
if XGB_AVAILABLE:
    print("â€¢ xgboost_feature_importance.png - XGBoost ç‰¹å¾µé‡è¦æ€§")
print("â€¢ all_models_performance_comparison.png - æ‰€æœ‰æ¨¡å‹æ€§èƒ½å°æ¯”")
print("â€¢ predictions_vs_actual.png - é æ¸¬vsçœŸå¯¦å€¼")
print("â€¢ residuals_analysis.png - æ®˜å·®åˆ†æ")
print("â€¢ application_case_comparison.png - å¯¦éš›æ‡‰ç”¨æ¡ˆä¾‹")

print("""
ğŸ’¡ é—œéµè¦é»ï¼š

æ¨¡å‹é¸æ“‡å»ºè­°ï¼š
â€¢ æ±ºç­–æ¨¹ï¼šé©åˆå¿«é€ŸåŸå‹ã€å¯è§£é‡‹æ€§è¦æ±‚é«˜
â€¢ éš¨æ©Ÿæ£®æ—ï¼šç©©å®šæ€§å¥½ã€æ˜“æ–¼ä½¿ç”¨ã€é¦–é¸æ–¹æ¡ˆ
â€¢ æ¢¯åº¦æå‡ï¼šæº–ç¢ºåº¦æœ€é«˜ï¼Œä½†éœ€è¦èª¿åƒ
â€¢ XGBoostï¼šå¤§è¦æ¨¡æ•¸æ“šã€ç«¶è³½é¦–é¸
â€¢ LightGBMï¼šè¶…å¤§è¦æ¨¡æ•¸æ“šã€é€Ÿåº¦è¦æ±‚é«˜

åƒæ•¸èª¿å„ªå»ºè­°ï¼š
1. æ±ºç­–æ¨¹ï¼šå…ˆèª¿ max_depthï¼ˆ3-10ï¼‰ï¼Œå†èª¿ min_samples_split
2. éš¨æ©Ÿæ£®æ—ï¼šn_estimatorsï¼ˆ100-500ï¼‰ï¼Œmax_depthï¼ˆ10-20ï¼‰
3. æ¢¯åº¦æå‡ï¼šlearning_rate å’Œ n_estimators éœ€è¦æ¬Šè¡¡
   - å° learning_rate + å¤§ n_estimators = æ›´å¥½æ€§èƒ½ä½†è¨“ç·´æ…¢
4. XGBoost/LightGBMï¼šä½¿ç”¨ early_stopping è‡ªå‹•ç¢ºå®šæœ€å„ªè¿­ä»£æ¬¡æ•¸

é˜²æ­¢éæ“¬åˆï¼š
â€¢ é™åˆ¶æ¨¹çš„æ·±åº¦ï¼ˆmax_depthï¼‰
â€¢ å¢åŠ æœ€å°æ¨£æœ¬æ•¸ï¼ˆmin_samples_split, min_samples_leafï¼‰
â€¢ ä½¿ç”¨æ­£å‰‡åŒ–ï¼ˆXGBoost çš„ reg_alpha, reg_lambdaï¼‰
â€¢ ä½¿ç”¨äº¤å‰é©—è­‰é¸æ“‡åƒæ•¸

ğŸ¯ ä¸‹ä¸€æ­¥ï¼š
â€¢ å­¸ç¿’æ¨¡å‹é›†æˆå’Œå †ç–Šï¼ˆStacking, Blendingï¼‰
â€¢ æ¢ç´¢æ·±åº¦å­¸ç¿’å›æ­¸æ–¹æ³•
â€¢ å¯¦è¸ Kaggle å›æ­¸ç«¶è³½
â€¢ å­¸ç¿’è¶…åƒæ•¸å„ªåŒ–ï¼ˆOptuna, Hyperoptï¼‰
""")

print("=" * 80)
print("æ•™ç¨‹çµæŸï¼æ‰€æœ‰åœ–è¡¨å·²ä¿å­˜åˆ° output/Regression/ ç›®éŒ„".center(80))
print("=" * 80)
