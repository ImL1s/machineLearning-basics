"""
éç·šæ€§å›æ­¸ï¼ˆNonlinear Regressionï¼‰
è™•ç†éç·šæ€§é—œä¿‚çš„å›æ­¸ç®—æ³•

åŒ…å«ï¼š
- å¤šé …å¼å›æ­¸ï¼ˆPolynomial Regressionï¼‰
- æ”¯æŒå‘é‡å›æ­¸ï¼ˆSVR - Support Vector Regressionï¼‰
- æ¨£æ¢å›æ­¸ï¼ˆSpline Regressionï¼‰
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import PolynomialFeatures, StandardScaler
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.svm import SVR
from sklearn.model_selection import train_test_split, GridSearchCV, learning_curve
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from sklearn.datasets import load_diabetes, make_regression
from scipy.interpolate import make_interp_spline, BSpline, UnivariateSpline
import warnings
warnings.filterwarnings('ignore')

# å°å…¥å·¥å…·æ¨¡å¡Š / Import utility modules
import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent.parent))
from utils import RANDOM_STATE, TEST_SIZE, setup_chinese_fonts, create_subplots, get_output_path, save_figure

# è¨­ç½®ä¸­æ–‡å­—é«” / Setup Chinese fonts
setup_chinese_fonts()

print("=" * 80)
print("éç·šæ€§å›æ­¸ï¼ˆNonlinear Regressionï¼‰æ•™ç¨‹".center(80))
print("=" * 80)

# ============================================================================
# ç¬¬ä¸€éƒ¨åˆ†ï¼šéç·šæ€§å›æ­¸æ¦‚è¿°
# Part 1: Nonlinear Regression Overview
# ============================================================================
print("\nã€ç¬¬ä¸€éƒ¨åˆ†ã€‘éç·šæ€§å›æ­¸æ¦‚è¿°")
print("-" * 80)
print("""
ä»€éº¼æ˜¯éç·šæ€§å›æ­¸ï¼Ÿ
What is Nonlinear Regression?

ç•¶å› è®Šé‡å’Œè‡ªè®Šé‡ä¹‹é–“ä¸æ˜¯ç·šæ€§é—œä¿‚æ™‚ï¼Œéœ€è¦ä½¿ç”¨éç·šæ€§å›æ­¸ã€‚
When the relationship between dependent and independent variables is not linear,
we need nonlinear regression.

èˆ‡ç·šæ€§å›æ­¸çš„å€åˆ¥ï¼š
Difference from Linear Regression:
â€¢ ç·šæ€§å›æ­¸ï¼šy = Î²â‚€ + Î²â‚xâ‚ + Î²â‚‚xâ‚‚ + ... (ç·šæ€§çµ„åˆ)
â€¢ éç·šæ€§å›æ­¸ï¼šy = f(x) (å¯ä»¥æ˜¯ä»»ä½•éç·šæ€§å‡½æ•¸)

å¸¸è¦‹éç·šæ€§å›æ­¸æ–¹æ³•ï¼š
Common Nonlinear Regression Methods:
1. å¤šé …å¼å›æ­¸ï¼ˆPolynomial Regressionï¼‰- ä½¿ç”¨å¤šé …å¼ç‰¹å¾µ
2. æ”¯æŒå‘é‡å›æ­¸ï¼ˆSVRï¼‰- ä½¿ç”¨æ ¸æŠ€å·§æ˜ å°„åˆ°é«˜ç¶­ç©ºé–“
3. æ¨£æ¢å›æ­¸ï¼ˆSpline Regressionï¼‰- åˆ†æ®µå¤šé …å¼æ“¬åˆ
4. æ±ºç­–æ¨¹å›æ­¸ï¼ˆä¸‹ä¸€å€‹æ•™ç¨‹ï¼‰
5. ç¥ç¶“ç¶²çµ¡å›æ­¸ï¼ˆæ·±åº¦å­¸ç¿’ç« ç¯€ï¼‰
""")

# ============================================================================
# è©•ä¼°å‡½æ•¸ / Evaluation Functions
# ============================================================================
def evaluate_regression(y_true, y_pred, model_name='Model'):
    """
    è¨ˆç®—å›æ­¸è©•ä¼°æŒ‡æ¨™
    Calculate regression evaluation metrics

    Args:
        y_true: çœŸå¯¦å€¼ / True values
        y_pred: é æ¸¬å€¼ / Predicted values
        model_name: æ¨¡å‹åç¨± / Model name

    Returns:
        dict: åŒ…å« RÂ², RMSE, MAE çš„å­—å…¸
    """
    r2 = r2_score(y_true, y_pred)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    mae = mean_absolute_error(y_true, y_pred)

    print(f"\n{model_name}:")
    print(f"  RÂ² Score: {r2:.4f}")
    print(f"  RMSE: {rmse:.4f}")
    print(f"  MAE: {mae:.4f}")

    return {'r2': r2, 'rmse': rmse, 'mae': mae}

# ============================================================================
# ç¬¬äºŒéƒ¨åˆ†ï¼šå¤šé …å¼å›æ­¸ï¼ˆPolynomial Regressionï¼‰
# Part 2: Polynomial Regression
# ============================================================================
print("\nã€ç¬¬äºŒéƒ¨åˆ†ã€‘å¤šé …å¼å›æ­¸ï¼ˆPolynomial Regressionï¼‰")
print("-" * 80)
print("""
å¤šé …å¼å›æ­¸åŸç†ï¼š
Polynomial Regression Principle:

å°‡ç‰¹å¾µé€²è¡Œå¤šé …å¼è½‰æ›ï¼Œç„¶å¾Œä½¿ç”¨ç·šæ€§å›æ­¸æ“¬åˆã€‚
Transform features into polynomial features, then fit with linear regression.

ä¾‹å¦‚ï¼Œå°æ–¼å–®è®Šé‡ xï¼š
For example, for single variable x:
â€¢ 1æ¬¡ï¼ˆç·šæ€§ï¼‰: y = Î²â‚€ + Î²â‚x
â€¢ 2æ¬¡ï¼ˆäºŒæ¬¡ï¼‰: y = Î²â‚€ + Î²â‚x + Î²â‚‚xÂ²
â€¢ 3æ¬¡ï¼ˆä¸‰æ¬¡ï¼‰: y = Î²â‚€ + Î²â‚x + Î²â‚‚xÂ² + Î²â‚ƒxÂ³
â€¢ næ¬¡ï¼šy = Î²â‚€ + Î²â‚x + Î²â‚‚xÂ² + ... + Î²â‚™xâ¿

PolynomialFeatures ä½¿ç”¨ï¼š
â€¢ degree: å¤šé …å¼éšæ•¸
â€¢ include_bias: æ˜¯å¦åŒ…å«æˆªè·é …
â€¢ interaction_only: æ˜¯å¦åªåŒ…å«äº¤äº’é …

æ³¨æ„äº‹é …ï¼š
Cautions:
âš  éšæ•¸éé«˜æœƒå°è‡´éæ“¬åˆ
âš  éœ€è¦ç‰¹å¾µç¸®æ”¾ï¼ˆStandardScalerï¼‰
âš  ç‰¹å¾µæ•¸é‡éš¨éšæ•¸æŒ‡æ•¸å¢é•·
""")

# ç”Ÿæˆéç·šæ€§æ•¸æ“š / Generate nonlinear data
np.random.seed(RANDOM_STATE)
n_samples = 100

# å‰µå»ºå…·æœ‰éç·šæ€§é—œä¿‚çš„æ•¸æ“š
X_poly = np.linspace(-3, 3, n_samples).reshape(-1, 1)
y_poly = 0.5 * X_poly.ravel()**3 - 2 * X_poly.ravel()**2 + X_poly.ravel() + np.random.randn(n_samples) * 2

# æ•¸æ“šåˆ†å‰²
X_train, X_test, y_train, y_test = train_test_split(
    X_poly, y_poly, test_size=TEST_SIZE, random_state=RANDOM_STATE
)

print(f"\nè¨“ç·´é›†å¤§å°ï¼š{X_train.shape}")
print(f"æ¸¬è©¦é›†å¤§å°ï¼š{X_test.shape}")

# ============================================================================
# 2.1 ä¸åŒéšæ•¸çš„å¤šé …å¼å°æ¯”
# ============================================================================
print("\nã€2.1ã€‘ä¸åŒéšæ•¸çš„å¤šé …å¼å°æ¯”")
print("-" * 80)

# æ¸¬è©¦ä¸åŒéšæ•¸çš„å¤šé …å¼
degrees = [1, 2, 3, 5, 10]
polynomial_results = {}

for degree in degrees:
    # å¤šé …å¼ç‰¹å¾µè½‰æ›
    poly_features = PolynomialFeatures(degree=degree, include_bias=False)
    X_train_poly = poly_features.fit_transform(X_train)
    X_test_poly = poly_features.transform(X_test)

    # è¨“ç·´ç·šæ€§å›æ­¸æ¨¡å‹
    model = LinearRegression()
    model.fit(X_train_poly, y_train)

    # é æ¸¬
    y_train_pred = model.predict(X_train_poly)
    y_test_pred = model.predict(X_test_poly)

    # è©•ä¼°
    train_r2 = r2_score(y_train, y_train_pred)
    test_r2 = r2_score(y_test, y_test_pred)
    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))

    polynomial_results[degree] = {
        'model': model,
        'poly_features': poly_features,
        'train_r2': train_r2,
        'test_r2': test_r2,
        'rmse': test_rmse,
        'y_train_pred': y_train_pred,
        'y_test_pred': y_test_pred
    }

    print(f"\nå¤šé …å¼éšæ•¸ {degree}:")
    print(f"  è¨“ç·´é›† RÂ²: {train_r2:.4f}")
    print(f"  æ¸¬è©¦é›† RÂ²: {test_r2:.4f}")
    print(f"  æ¸¬è©¦é›† RMSE: {test_rmse:.4f}")
    print(f"  ç‰¹å¾µæ•¸é‡: {X_train_poly.shape[1]}")

    if abs(train_r2 - test_r2) > 0.1:
        print(f"  âš  è­¦å‘Šï¼šå¯èƒ½å­˜åœ¨éæ“¬åˆï¼ˆè¨“ç·´é›†å’Œæ¸¬è©¦é›†RÂ²å·®ç•°: {abs(train_r2 - test_r2):.4f}ï¼‰")

# ============================================================================
# 2.2 å¯è¦–åŒ–ï¼šå¤šé …å¼å›æ­¸æ‹Ÿåˆæ›²ç·šï¼ˆ2x3å­åœ–ï¼‰
# ============================================================================
print("\nã€2.2ã€‘ç”Ÿæˆå¯è¦–åŒ–ï¼šå¤šé …å¼å›æ­¸æ‹Ÿåˆæ›²ç·š")

fig, axes = create_subplots(2, 3, figsize=(18, 12))
axes = axes.flatten()

# ç”¨æ–¼ç¹ªè£½å¹³æ»‘æ›²ç·šçš„é»
X_plot = np.linspace(X_poly.min(), X_poly.max(), 300).reshape(-1, 1)

for idx, degree in enumerate(degrees):
    result = polynomial_results[degree]

    # è½‰æ›ç¹ªåœ–æ•¸æ“š
    X_plot_poly = result['poly_features'].transform(X_plot)
    y_plot = result['model'].predict(X_plot_poly)

    # ç¹ªè£½
    axes[idx].scatter(X_train, y_train, alpha=0.5, s=30, label='è¨“ç·´æ•¸æ“š', color='blue')
    axes[idx].scatter(X_test, y_test, alpha=0.5, s=30, label='æ¸¬è©¦æ•¸æ“š', color='green')
    axes[idx].plot(X_plot, y_plot, 'r-', linewidth=2, label=f'å¤šé …å¼æ“¬åˆ (degree={degree})')

    axes[idx].set_xlabel('X', fontsize=11)
    axes[idx].set_ylabel('y', fontsize=11)
    axes[idx].set_title(f'å¤šé …å¼å›æ­¸ - éšæ•¸ {degree}\nTrain RÂ²={result["train_r2"]:.3f}, Test RÂ²={result["test_r2"]:.3f}',
                       fontsize=12, fontweight='bold')
    axes[idx].legend(fontsize=9)
    axes[idx].grid(True, alpha=0.3)

# æœ€å¾Œä¸€å€‹å­åœ–ï¼šå°æ¯”æ‰€æœ‰æ¨¡å‹
for degree in degrees:
    result = polynomial_results[degree]
    X_plot_poly = result['poly_features'].transform(X_plot)
    y_plot = result['model'].predict(X_plot_poly)
    axes[5].plot(X_plot, y_plot, linewidth=2, label=f'Degree {degree}', alpha=0.7)

axes[5].scatter(X_train, y_train, alpha=0.3, s=20, color='gray', label='æ•¸æ“šé»')
axes[5].set_xlabel('X', fontsize=11)
axes[5].set_ylabel('y', fontsize=11)
axes[5].set_title('æ‰€æœ‰å¤šé …å¼éšæ•¸å°æ¯”\nAll Polynomial Degrees Comparison', fontsize=12, fontweight='bold')
axes[5].legend(fontsize=9)
axes[5].grid(True, alpha=0.3)

plt.tight_layout()
save_figure(fig, get_output_path('polynomial_regression_comparison.png', 'Regression'))

# ============================================================================
# 2.3 éæ“¬åˆè­˜åˆ¥
# ============================================================================
print("\nã€2.3ã€‘éæ“¬åˆè­˜åˆ¥")
print("-" * 80)

fig, axes = create_subplots(1, 2, figsize=(16, 6))

# å·¦åœ–ï¼šè¨“ç·´èª¤å·® vs æ¸¬è©¦èª¤å·®
train_r2_scores = [polynomial_results[d]['train_r2'] for d in degrees]
test_r2_scores = [polynomial_results[d]['test_r2'] for d in degrees]

axes[0].plot(degrees, train_r2_scores, 'o-', linewidth=2, markersize=8, label='è¨“ç·´é›† RÂ²', color='blue')
axes[0].plot(degrees, test_r2_scores, 's-', linewidth=2, markersize=8, label='æ¸¬è©¦é›† RÂ²', color='red')
axes[0].set_xlabel('å¤šé …å¼éšæ•¸ / Polynomial Degree', fontsize=12)
axes[0].set_ylabel('RÂ² Score', fontsize=12)
axes[0].set_title('è¨“ç·´èª¤å·® vs æ¸¬è©¦èª¤å·®\nTraining vs Testing Error', fontsize=13, fontweight='bold')
axes[0].legend(fontsize=11)
axes[0].grid(True, alpha=0.3)
axes[0].axvline(x=3, color='green', linestyle='--', alpha=0.5, label='æœ€ä½³éšæ•¸')

# å³åœ–ï¼šRMSE
rmse_scores = [polynomial_results[d]['rmse'] for d in degrees]
axes[1].bar(degrees, rmse_scores, color='steelblue', alpha=0.7, edgecolor='black')
axes[1].set_xlabel('å¤šé …å¼éšæ•¸ / Polynomial Degree', fontsize=12)
axes[1].set_ylabel('RMSE', fontsize=12)
axes[1].set_title('æ¸¬è©¦é›† RMSE vs å¤šé …å¼éšæ•¸\nTest RMSE vs Polynomial Degree', fontsize=13, fontweight='bold')
axes[1].grid(True, alpha=0.3, axis='y')

# æ¨™è¨»æœ€å°å€¼
min_idx = np.argmin(rmse_scores)
axes[1].bar(degrees[min_idx], rmse_scores[min_idx], color='green', alpha=0.7, edgecolor='black')

plt.tight_layout()
save_figure(fig, get_output_path('polynomial_overfitting_analysis.png', 'Regression'))

print("\nè§€å¯Ÿçµæœï¼š")
print("â€¢ éšæ•¸å¤ªä½ï¼ˆ1æ¬¡ï¼‰ï¼šæ¬ æ“¬åˆï¼Œè¨“ç·´å’Œæ¸¬è©¦èª¤å·®éƒ½é«˜")
print("â€¢ éšæ•¸é©ä¸­ï¼ˆ2-3æ¬¡ï¼‰ï¼šæœ€ä½³ï¼Œæ¸¬è©¦èª¤å·®æœ€å°")
print("â€¢ éšæ•¸å¤ªé«˜ï¼ˆ10æ¬¡ï¼‰ï¼šéæ“¬åˆï¼Œè¨“ç·´èª¤å·®å¾ˆä½ä½†æ¸¬è©¦èª¤å·®é«˜")

# ============================================================================
# ç¬¬ä¸‰éƒ¨åˆ†ï¼šæ”¯æŒå‘é‡å›æ­¸ï¼ˆSVR - Support Vector Regressionï¼‰
# Part 3: Support Vector Regression
# ============================================================================
print("\nã€ç¬¬ä¸‰éƒ¨åˆ†ã€‘æ”¯æŒå‘é‡å›æ­¸ï¼ˆSVRï¼‰")
print("-" * 80)
print("""
SVR åŸç†ï¼š
SVR Principle:

æ”¯æŒå‘é‡å›æ­¸æ˜¯ SVM åœ¨å›æ­¸å•é¡Œä¸Šçš„æ‡‰ç”¨ã€‚
Support Vector Regression is the application of SVM to regression problems.

æ ¸å¿ƒæ¦‚å¿µï¼š
Core Concepts:
1. Îµ-ä¸æ•æ„Ÿæå¤±å‡½æ•¸ï¼ˆÎµ-insensitive lossï¼‰
   - åªæœ‰ç•¶èª¤å·® > Îµ æ™‚æ‰è¨ˆç®—æå¤±
   - åœ¨ Îµ ç¯„åœå…§çš„é»ä¸è²¢ç»æå¤±

2. æ ¸æŠ€å·§ï¼ˆKernel Trickï¼‰
   - å°‡æ•¸æ“šæ˜ å°„åˆ°é«˜ç¶­ç©ºé–“
   - åœ¨é«˜ç¶­ç©ºé–“ä¸­é€²è¡Œç·šæ€§å›æ­¸

å¸¸ç”¨æ ¸å‡½æ•¸ï¼š
Common Kernel Functions:
â€¢ Linear: K(x, x') = x^T x'
â€¢ Polynomial: K(x, x') = (Î³x^T x' + r)^d
â€¢ RBF (é«˜æ–¯): K(x, x') = exp(-Î³||x - x'||Â²)
â€¢ Sigmoid: K(x, x') = tanh(Î³x^T x' + r)

é—œéµåƒæ•¸ï¼š
Key Parameters:
â€¢ C: æ‡²ç½°åƒæ•¸ï¼Œæ§åˆ¶å°èª¤å·®çš„å®¹å¿åº¦
â€¢ epsilon (Îµ): ä¸æ•æ„Ÿå€åŸŸçš„å¯¬åº¦
â€¢ gamma (Î³): RBFã€Polyã€Sigmoid æ ¸çš„åƒæ•¸

å„ªé»ï¼š
âœ“ å¯è™•ç†éç·šæ€§é—œä¿‚
âœ“ å°é«˜ç¶­æ•¸æ“šæœ‰æ•ˆ
âœ“ æ³›åŒ–èƒ½åŠ›å¼·

ç¼ºé»ï¼š
âœ— è¨“ç·´æ™‚é–“é•·ï¼ˆå¤§æ•¸æ“šé›†ï¼‰
âœ— éœ€è¦é¸æ“‡åˆé©çš„æ ¸å‡½æ•¸å’Œåƒæ•¸
âœ— éœ€è¦ç‰¹å¾µç¸®æ”¾
""")

# åŠ è¼‰ç³–å°¿ç—…æ•¸æ“šé›† / Load diabetes dataset
diabetes = load_diabetes()
X_diabetes, y_diabetes = diabetes.data, diabetes.target

# æ•¸æ“šæ¨™æº–åŒ–ï¼ˆSVR éœ€è¦ï¼‰
scaler = StandardScaler()
X_train_scaled, X_test_scaled, y_train_svr, y_test_svr = train_test_split(
    X_diabetes, y_diabetes, test_size=TEST_SIZE, random_state=RANDOM_STATE
)
X_train_scaled = scaler.fit_transform(X_train_scaled)
X_test_scaled = scaler.transform(X_test_scaled)

print(f"\næ•¸æ“šé›†ï¼š{diabetes.DESCR.split('Diabetes dataset')[0]}")
print(f"ç‰¹å¾µæ•¸é‡ï¼š{X_diabetes.shape[1]}")
print(f"æ¨£æœ¬æ•¸é‡ï¼š{X_diabetes.shape[0]}")

# ============================================================================
# 3.1 ä¸åŒæ ¸å‡½æ•¸å°æ¯”
# ============================================================================
print("\nã€3.1ã€‘ä¸åŒæ ¸å‡½æ•¸å°æ¯”")
print("-" * 80)

kernels = {
    'Linear': SVR(kernel='linear', C=1.0),
    'Polynomial (d=2)': SVR(kernel='poly', degree=2, C=1.0),
    'Polynomial (d=3)': SVR(kernel='poly', degree=3, C=1.0),
    'RBF': SVR(kernel='rbf', C=1.0, gamma='scale'),
    'Sigmoid': SVR(kernel='sigmoid', C=1.0, gamma='scale')
}

svr_results = {}

for name, model in kernels.items():
    # è¨“ç·´
    model.fit(X_train_scaled, y_train_svr)

    # é æ¸¬
    y_train_pred = model.predict(X_train_scaled)
    y_test_pred = model.predict(X_test_scaled)

    # è©•ä¼°
    results = evaluate_regression(y_test_svr, y_test_pred, f"SVR ({name})")
    results['train_r2'] = r2_score(y_train_svr, y_train_pred)
    results['y_pred'] = y_test_pred

    svr_results[name] = results

# ============================================================================
# 3.2 å¯è¦–åŒ–ï¼šSVR ä¸åŒæ ¸å‡½æ•¸å°æ¯”ï¼ˆ2x3å­åœ–ï¼‰
# ============================================================================
print("\nã€3.2ã€‘ç”Ÿæˆå¯è¦–åŒ–ï¼šSVR æ ¸å‡½æ•¸å°æ¯”")

fig, axes = create_subplots(2, 3, figsize=(18, 12))
axes = axes.flatten()

for idx, (name, results) in enumerate(svr_results.items()):
    # é æ¸¬ vs çœŸå¯¦å€¼æ•£é»åœ–
    axes[idx].scatter(y_test_svr, results['y_pred'], alpha=0.6, s=50, edgecolors='k', linewidth=0.5)
    axes[idx].plot([y_test_svr.min(), y_test_svr.max()],
                   [y_test_svr.min(), y_test_svr.max()],
                   'r--', lw=2, label='å®Œç¾é æ¸¬ç·š')

    axes[idx].set_xlabel('çœŸå¯¦å€¼ / True Values', fontsize=11)
    axes[idx].set_ylabel('é æ¸¬å€¼ / Predictions', fontsize=11)
    axes[idx].set_title(f'SVR - {name}\nRÂ²={results["r2"]:.3f}, RMSE={results["rmse"]:.2f}',
                       fontsize=12, fontweight='bold')
    axes[idx].legend(fontsize=9)
    axes[idx].grid(True, alpha=0.3)

# æœ€å¾Œä¸€å€‹å­åœ–ï¼šæ€§èƒ½å°æ¯”æŸ±ç‹€åœ–
kernel_names = list(svr_results.keys())
r2_scores = [svr_results[k]['r2'] for k in kernel_names]
rmse_scores = [svr_results[k]['rmse'] for k in kernel_names]

x_pos = np.arange(len(kernel_names))
width = 0.35

axes[5].bar(x_pos - width/2, r2_scores, width, label='RÂ² Score', alpha=0.8, color='steelblue')
axes[5].bar(x_pos + width/2, [r/100 for r in rmse_scores], width, label='RMSE/100', alpha=0.8, color='coral')
axes[5].set_xlabel('æ ¸å‡½æ•¸é¡å‹ / Kernel Type', fontsize=11)
axes[5].set_ylabel('åˆ†æ•¸ / Score', fontsize=11)
axes[5].set_title('SVR æ ¸å‡½æ•¸æ€§èƒ½å°æ¯”\nSVR Kernel Performance Comparison', fontsize=12, fontweight='bold')
axes[5].set_xticks(x_pos)
axes[5].set_xticklabels(kernel_names, rotation=15, ha='right', fontsize=9)
axes[5].legend(fontsize=10)
axes[5].grid(True, alpha=0.3, axis='y')

plt.tight_layout()
save_figure(fig, get_output_path('svr_kernel_comparison.png', 'Regression'))

# ============================================================================
# 3.3 SVR åƒæ•¸èª¿å„ª - C åƒæ•¸å½±éŸ¿
# ============================================================================
print("\nã€3.3ã€‘SVR åƒæ•¸èª¿å„ª - C åƒæ•¸å½±éŸ¿")
print("-" * 80)

C_values = [0.1, 1, 10, 100, 1000]
c_results = {}

for C in C_values:
    model = SVR(kernel='rbf', C=C, gamma='scale')
    model.fit(X_train_scaled, y_train_svr)
    y_pred = model.predict(X_test_scaled)

    r2 = r2_score(y_test_svr, y_pred)
    rmse = np.sqrt(mean_squared_error(y_test_svr, y_pred))

    c_results[C] = {'r2': r2, 'rmse': rmse}
    print(f"C={C:6}: RÂ²={r2:.4f}, RMSE={rmse:.2f}")

# ============================================================================
# 3.4 å¯è¦–åŒ–ï¼šåƒæ•¸å½±éŸ¿åˆ†æ
# ============================================================================
print("\nã€3.4ã€‘ç”Ÿæˆå¯è¦–åŒ–ï¼šSVR åƒæ•¸å½±éŸ¿")

fig, axes = create_subplots(1, 3, figsize=(18, 5))

# C åƒæ•¸å½±éŸ¿
C_list = list(c_results.keys())
C_r2 = [c_results[c]['r2'] for c in C_list]
C_rmse = [c_results[c]['rmse'] for c in C_list]

axes[0].semilogx(C_list, C_r2, 'o-', linewidth=2, markersize=8, color='blue')
axes[0].set_xlabel('C (æ‡²ç½°åƒæ•¸)', fontsize=12)
axes[0].set_ylabel('RÂ² Score', fontsize=12)
axes[0].set_title('C åƒæ•¸å° RÂ² çš„å½±éŸ¿\nEffect of C on RÂ²', fontsize=13, fontweight='bold')
axes[0].grid(True, alpha=0.3)

# epsilon åƒæ•¸å½±éŸ¿
epsilon_values = [0.01, 0.1, 0.5, 1.0, 2.0]
epsilon_r2 = []

for eps in epsilon_values:
    model = SVR(kernel='rbf', C=1.0, epsilon=eps, gamma='scale')
    model.fit(X_train_scaled, y_train_svr)
    y_pred = model.predict(X_test_scaled)
    epsilon_r2.append(r2_score(y_test_svr, y_pred))

axes[1].plot(epsilon_values, epsilon_r2, 'o-', linewidth=2, markersize=8, color='green')
axes[1].set_xlabel('Epsilon (Îµ)', fontsize=12)
axes[1].set_ylabel('RÂ² Score', fontsize=12)
axes[1].set_title('Epsilon åƒæ•¸å° RÂ² çš„å½±éŸ¿\nEffect of Epsilon on RÂ²', fontsize=13, fontweight='bold')
axes[1].grid(True, alpha=0.3)

# gamma åƒæ•¸å½±éŸ¿ï¼ˆRBF kernelï¼‰
gamma_values = [0.001, 0.01, 0.1, 1, 10]
gamma_r2 = []

for gamma in gamma_values:
    model = SVR(kernel='rbf', C=1.0, gamma=gamma)
    model.fit(X_train_scaled, y_train_svr)
    y_pred = model.predict(X_test_scaled)
    gamma_r2.append(r2_score(y_test_svr, y_pred))

axes[2].semilogx(gamma_values, gamma_r2, 'o-', linewidth=2, markersize=8, color='red')
axes[2].set_xlabel('Gamma (Î³)', fontsize=12)
axes[2].set_ylabel('RÂ² Score', fontsize=12)
axes[2].set_title('Gamma åƒæ•¸å° RÂ² çš„å½±éŸ¿ (RBF)\nEffect of Gamma on RÂ²', fontsize=13, fontweight='bold')
axes[2].grid(True, alpha=0.3)

plt.tight_layout()
save_figure(fig, get_output_path('svr_parameter_tuning.png', 'Regression'))

# ============================================================================
# ç¬¬å››éƒ¨åˆ†ï¼šæ¨£æ¢å›æ­¸ï¼ˆSpline Regressionï¼‰
# Part 4: Spline Regression
# ============================================================================
print("\nã€ç¬¬å››éƒ¨åˆ†ã€‘æ¨£æ¢å›æ­¸ï¼ˆSpline Regressionï¼‰")
print("-" * 80)
print("""
æ¨£æ¢å›æ­¸åŸç†ï¼š
Spline Regression Principle:

æ¨£æ¢å›æ­¸ä½¿ç”¨åˆ†æ®µå¤šé …å¼é€²è¡Œæ“¬åˆï¼Œåœ¨é€£æ¥é»ä¿æŒå¹³æ»‘ã€‚
Spline regression uses piecewise polynomials that are smooth at connection points.

ä¸»è¦é¡å‹ï¼š
Main Types:
1. ç·šæ€§æ¨£æ¢ï¼ˆLinear Splineï¼‰- åˆ†æ®µç·šæ€§
2. äºŒæ¬¡æ¨£æ¢ï¼ˆQuadratic Splineï¼‰- åˆ†æ®µäºŒæ¬¡å¤šé …å¼
3. ä¸‰æ¬¡æ¨£æ¢ï¼ˆCubic Splineï¼‰- åˆ†æ®µä¸‰æ¬¡å¤šé …å¼ï¼ˆæœ€å¸¸ç”¨ï¼‰
4. Bæ¨£æ¢ï¼ˆB-Splineï¼‰- åŸºç¤æ¨£æ¢
5. å¹³æ»‘æ¨£æ¢ï¼ˆSmoothing Splineï¼‰- å¸¶å¹³æ»‘æ‡²ç½°

å„ªé»ï¼š
âœ“ æ¯”é«˜éšå¤šé …å¼æ›´ç©©å®š
âœ“ å±€éƒ¨æ“¬åˆï¼Œä¸æœƒå‡ºç¾é¾æ ¼ç¾è±¡
âœ“ éˆæ´»æ€§é«˜

ç¼ºé»ï¼š
âœ— éœ€è¦é¸æ“‡ç¯€é»ä½ç½®
âœ— é‚Šç•Œè™•å¯èƒ½ä¸ç©©å®š
""")

# ä½¿ç”¨ä¹‹å‰çš„éç·šæ€§æ•¸æ“š
X_spline = X_poly
y_spline = y_poly

# æ’åºï¼ˆæ¨£æ¢æ’å€¼éœ€è¦ï¼‰
sort_idx = X_spline.ravel().argsort()
X_spline_sorted = X_spline.ravel()[sort_idx]
y_spline_sorted = y_spline[sort_idx]

# ============================================================================
# 4.1 ä¸åŒæ¨£æ¢æ–¹æ³•
# ============================================================================
print("\nã€4.1ã€‘ä¸åŒæ¨£æ¢æ–¹æ³•å°æ¯”")

# å‰µå»ºå¯†é›†é»ç”¨æ–¼ç¹ªè£½å¹³æ»‘æ›²ç·š
X_dense = np.linspace(X_spline.min(), X_spline.max(), 300)

# 1. B-Spline (cubic)
spl_cubic = make_interp_spline(X_spline_sorted, y_spline_sorted, k=3)
y_cubic = spl_cubic(X_dense)

# 2. Univariate Spline (smoothing spline)
spl_smooth = UnivariateSpline(X_spline_sorted, y_spline_sorted, s=50)
y_smooth = spl_smooth(X_dense)

# 3. å¤šé …å¼å›æ­¸ï¼ˆ3æ¬¡ï¼‰ä½œç‚ºå°æ¯”
poly_3 = PolynomialFeatures(degree=3)
X_poly_3 = poly_3.fit_transform(X_spline_sorted.reshape(-1, 1))
lr_poly = LinearRegression()
lr_poly.fit(X_poly_3, y_spline_sorted)
X_dense_poly = poly_3.transform(X_dense.reshape(-1, 1))
y_poly_3 = lr_poly.predict(X_dense_poly)

# ============================================================================
# 4.2 å¯è¦–åŒ–ï¼šæ¨£æ¢å›æ­¸å°æ¯”
# ============================================================================
print("\nã€4.2ã€‘ç”Ÿæˆå¯è¦–åŒ–ï¼šæ¨£æ¢å›æ­¸")

fig, axes = create_subplots(1, 2, figsize=(16, 6))

# å·¦åœ–ï¼šä¸åŒæ¨£æ¢æ–¹æ³•å°æ¯”
axes[0].scatter(X_spline, y_spline, alpha=0.5, s=30, label='åŸå§‹æ•¸æ“š', color='gray')
axes[0].plot(X_dense, y_cubic, linewidth=2, label='ä¸‰æ¬¡ B-Spline', color='blue')
axes[0].plot(X_dense, y_smooth, linewidth=2, label='å¹³æ»‘æ¨£æ¢ (s=50)', color='green')
axes[0].plot(X_dense, y_poly_3, linewidth=2, label='3æ¬¡å¤šé …å¼', color='red', linestyle='--')
axes[0].set_xlabel('X', fontsize=12)
axes[0].set_ylabel('y', fontsize=12)
axes[0].set_title('æ¨£æ¢å›æ­¸æ–¹æ³•å°æ¯”\nSpline Regression Methods Comparison', fontsize=13, fontweight='bold')
axes[0].legend(fontsize=11)
axes[0].grid(True, alpha=0.3)

# å³åœ–ï¼šä¸åŒå¹³æ»‘åƒæ•¸çš„å½±éŸ¿
smoothness_params = [10, 50, 100, 500]
for s in smoothness_params:
    spl = UnivariateSpline(X_spline_sorted, y_spline_sorted, s=s)
    y_spl = spl(X_dense)
    axes[1].plot(X_dense, y_spl, linewidth=2, label=f's={s}', alpha=0.7)

axes[1].scatter(X_spline, y_spline, alpha=0.3, s=20, color='gray', label='æ•¸æ“šé»')
axes[1].set_xlabel('X', fontsize=12)
axes[1].set_ylabel('y', fontsize=12)
axes[1].set_title('å¹³æ»‘åƒæ•¸ s çš„å½±éŸ¿\nEffect of Smoothing Parameter s', fontsize=13, fontweight='bold')
axes[1].legend(fontsize=11)
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
save_figure(fig, get_output_path('spline_regression.png', 'Regression'))

# ============================================================================
# ç¬¬äº”éƒ¨åˆ†ï¼šç¶œåˆå°æ¯”
# Part 5: Comprehensive Comparison
# ============================================================================
print("\nã€ç¬¬äº”éƒ¨åˆ†ã€‘ç¶œåˆå°æ¯” - æ‰€æœ‰éç·šæ€§æ–¹æ³•")
print("-" * 80)

# å‰µå»ºä¸åŒé¡å‹çš„æ¸¬è©¦æ•¸æ“š
np.random.seed(RANDOM_STATE)
n_test = 100

# æ•¸æ“šé›† 1ï¼šç°¡å–®éç·šæ€§ï¼ˆå–®å³°ï¼‰
X_simple = np.linspace(-3, 3, n_test).reshape(-1, 1)
y_simple = X_simple.ravel()**2 + np.random.randn(n_test) * 2

# æ•¸æ“šé›† 2ï¼šè¤‡é›œéç·šæ€§ï¼ˆå¤šå³°ï¼‰
X_complex = np.linspace(0, 4*np.pi, n_test).reshape(-1, 1)
y_complex = np.sin(X_complex.ravel()) * X_complex.ravel() + np.random.randn(n_test) * 0.5

# æ•¸æ“šé›† 3ï¼šå™ªè²æ•¸æ“š
X_noisy = np.linspace(-3, 3, n_test).reshape(-1, 1)
y_noisy = 0.5 * X_noisy.ravel()**3 + np.random.randn(n_test) * 5

datasets = {
    'ç°¡å–®éç·šæ€§ (å–®å³°)': (X_simple, y_simple),
    'è¤‡é›œéç·šæ€§ (å¤šå³°)': (X_complex, y_complex),
    'é«˜å™ªè²æ•¸æ“š': (X_noisy, y_noisy)
}

# ============================================================================
# 5.1 åœ¨ä¸åŒæ•¸æ“šé›†ä¸Šæ¸¬è©¦æ‰€æœ‰æ–¹æ³•
# ============================================================================
print("\nã€5.1ã€‘åœ¨ä¸åŒæ•¸æ“šé›†ä¸Šæ¸¬è©¦æ‰€æœ‰æ–¹æ³•")

comparison_results = {}

for dataset_name, (X_data, y_data) in datasets.items():
    print(f"\næ•¸æ“šé›†ï¼š{dataset_name}")
    print("-" * 40)

    # æ•¸æ“šåˆ†å‰²
    X_tr, X_te, y_tr, y_te = train_test_split(X_data, y_data, test_size=0.3, random_state=RANDOM_STATE)

    results = {}

    # 1. å¤šé …å¼å›æ­¸ (degree=3)
    poly = PolynomialFeatures(degree=3)
    X_tr_poly = poly.fit_transform(X_tr)
    X_te_poly = poly.transform(X_te)
    lr = LinearRegression()
    lr.fit(X_tr_poly, y_tr)
    y_pred = lr.predict(X_te_poly)
    results['å¤šé …å¼å›æ­¸ (d=3)'] = r2_score(y_te, y_pred)
    print(f"  å¤šé …å¼å›æ­¸ (d=3): RÂ²={results['å¤šé …å¼å›æ­¸ (d=3)']:.4f}")

    # 2. SVR (RBF)
    scaler_temp = StandardScaler()
    X_tr_scaled = scaler_temp.fit_transform(X_tr)
    X_te_scaled = scaler_temp.transform(X_te)
    svr = SVR(kernel='rbf', C=10, gamma='scale')
    svr.fit(X_tr_scaled, y_tr)
    y_pred = svr.predict(X_te_scaled)
    results['SVR (RBF)'] = r2_score(y_te, y_pred)
    print(f"  SVR (RBF): RÂ²={results['SVR (RBF)']:.4f}")

    # 3. Ridgeå›æ­¸ï¼ˆå¤šé …å¼ç‰¹å¾µï¼‰
    ridge = Ridge(alpha=1.0)
    ridge.fit(X_tr_poly, y_tr)
    y_pred = ridge.predict(X_te_poly)
    results['Ridgeå›æ­¸ (poly d=3)'] = r2_score(y_te, y_pred)
    print(f"  Ridgeå›æ­¸ (poly d=3): RÂ²={results['Ridgeå›æ­¸ (poly d=3)']:.4f}")

    comparison_results[dataset_name] = results

# ============================================================================
# 5.2 å¯è¦–åŒ–ï¼šæ€§èƒ½å°æ¯”è¡¨æ ¼
# ============================================================================
print("\nã€5.2ã€‘ç”Ÿæˆå¯è¦–åŒ–ï¼šç¶œåˆæ€§èƒ½å°æ¯”")

fig, axes = create_subplots(1, 2, figsize=(16, 6))

# å‰µå»ºå°æ¯”è¡¨æ ¼æ•¸æ“š
methods = ['å¤šé …å¼å›æ­¸ (d=3)', 'SVR (RBF)', 'Ridgeå›æ­¸ (poly d=3)']
dataset_names = list(comparison_results.keys())

table_data = []
for method in methods:
    row = [comparison_results[ds][method] for ds in dataset_names]
    table_data.append(row)

table_data = np.array(table_data)

# ç†±åŠ›åœ–
im = axes[0].imshow(table_data, cmap='RdYlGn', aspect='auto', vmin=0, vmax=1)
axes[0].set_xticks(np.arange(len(dataset_names)))
axes[0].set_yticks(np.arange(len(methods)))
axes[0].set_xticklabels(dataset_names, fontsize=10)
axes[0].set_yticklabels(methods, fontsize=10)
axes[0].set_title('ä¸åŒæ•¸æ“šé›†ä¸Šçš„ RÂ² åˆ†æ•¸\nRÂ² Scores on Different Datasets', fontsize=13, fontweight='bold')

# æ·»åŠ æ•¸å€¼æ¨™è¨»
for i in range(len(methods)):
    for j in range(len(dataset_names)):
        text = axes[0].text(j, i, f'{table_data[i, j]:.3f}',
                           ha="center", va="center", color="black", fontsize=11, fontweight='bold')

fig.colorbar(im, ax=axes[0], label='RÂ² Score')

# æŸ±ç‹€åœ–å°æ¯”
x = np.arange(len(dataset_names))
width = 0.25

for i, method in enumerate(methods):
    offset = width * (i - 1)
    values = [comparison_results[ds][method] for ds in dataset_names]
    axes[1].bar(x + offset, values, width, label=method, alpha=0.8)

axes[1].set_xlabel('æ•¸æ“šé›†é¡å‹', fontsize=12)
axes[1].set_ylabel('RÂ² Score', fontsize=12)
axes[1].set_title('æ¨¡å‹åœ¨ä¸åŒæ•¸æ“šé›†ä¸Šçš„æ€§èƒ½å°æ¯”\nModel Performance on Different Datasets',
                  fontsize=13, fontweight='bold')
axes[1].set_xticks(x)
axes[1].set_xticklabels(dataset_names, fontsize=10)
axes[1].legend(fontsize=10)
axes[1].grid(True, alpha=0.3, axis='y')

plt.tight_layout()
save_figure(fig, get_output_path('comprehensive_comparison.png', 'Regression'))

# ============================================================================
# ç¬¬å…­éƒ¨åˆ†ï¼šå¯¦éš›æ‡‰ç”¨æ¡ˆä¾‹
# Part 6: Practical Applications
# ============================================================================
print("\nã€ç¬¬å…­éƒ¨åˆ†ã€‘å¯¦éš›æ‡‰ç”¨æ¡ˆä¾‹ - ç³–å°¿ç—…æ•¸æ“šé æ¸¬")
print("-" * 80)

# ä½¿ç”¨ç³–å°¿ç—…æ•¸æ“šé›†é€²è¡Œå®Œæ•´çš„å»ºæ¨¡æµç¨‹
X_app, y_app = diabetes.data, diabetes.target

# æ•¸æ“šåˆ†å‰²
X_train_app, X_test_app, y_train_app, y_test_app = train_test_split(
    X_app, y_app, test_size=TEST_SIZE, random_state=RANDOM_STATE
)

# ç‰¹å¾µç¸®æ”¾
scaler_app = StandardScaler()
X_train_app_scaled = scaler_app.fit_transform(X_train_app)
X_test_app_scaled = scaler_app.transform(X_test_app)

# æ¸¬è©¦å¤šå€‹æ¨¡å‹
models_app = {
    'ç·šæ€§å›æ­¸': LinearRegression(),
    'å¤šé …å¼å›æ­¸ (d=2)': None,  # éœ€è¦ç‰¹æ®Šè™•ç†
    'Ridgeå›æ­¸': Ridge(alpha=1.0),
    'SVR (RBF)': SVR(kernel='rbf', C=10, gamma='scale'),
    'SVR (Poly)': SVR(kernel='poly', degree=2, C=10)
}

app_results = {}

# ç·šæ€§å›æ­¸ã€Ridgeã€SVR
for name, model in models_app.items():
    if model is not None:
        if 'SVR' in name:
            model.fit(X_train_app_scaled, y_train_app)
            y_pred = model.predict(X_test_app_scaled)
        else:
            model.fit(X_train_app, y_train_app)
            y_pred = model.predict(X_test_app)

        app_results[name] = {
            'r2': r2_score(y_test_app, y_pred),
            'rmse': np.sqrt(mean_squared_error(y_test_app, y_pred)),
            'mae': mean_absolute_error(y_test_app, y_pred),
            'y_pred': y_pred
        }

        print(f"\n{name}:")
        print(f"  RÂ²: {app_results[name]['r2']:.4f}")
        print(f"  RMSE: {app_results[name]['rmse']:.2f}")
        print(f"  MAE: {app_results[name]['mae']:.2f}")

# å¤šé …å¼å›æ­¸
poly_app = PolynomialFeatures(degree=2)
X_train_poly_app = poly_app.fit_transform(X_train_app)
X_test_poly_app = poly_app.transform(X_test_app)
lr_poly_app = LinearRegression()
lr_poly_app.fit(X_train_poly_app, y_train_app)
y_pred_poly = lr_poly_app.predict(X_test_poly_app)

app_results['å¤šé …å¼å›æ­¸ (d=2)'] = {
    'r2': r2_score(y_test_app, y_pred_poly),
    'rmse': np.sqrt(mean_squared_error(y_test_app, y_pred_poly)),
    'mae': mean_absolute_error(y_test_app, y_pred_poly),
    'y_pred': y_pred_poly
}

print(f"\nå¤šé …å¼å›æ­¸ (d=2):")
print(f"  RÂ²: {app_results['å¤šé …å¼å›æ­¸ (d=2)']['r2']:.4f}")
print(f"  RMSE: {app_results['å¤šé …å¼å›æ­¸ (d=2)']['rmse']:.2f}")
print(f"  MAE: {app_results['å¤šé …å¼å›æ­¸ (d=2)']['mae']:.2f}")

# ============================================================================
# 6.1 å¯è¦–åŒ–ï¼šå¯¦éš›æ‡‰ç”¨çµæœ
# ============================================================================
print("\nã€6.1ã€‘ç”Ÿæˆå¯è¦–åŒ–ï¼šå¯¦éš›æ‡‰ç”¨æ¡ˆä¾‹çµæœ")

fig, axes = create_subplots(2, 3, figsize=(18, 12))
axes = axes.flatten()

# ç‚ºæ¯å€‹æ¨¡å‹ç¹ªè£½é æ¸¬vsçœŸå¯¦å€¼
for idx, (name, results) in enumerate(app_results.items()):
    axes[idx].scatter(y_test_app, results['y_pred'], alpha=0.6, s=50, edgecolors='k', linewidth=0.5)
    axes[idx].plot([y_test_app.min(), y_test_app.max()],
                   [y_test_app.min(), y_test_app.max()],
                   'r--', lw=2, label='å®Œç¾é æ¸¬')

    axes[idx].set_xlabel('çœŸå¯¦å€¼ / True Values', fontsize=11)
    axes[idx].set_ylabel('é æ¸¬å€¼ / Predictions', fontsize=11)
    axes[idx].set_title(f'{name}\nRÂ²={results["r2"]:.3f}, RMSE={results["rmse"]:.2f}',
                       fontsize=12, fontweight='bold')
    axes[idx].legend(fontsize=9)
    axes[idx].grid(True, alpha=0.3)

# æœ€å¾Œä¸€å€‹å­åœ–ï¼šæ‰€æœ‰æ¨¡å‹æ€§èƒ½å°æ¯”
model_names = list(app_results.keys())
r2_values = [app_results[m]['r2'] for m in model_names]

axes[5].barh(model_names, r2_values, color='steelblue', alpha=0.7, edgecolor='black')
axes[5].set_xlabel('RÂ² Score', fontsize=12)
axes[5].set_title('æ‰€æœ‰æ¨¡å‹ RÂ² å°æ¯”\nRÂ² Comparison of All Models', fontsize=13, fontweight='bold')
axes[5].grid(True, alpha=0.3, axis='x')
axes[5].set_xlim(0, max(r2_values) * 1.1)

# æ¨™è¨»æ•¸å€¼
for i, (name, value) in enumerate(zip(model_names, r2_values)):
    axes[5].text(value + 0.01, i, f'{value:.4f}', va='center', fontsize=10, fontweight='bold')

plt.tight_layout()
save_figure(fig, get_output_path('application_case_results.png', 'Regression'))

# ============================================================================
# ç¸½çµå ±å‘Š
# Summary Report
# ============================================================================
print("\n" + "=" * 80)
print("éç·šæ€§å›æ­¸æ•™ç¨‹ç¸½çµ".center(80))
print("=" * 80)

print("""
ğŸ“Š æœ¬æ•™ç¨‹æ¶µè“‹çš„å…§å®¹ï¼š

1. å¤šé …å¼å›æ­¸ï¼ˆPolynomial Regressionï¼‰
   âœ“ ä¸åŒéšæ•¸çš„å½±éŸ¿ï¼ˆ1-10æ¬¡ï¼‰
   âœ“ éæ“¬åˆèˆ‡æ¬ æ“¬åˆè­˜åˆ¥
   âœ“ æœ€ä½³éšæ•¸é¸æ“‡

2. æ”¯æŒå‘é‡å›æ­¸ï¼ˆSVRï¼‰
   âœ“ ä¸åŒæ ¸å‡½æ•¸ï¼ˆLinear, Polynomial, RBF, Sigmoidï¼‰
   âœ“ åƒæ•¸èª¿å„ªï¼ˆC, epsilon, gammaï¼‰
   âœ“ æ ¸å‡½æ•¸é¸æ“‡ç­–ç•¥

3. æ¨£æ¢å›æ­¸ï¼ˆSpline Regressionï¼‰
   âœ“ B-Spline
   âœ“ å¹³æ»‘æ¨£æ¢
   âœ“ å¹³æ»‘åƒæ•¸å½±éŸ¿

4. ç¶œåˆå°æ¯”
   âœ“ åœ¨ä¸åŒæ•¸æ“šé›†ä¸Šçš„è¡¨ç¾
   âœ“ æ–¹æ³•é¸æ“‡å»ºè­°

5. å¯¦éš›æ‡‰ç”¨
   âœ“ ç³–å°¿ç—…æ•¸æ“šé æ¸¬
   âœ“ å®Œæ•´å»ºæ¨¡æµç¨‹

ğŸ“ˆ ç”Ÿæˆçš„å¯è¦–åŒ–åœ–è¡¨ï¼š
â€¢ polynomial_regression_comparison.png - å¤šé …å¼å›æ­¸éšæ•¸å°æ¯”
â€¢ polynomial_overfitting_analysis.png - éæ“¬åˆåˆ†æ
â€¢ svr_kernel_comparison.png - SVRæ ¸å‡½æ•¸å°æ¯”
â€¢ svr_parameter_tuning.png - SVRåƒæ•¸èª¿å„ª
â€¢ spline_regression.png - æ¨£æ¢å›æ­¸
â€¢ comprehensive_comparison.png - ç¶œåˆå°æ¯”
â€¢ application_case_results.png - å¯¦éš›æ‡‰ç”¨æ¡ˆä¾‹

ğŸ’¡ é—œéµè¦é»ï¼š
1. å¤šé …å¼å›æ­¸ç°¡å–®ä½†å®¹æ˜“éæ“¬åˆï¼Œéœ€è¦è¬¹æ…é¸æ“‡éšæ•¸
2. SVRé©åˆå°åˆ°ä¸­ç­‰è¦æ¨¡æ•¸æ“šï¼Œéœ€è¦ç‰¹å¾µç¸®æ”¾
3. RBFæ ¸æ˜¯SVRçš„å¥½é¸æ“‡ï¼Œåƒæ•¸éœ€è¦èª¿å„ª
4. æ¨£æ¢å›æ­¸åœ¨å±€éƒ¨æ“¬åˆä¸Šè¡¨ç¾å„ªç§€
5. ä¸åŒæ–¹æ³•é©ç”¨æ–¼ä¸åŒé¡å‹çš„æ•¸æ“š

ğŸ¯ ä¸‹ä¸€æ­¥ï¼š
â€¢ å­¸ç¿’åŸºæ–¼æ¨¹çš„å›æ­¸æ–¹æ³•ï¼ˆDecision Tree, Random Forest, XGBoostï¼‰
â€¢ æ¢ç´¢æ·±åº¦å­¸ç¿’å›æ­¸ï¼ˆç¥ç¶“ç¶²çµ¡ï¼‰
â€¢ å¯¦è¸æ›´å¤šçœŸå¯¦æ¡ˆä¾‹
""")

print("=" * 80)
print("æ•™ç¨‹çµæŸï¼æ‰€æœ‰åœ–è¡¨å·²ä¿å­˜åˆ° output/Regression/ ç›®éŒ„".center(80))
print("=" * 80)
